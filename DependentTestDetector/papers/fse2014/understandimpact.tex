\section{Evaluating the Impact}

\newcommand{\jt}{Joda-Time\xspace}

\newcommand{\jfreecharttests}{2234\xspace}%change the total num
\newcommand{\jodatimetests}{3875\xspace}
\newcommand{\xmlsecuritytests}{108\xspace}
\newcommand{\crystaltests}{75\xspace}
\newcommand{\synoptictests}{118\xspace}
\newcommand{\totaltests}{4176\xspace}

\newcommand{\jfreechartautotests}{2946\xspace}
\newcommand{\jodatimeautotests}{2639\xspace}
\newcommand{\xmlsecurityautotests}{665\xspace}
\newcommand{\crystalautotests}{3198\xspace}
\newcommand{\synopticautotests}{2467\xspace}
\newcommand{\totalautotests}{8969\xspace}

\label{sec:impact}

This section describes our empirical evaluation of
the impact of test dependence on test prioritization,
test selection, and test parallelization techniques.

\begin{table}
\centering
\setlength{\tabcolsep}{0.25\tabcolsep}
\begin{tabular}{|l|l|c|c|l|}
%\toprule
\hline
\textbf{Program} & \textbf{LOC} & \textbf{\#Tests} & \textbf{\#Auto Tests} & \textbf{Revision}
\\
\hline
%JFreechart & 92253 & \jfreecharttests & \jfreechartautotests& 1.0.15\\
%if we add JFreechart, need to change the total num, and num of subject program
%\midrule
\jt & 27183 & \jodatimetests
% 3875 is retrieved by running mvn test on the related revision
& -- &  b609d7d66d\\
XML Security & 18302 & \xmlsecuritytests & \xmlsecurityautotests& version 1.0.4 \\ 
Crystal & 4676 & \crystaltests & \crystalautotests& trunk version\\
Synoptic & 28872 & \synoptictests & \synopticautotests&  trunk version\\ 
JFreechart& xxx & xxx & xxx &  xxx \\ 
%\bottomrule
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Subject programs used in our evaluation.
Column ``\#Tests'' shows the number of human-written
unit tests. Column
``\#Auto Tests'' shows the number of 
unit tests generated by Randoop~\cite{PachecoLET2007}.
}
\label{tab:subjects}
\end{table}

\subsection{Subject Programs}

Table~\ref{tab:subjects} lists the programs and
tests used in our evaluation. We used these subject
programs because they have been developed for
a considerable amount of time (3--10 years) and each
of them includes a well-written unit test suite.

\jt~\cite{jodatime} is an open source
date and time library. It is a mature project that
has been under active development
for ten years. XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. XML Security is included in
the SIR repository~\cite{sir} and has been used widely
as a subject program in the software testing community.
Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and
identifies textual, compilation, and behavioral conflicts.
Synoptic~\cite{synoptic} is a tool to mine a finite state
machine model representation of a system from logs.
All of the subject programs' test suites are designed to be executed in
a single JVM, rather than requiring separate processes per test case~\cite{vmvm}.

Given the increasing importance of automated test generation
tools~\cite{PachecoLET2007, ZhangSBE2011, Csallner:2004, fraseretal:ISSTA:2011},
we also want to investigate dependent tests in automatically-generated
test suites. For each subject program, we use
Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to create a suite of 5,000 tests.
Randoop automatically drops textually-redundant tests 
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.

We discarded the automatically-generated test suite of
\jt, since many tests in it are non-deterministic ---
they depend on the current time.

\todo{explain two versions}
\todo{explain why these subject programs}

\subsection{Methodology}

We implemented all testing techniques listed
in Tables~\ref{},~\ref{}, and~\ref{}, and evalaute
each technique on our subject programs in
Table~\ref{}.

For each subject program, we first execute its
test suite in the \textit{default} order and record
the execution result of each test.
We adopt the results of the default
order of execution of a test suite as the expected results; these
are the results that a developer sees when running the suite
in the standard way. 

A test prioritization technique outputs a reordered test suite.
We execute the reordered test suite and count the number
of dependent tests that return different results
in the prioritized roder as they do when executed in the
unprioritized roder.

A test selection technique identifies a subset of the
input test suite to run during regression testing, but does not
change the relative order among the test suite. We run execute
the selected subset and count the number of dependent tests
that return different results as they do when executed
in the original test suite. In our experiment, for each
subject program in Table~\ref{}, we use the next revision
as the ``new'' program version, and apply the test selection
technique to select a subset of the test suite that should
be re-run.

A test parallelization technique divides the input
test suite into multiple subsets, and schedules each
subset for execution on a different machine. Similar to
test selection, test parallelization techniques usually
do not change the relative order among the test suite.
We count the number of dependent tests that return different
results as they do when executed in the original test suite.
Our experiment focuses on the execution result of each test.
Due to resource limits, we run all subsets of the test suite
on a single machine one by one. Before running a subset,
we completely re-initialize the execution environment and launch
a new JVM.

\subsection{Results}

This section presents the evaluation results
for test prirotization, test selection, and
test parallelization techniques.

\subsubsection{Impact on Test Prioritization}

\begin{table}
\centering
\setlength{\tabcolsep}{1.25\tabcolsep}
\begin{tabular}{|l|l|l|l|l|l|}
%\toprule
\hline
\textbf{Subject Program} & T1 & T3 & T4 & T5 & T7 \\
\hline
\multicolumn{6}{|l|}{}  \\
\multicolumn{6}{|l|}{\textbf{Human-written Test Suites}}  \\
\hline
\jt& 0 & 0 & 1 & 0 & 0\\
XML Security& 0 & 0 & 0 & 0 & 0 \\
Crystal& 12 & 11 & 16 & 11 & 12 \\
Synoptic& 0 & 0 & 0 & 0 & 0 \\
JFreechart& 0 & 3 & 3 & 3 & 0 \\
%\bottomrule
\hline
\textbf{Total} & 12 & 14 & 20 & 14 & 12\\
\hline
\multicolumn{6}{|l|}{}  \\
\multicolumn{6}{|l|}{\textbf{Automatically-generated Test Suites}}  \\
\hline
\jt& 258 & 289 & 269 & 291 & 286\\
XML Security& 82 & 77 & 57 & 75 & 54 \\
Crystal& 79 & 74 & 86 & 89 & 90 \\
Synoptic& 0 & 2 & 3 & 3 & 3 \\
JFreechart& 3 & 5 & 5 & 5 & 5 \\
%\bottomrule
\hline
\textbf{Total} & 422 & 447 & 420 & 463 & 438\\
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Results of evaluating the \prionum test prioritization techniques
in Table~\ref{tab:testprio} on four human-written unit test suites.
Each cell shows the number of dependent tests
that do not return the same results as they do when executed
in the default, unprioritized order. \todo{revise the number here}
}
\label{tab:testprioresult}
\end{table}

The dependent tests in our subject programs interfere with
all the XXX test prioritization techniques in Table~\ref{}. This
is because all these techniques implicitly assume that there
are no test dependences in the input test suite. Violation of
this assumption, as happened in real-world unit test suites,
causes undesired output. Further, test prioritization algorithms
usually donot take the potential test dependence into
consideration when reordering the test suite.

\todo{which test prioritization techniques are the most sensitive ones}

\todo{break down the data a bit and how many tests are changing from
passing to failing? or vice versa}

\todo{give specific examples here}


\subsubsection{Impact on Test Selection}

\begin{table}
\centering
\setlength{\tabcolsep}{1.25\tabcolsep}
\begin{tabular}{|l|c|c|}
%\toprule
\hline
\textbf{Subject Program} & S1 (statement-level) & S2 (method-level)  \\
\hline
\multicolumn{3}{|l|}{}  \\
\multicolumn{3}{|l|}{\textbf{Human-written Test Suites}}  \\
\hline
\jt& 0 & 0 \\
XML Security& 0 & 0 \\
Crystal& 0 & 0\\
Synoptic& 0 & 0  \\
JFreechart& 0 & 0  \\
%\bottomrule
\hline
\textbf{Total} & 0 & 0  \\
\hline
\multicolumn{3}{|l|}{}  \\
\multicolumn{3}{|l|}{\textbf{Automatically-generated Test Suites}}  \\
\hline
\jt& 64 & 261 \\
XML Security& 24 & 24  \\
Crystal& 0 & 0  \\
Synoptic& 0 & 0  \\
JFreechart& 0 & 3  \\
%\bottomrule
\hline
\textbf{Total} & 88 & 288 \\
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Results of evaluating the \selnum test selection techniques
in Table~\ref{tab:testprio} on four human-written unit test suites.
Each cell shows the number of dependent tests
that do not return the same results as they do when executed
in the original test suite. \todo{revise the number here}
}
\label{tab:testselresult}
\end{table}

\subsubsection{Impact on Test Parallelization}

\begin{table*}
\centering
\setlength{\tabcolsep}{1.25\tabcolsep}
\begin{tabular}{|l| l|l|l|l| l|l|l|l| l|l|l|l|}
%\toprule
\hline
\textbf{Subject Program} & \multicolumn{4}{|l|}{P1 (Original Order Parallelization)} &  \multicolumn{4}{|l|}{P2 (Random Parallelization)} & \multicolumn{4}{|l|}{P3 (Time-Minimized Parallelization)}\\
\cline{2-13}
& k=2 & k=4 & k=8 & k=16 & k=2 &k=4& k=8& k=16 & k=2 &k=4& k=8& k=16\\
\hline
\multicolumn{13}{|l|}{}  \\
\multicolumn{13}{|l|}{\textbf{Human-written Test Suites}}  \\
\hline
\jt& 0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 3 & 1 & 0 & 1\\
XML Security& 0 & 0 & 4 & 4 & 0  & 0 & 0 & 2& 2 & 4 & 4& 4\\
Crystal& 8 & 8 & 18  & 15 & 17 &16& 18 & 18& 9 & 16& 15& 18\\
Synoptic& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0& 0\\
JFreechart& 0 & 0 & 0 & 0 & 1 &1 & 0 & 0& 2 & 0 & 1 & 1\\
%\bottomrule
\hline
\textbf{Total} & 8 & 8 & 22  & 19  & 19 & 18& 19& 21& 16& 21& 20& 24\\
\hline
\multicolumn{13}{|l|}{}  \\
\multicolumn{13}{|l|}{\textbf{Automatically-generated Test Suites}}  \\
\hline
\jt& 22 & 48 & 201 & 236 & 324 & 267 & 300 & 324& 354 & 268 & 305 & 307\\
XML Security& 66 & 78 & 104 & 111 & 91 & 103 & 111 & 116& 73 & 115 & 113 & 116\\
Crystal& 0  & 0 & 1 & 4 & 93 & 86 & 81 & 91& 88 & 88 & 86 & 102\\
Synoptic& 1 & 1 & 2 & 2 & 2 & 1 & 2 & 1& 2 & 2 & 2 & 2 \\
JFreechart& 1 & 3 & 3 & 3 & 2 & 4 & 3 & 3 & 2 & 2 & 2 & 3\\
%\bottomrule
\hline
\textbf{Total} & 90 & 130  & 311  & 356 & 512 & 461 & 497& 535 & 519& 475& 508& 530\\
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Results of evaluating the \parnum test parallelization techniques
in Table~\ref{tab:testprio} on four human-written unit test suites.
Each cell shows the number of dependent tests
that do not return the same results as they do when executed
in the original test suite. \todo{revise the number here}
}
\label{tab:testparresult}
\end{table*}

\subsection{Discussion}

\subsubsection{Root Causes of Test Dependence}

Essentially, test dependence results from
interactions with other tests, as reflected in
the execution environment. Tests may make
implicit assumptions about their execution environment --
values of global variables, contents of files, etc. A dependent
test manifests when another alters the execution environment
in a way that invalidates those assumptions.

Our previous study~\cite{} suggested three common
root causes of test dependence: (1) improper access
to shared global variables (i.e., static
variables in Java), (2) improper access
to the file systems, and (3) improper access
to other external resources (e.g., databases,
networks). Improper access to
shared global variables is the most common
root causes, accounting for at least 61\% of 
all dependent tests we have studied. All dependent
tests in our experiments arise due to access
to static Java variables.

The downstream testing techniques we have evaluated
have indirectly changed the execution environment
that a test may implicitly assume, such as alternating
the test execution order (as test prioritization
does) and selecting a subset of
test to execute (as test selection and parallelization
do).



\subsubsection{Threats to Validity}

Our findings apply in the context of our
study and methodology and may not apply to arbitrary
programs. First, the applications we studied are all written
in Java and have JUnit test suites.
The XXX open-source programs and
their test suites may not be representative enough.
However, these are the first XXX subject programs we
tried, and the fact that we found dependent tests in all of
them and these dependent tests affect all evaluated
downstream testing techniques is suggestive.
Second, in our study, we only evaluated XXX
test prioritization, XXX test selection, and
XXX test parallelization techniques. Evaluating
dependent tests on other testing techniques
may yield different results.

\subsubsection{Experimental Conclusion}

We have three chief findings. \textbf{(1)}
Dependent tests in both human-written
and automatically-generated test suites
can affect the results of test prioritization,
test selection, and test parallelization
techniques. \textbf{(2)} The degree of 
impact varies across different testing
techniques. \todo{explain more}
\textbf{(3)} \todo{talk about the reason}
