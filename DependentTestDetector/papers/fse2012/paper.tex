%File: paper.tex
%Part of: FSE 2012 paper
%Author: Jochen Wuttke
%Date: 2012-2-06


\documentclass[letterpaper]{sig-alternate}
\usepackage{graphicx}
\usepackage[draft]{fixme}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{color}
\usepackage[draft]{hyperref}
\usepackage{xspace}
\usepackage{subfigure}
\usepackage{bold-extra}

\usepackage{color}
\newcommand{\CodeIn}[1]{{\small\texttt{#1}}}

% % Add line between figure and text
%\makeatletter
%\def\topfigrule{\kern3\p@ \hrule \kern -3.4\p@} % the \hrule is .4pt high
%\def\botfigrule{\kern-3\p@ \hrule \kern 2.6\p@} % the \hrule is .4pt high
%\def\dblfigrule{\kern3\p@ \hrule \kern -3.4\p@} % the \hrule is .4pt high
%\makeatother
 % If there is a line, you can get away with reducing the separation between
 % figures and text.  Don't do this without the line, though.
%\addtolength{\textfloatsep}{-.5\textfloatsep}
%\addtolength{\dbltextfloatsep}{-.5\dbltextfloatsep}
%\addtolength{\floatsep}{-.5\floatsep}
%\addtolength{\dblfloatsep}{-.5\dblfloatsep}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
%\newtheorem{proof}{Proof}
\newcommand{\pass}{\ensuremath{\mathit{PASS}}\xspace}
\newcommand{\fail}{\ensuremath{\mathit{FAIL}}\xspace}

% commands for formalization
\newcommand{\suites}[0]{\ensuremath{\mathcal{S}\xspace}}
\newcommand{\environs}[0]{\ensuremath{\mathcal{E}\xspace}}
\newcommand{\manifest}[1]{\ensuremath{\prec_{#1}}}
\newcommand{\suite}[1]{\ensuremath{ \langle  #1 \rangle }}
\newcommand{\env}[0]{\ensuremath{\mathbf{E}}\xspace}
\newcommand{\exec}[2]{\ensuremath{\varepsilon(#1,#2)}}
\newcommand{\result}[2]{\ensuremath{R(#1|#2)}}

%commands must be the last package imported
% This package provides the \todo{Name}{Comment} command
\usepackage{commands}
%remove syntax highlighting from Java code
\lstset{basicstyle=\ttfamily,tabsize=2,keywordstyle=\ttfamily,stringstyle=\ttfamily,commentstyle=\ttfamily,
captionpos=b,numberstyle=\small\ttfamily,numbersep=1ex,
keywordstyle=\color{red}}

%\renewcommand{\todo}[2]{}

\author{
\hfill Jochen Wuttke \hspace{1cm} K\i{}van\c{c} Mu\c{s}lu \hspace{1cm} Sai Zhang
\hspace{1cm} David Notkin \hfill\\ 
\affaddr{Computer Science \& Engineering}\\ 
\affaddr{University of Washington} \\ 
\affaddr{Seattle, WA, USA} \\
\email{\{wuttke|kivanc|szhang|notkin\}@cs.washington.edu}
}
%\conferenceinfo{ASE}{'09 Auckland, New Zealand}

%\title{Reexamining the Test Independence Assumption}
%Does Test Execution Order Matter?
%\title{Test Dependence: Theory and Observations}
\title{Test Dependence: Theory and Manifestation}

\begin{document}
\maketitle
\begin{abstract}
%{\color{red}
%\noindent Dear friendly pre-reviewer: 
%
%\noindent This is a reasonably complete draft that we plan to submit
%to FSE 2012 on Friday March 16.  There are a few specifics we'd love
%for you to look at, but you're of course free to comment on other
%stuff (including typos, grammar, etc.); the conclusion is probably
%the part we've worked on the least so far, if you need to do something
%more lightly.  (OK, the abstract has been worked on even less!)  Among
%the specifics we'd like to hear about are: (a) The related work is
%reasonable extensive, but isn't done.  If you have any suggestions of
%work we've missed, or work that would help others put our work in
%context, or about the ``feel'' of what it says, let us know. (b) The
%core content is even more important; do we have something here?  Are
%you interested?  If not, why not?
%
%\noindent Thanks in advance for anything you point out!
%
%\noindent David, Jochen, Kivanc and Sai
%}
Test dependence arises when executing a test in different environments
causes it to return different results.  In this paper, we
show through a set of substantive real-world examples that test dependence arises in practice. 
We also show that test dependence can have potentially costly
repercussions such as masking program faults, and can be hard to identify
unless explicitly searched for: We found a dependence
that only manifests when a sequence of three tests are run in a specified, non-default order.
%.  For example, we identify several situations where test dependence masks
%program faults: in these situations, running the test suite in the default order does not expose a fault
%but running the suite in a different order does.  
%We also argue that existing
%tools rarely ``surface'' test dependences in a direct way, making it harder for developers
%to observe them.

We formally define test dependence in terms of test suites as ordered
sequences of tests along with explicit environments in which these tests are
executed. We use this formalization to formulate the concrete problem
of detecting dependence in test suites, prove that a useful special
case is NP-complete, and propose an initial algorithm that
approximates solutions to this problem.

%To a lesser degree, we describe how two trends in software testing may interact
%with test dependence: one, downstream testing tools such as selection, prioritization,
%and parallelization are increasingly common, and may assume that the
%suites they take as input have no test dependences; and, two, automated test
%generation tools are becoming more common, and we provide some initial
%evidence that test dependence appears to be orders of magnitude more common
%in automatically-generated test suites than in manually-produced suites.
%

%WE show that, in practice,
%test dependence does occur sometimes and does have grave consequences.
%We further argue, that given the increasing importance of second-order
%testing techniques, such as prioritization and parallelization, which
%are directly affected by test dependence, this topic deserves
%attention from the research community. As a first step, 
%
%
%\todo{KM}{The abstract was only about the ``theory'' side of our paper, so I
%tried to start writing one more paragraph about the ``manifestation'' side. It
%is not perfect (and not complete) but I still think that something like this
%needed in the abstract.} 
%In the second half of the paper, we explain the dependences in the manual
%written and auto-generated test suites of known and popular open source
%software. We present an initial exploration over six software and report 75
%(resp. 1975) dependent tests in manual written (resp. auto-ge\-ne\-rated) test
%suites.

\end{abstract}

%\category{D.2.4.}{Software/Program Verification}{Assertion Checkers}
\category{D.2.5}{Testing and Debugging}%{Monitors}
%
%\terms{Design, Reliability}
%
\keywords{Software testing; test dependence; test selection; test prioritization}

\section{Introduction}

Informally, \emph{dependent tests} produce different test results when
executed in different contexts. %, while \emph{independent tests} produce
%the same test results regardless of execution order.  
It is easy to
construct an example of dependence between two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to both tests passing, while running \code{B} and then
\code{A} leads to either or both tests failing---the order
of applying the tests, in this case, changes the execution context.

%~\cite{KapfhammerS03}
%Chays:2000:FTD:347324.348954,
%Gray:1994:QGB:191843.191886}, 

Definitions in the testing literature are generally clear that the
conditions under which a test is executed may affect its result.  The
importance of context in testing has been explored in some depth in
some domains including databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,
kapfhammeretal:FSE:2003}, with results about test
generation, test adequacy criteria, etc., and mobile
applications~\cite{Wang:2007:AGC}.
For the database domain, Kapfhammer and Soffa formally
define and distinguish independent test suites from those that are
\emph{non-restricted\/} and thus ``can capture more of an application's
interaction with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations of test
adequacy''~\cite[p.~101]{kapfhammeretal:FSE:2003}.

At the same time, there is little focus on the core
issues of
test dependence itself.
Is this because test dependence does
not arise in practice (beyond domains such as databases)?  Is it because, even if-and-when it does arise, there
are few if any repercussions?  Is it because it is difficult to
notice if-and-when it arises?

\subsection{Manifest Test Dependence}

To explore these questions, we consider a narrow characterization
of test dependence that:
\begin{itemize}
\item Adopts the results of the default, usually implicit,
  order of execution of a test suite as the \emph{expected results}. 
\item Asserts \emph{test dependence\/} when there is a possibly
  reordered subsequence of the original test suite that, when
  executed, has at least one test result that differs from the
  expected result for that test.  
\end{itemize}
That is, we focus on a \emph{manifest\/} perspective of test dependence,
requiring a \emph{concrete\/} variant of the test suite that
\emph{dynamically\/} produces different results from the expected.  Our
definition differs from that of Kapfhammer and Soffa by considering
test results rather than program and database states.
As we discuss later, considering only manifest test dependences allows
us to more easily situate this research in the empirical domain.

%: (a) given the lack of
%attention to test dependence, reporting solely on potential but
%unrealized dependence, that is, ``false positives'', might be of little value; and
%(b) computational advantages arise from computing manifest rather than
%potential dependence.

\subsection{Examples and Repercussions}

We have identified a number of substantive examples of test suites
from fielded programs that manifest dependences.
%in fielded programs. 
%with test suites that manifest test dependence.  
We examined
six projects and found in their human-written test suites a total
of 75 dependent tests ($1.4 \%$). For the same set of
programs, we also generated test suites automatically using
Randoop~\cite{PachecoLET2007} and found that on average $14 \%$ of
the generated tests are dependent.

%Further, considering the increasing importance of
%automated test generation techniques, we wanted to get an impression
%of whether test dependence occurs in automatically generated test
%suites. We applied Randoop to the same
%set of programs and 

By analyzing these examples of test dependence, we identified three
categories of problems that can arise due to the presence of dependent tests.
First, test suites that unexpectedly contain dependent tests can
\emph{mask faults in a program}.  We present examples where
executing a test suite in the default order does not expose the fault, whereas
executing the same test suite in a different order does.
Second, test suites that unexpectedly contain dependent tests can \emph{conceal
weaknesses in the test suite} itself.  We present examples where exposing
dependent tests can identify situations where some tests do not perform
proper initialization.
Third, a test suite containing undocumented test dependences can lead
to \emph{spurious bug reports}.  We present an example where it took the developers
more than a month to realize that the test dependences were intentional,
allowing them to close the bug report without a change to the system.

%In practice, test suites that are thought to include only independent
%tests but that manifest dependence can cause problems including:
%\begin{itemize}
%\item masking faults in the program that are not exposed in one execution order but that are
%in another order; 
%\item exhibiting unexpected
%results if reordered (for instance, by downstream testing techniques such as
%test selection or prioritization), a likely indication of poor test construction; and,
%\item reporting of spurious bugs.
%% if the tests are intended to be dependent but the dependence
%%is undocumented. \todo{DN}{I'm thinking of removing this bullet from the intro.
%%It's a bit different, in that the test writers understand the dependence, of course.
%%I'm just afraid that it'll increase confusion instead of clarifying things.}
%\end{itemize}

%As an example of the first category, the JodaTime library
%(Section~\ref{sec:jodatime}) defines a complex caching system.  Its
%test suite includes tests that check the rather complicated function that normalizes
%object states into cache keys.
%However, an unexpected test dependence between two tests masks a bug in this code. The default
%test execution order exercises an unintended path for one test because an object is
%already cached due to a previously executed test; the fault is exposed if that object
%is not initially cached, as would happen
%if the two tests
%are executed in the reverse order.
%Examples of the other categories are
%described in Section~\ref{sec:examples}.

%Test suites with unknown dependent tests may also exhibit unexpected
%results if reordered by downstream testing techniques such as
%test selection or prioritization  In addition, undocumented
%but desired test dependence can
%lead to spurious bug reports.

\subsection{Test Execution Environment}

Our examples highlight varying execution environments as the
unsurprising central cause of test dependence. Specifically, when a
test is executed in different environments---global variables
with different values, differences in the file system, differences in
data obtained from web services, etc.---it has the potential to return
a different result.  
%
%Changing a test suite's execution order can
%increase the potential to change the execution environment for a given test:
%different tests may be executed before that given test, and they may
%produce an environment that may cause the test to have a different result.
Most of the dependences we see in our
examples ultimately stem from incorrect or incomplete initialization
of the program environment.

Why does this happen? Especially given frameworks such as
%
%It is justified to argue that developers 
%should take the utmost care to initialize their tests correctly and completely. In principle,
%such setup code could be part of each individual test case. Frameworks
JUnit that facilitate the process of clean setup by providing means to
automatically execute methods (\code{setUp()} and \code{tearDown()} in JUnit
3.x, and methods annotated with \code{@Before} and \code{@After} in
JUnit 4.x) that should handle all common setup and clean-up between
test cases. 

It appears that the answer is that developers are as likely
to make mistakes when writing tests as when they are writing other code.
And while frameworks make it easier to get environment setup right, 
they cannot ensure that it is done properly. 
%The frameworks provide effective mechanisms for setting test execution
%environments, but they do not ensure that these mechanisms are used properly.
Like with other code, this means that tests in some cases will have
unintended and unexpected behaviors.  And as programs increase in complexity,
so may tests, which may increase the frequency of such problems in tests,
which may in turn increase the frequency of test dependence.
%And the more complex a programs structure
%is, the more likely it is that some initialization of global variables
%will be forgotten. By identifying test dependence
%as a more broadly discussed issue, and by providing algorithms and
%tools for identifying test dependences, we hope to reduce their


%Yet no framework can ensure that these methods are used
%correctly. 
%Since we are most interested in practical issues, we argue that
%developers are as likely to make mistakes when writing tests, as they
%are when writing code. 
%\todo{mark it red to avoid been overlooked}{frequency and their cost.}

%\subsection{Downstream Testing Tools}

Another situation in which the underlying test execution context can
unexpectedly change is when a tool or technique that takes a test
suite as input is used.  Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite that will guarantee some properties during
regression testing)~\cite{harroldetal:OOPSLA:2001}, test prioritization techniques (that reorder the
input to increase the rate of fault detection)~\cite{Elbaum:2000:PTC:347324.348910}, test parallelization
techniques (that schedule the input for execution across multiple
CPUs), test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which take system tests as
input and create unit tests and differential unit tests,
respectively), etc. 

Of these techniques, we are most concerned about
those that modify the organization of test suites, rather than the tests
they contain.
Many such downstream testing techniques implicitly assume that
there are no test dependences in the input test suite.  Our concern is
that this assumption can cause \emph{delayed problems} in the face
of latent test dependence in the input.  As an
example, test selection may report a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.
%In essence, these tools may have an unstated precondition---''the input
%must have no test dependences''---that may not be checked or satisfied in some cases.
%An alternative would be for the tools to detect and eliminate dependences.
%
%If this selection happened for regression testing, developers may be
%led to investigate only modified and newly added code to find the
%fault. But it is possible that the fault lies
%elsewhere and has not been discovered due to the dependence in the
%test suite. \todo{KM}{I don't buy this. Once you have the failing test, it
%should not be very hard for a reasonable developer to find the real reason
%behind the failure.}

%
%The two fundamental operations that such tools can apply to test
%suites are sub-suite selection, and suite reordering. Our
%formalization in Section~\ref{sec:formalism} and the examples in
%Section~\ref{sec:examples} show that both these operations can
%produce test suites that exhibit manifest dependences, because all
%these operations lead to tests being executed in potentially different
%environments. While there are some differences between selection and
%re-ordering, these are not significant for the following discussion. 
%Therefore, in Section~\ref{sec:related} we consider only
%related work in test prioritization as a representative of this
%entire class of techniques. %, especially due to its concern with reordering.
%

%The value of studying manifest dependence lies in the fact that
%it can impact second
%order techniques, such as test prioritization or parallelization.
%One premise of such techniques is usually that executing tests in any
%order or in parallel will produce the same results. Therefore, we
%consider as test dependence, effects that cause the results of tests
%to differ when they are executed in different environments.
%Such test dependence may arise when the test results rely on a particular
%context that may unexpectedly differ from one execution order to
%another (Figure~\ref{fig:downstream}).  
%For example, if test \code{A} assumes that a global variable has been
%initialized by some other test, executing test \code{A} before those
%tests may cause different test results.
%Conversely, tests are independent when
%they either do not rely on context at all, or assure the correct
%context before executing.



% \todo{KM}{I don't think what this paragraph says is true. I recommend cutting
% it (the above paragraph also contains what it tries to say).} Our examples also
% show extensive use of test infrastructure that can reduce test dependence: in
% JUnit\footnote{\url{http://www.junit.org}}, \code{setUp()} and
% \code{tearDown()} as well as \code{@before} and \code{@after}
% annotations help developers significantly but allowing them to more
% easily establish the execution environment.  However, these and
% related features are mechanisms only and are not intended to, and do
% not, enforce any policies to ensure that developers use the
% mechanisms consistently and effectively.
% 
% Why do some tests allow varying execution environments?  Can't this
% be easily avoided?
% We contend
% that this is for the same reasons that developers still create bugs,
% still don't always initialize program variables, still don't always check
% array lengths before indexing, etc.  By identifying test dependence
% as a more broadly discussed issue, and by providing algorithms and
% tools for identifying test dependences, we hope to reduce their
% frequency and their cost.

\subsection{Contributions}

At its heart, this paper addresses and questions conventional wisdom about the
test independence assumption.  It is intended to balance a precise 
characterization of test dependence with substantive empiric examples and concerns. The contributions of the paper include:
\begin{itemize}
%  \item Evidence from the literature that test independence is broadly assumed but rarely addressed (Section~\ref{sec:related}).
  \item A precise formalization of test dependence in terms of test suites as ordered sequences
  of tests and explicit execution environments for test suites, which enables reasoning about test dependence
  as well as a proof that finding \emph{manifest} dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).
  \item Examples from fielded software of test suites where manifest test
  dependences lead to identifiable concerns with the underlying programs or test suites (Section~\ref{sec:examples}).
  \item Motivation for and presentation of our initial approximate algorithms and a supporting tool for identifying test dependences (Section~\ref{sec:algorithm-tool}).
\end{itemize}

%: in practice, it does not always hold and, 
%this can cause problems. 
%We provide evidence
%from fielded software that shows that, while perhaps uncommon, test
%dependence arises in practice. 
Although we provide evidence that test dependence unexpectedly arises in practice,  
a broad
study of how often test dependences arise, and the costs that these may lead to,
is beyond the scope of this paper. 
We conclude the paper with a set of open questions addressing this and
other possible concerns in Section~\ref{sec:questions}.

%While superficially straightforward, reasoning about test dependence
%%and the potential causes and consequences of test dependence is
%is intricate and non-trivial. We introduce a formalism to help
%understand and reason about test dependence.  The two key aspects
%of the formalism are (a) defining
%test
%suites as ordered sequences of tests and (b) making explicit the
%context in which tests are run. The formalism provides a precise
%basis for defining test dependence, for proving the (NP-hard) complexity
%of determining if a suite can manifest test dependences, and for
%algorithms that can efficiently identify important classes of dependences.
%
%
%may arise due to the context 
%in which tests are
%executed (Section~\ref{sec:formalism} formalizes context). Hence, the
%notion and use of context is the second fundamental part of our
%formalism.


%We raise awareness of the problems caused by the test
%  independence assumption but demonstrating through theory and
%  manifestations of test dependence that the consequences can be
%  severe. 
%  \item We define a formalism to reason about test suites as sequences
%  and show how dependences arise in theory and practice.
%  \item We lay a foundation for efficient heuristic algorithms to
%  detect dependences in existing test suites and show with some
%  examples that heuristics rather than exhaustive algorithms already
%  have signigificant benefits.


%\todo{JW}{I couldn't fit this into the rewritten intro. I might like
%to use in in Sec 4 or 5}
%The two ways
%  of altering context that we address here are \emph{isolation} and
%  \emph{ordering}.  By isolation, we mean executing each test in a
%  test suite separately: for example, in a different instance of JUnit
%  or in a different virtual machine.  This isolates, and may provide a
%  different context for a test, by ensuring that the initial context
%  is reinitialized for each test.  In contrast, most conventional
%  approaches execute tests in a sequence in the same context, giving
%  (for example) the second test an execution context that can in
%  principle depend in part on how the first test may have modified the
%  context.  By ordering (which as we show in
%  Section~\ref{sec:formalism} is strictly more general than
%  isolation), we mean that the sequence in which tests in a test suite
%  are executed can be varied.  A different ordering of test execution
%  can cause a given test to execute in a different context and,
%  perhaps, provide a different result.
%

%\begin{itemize}
%  \item Why do we think knowing about dependencies is important?
%
%Give a neat, clear, if constructed example.
%
%\item Are they a real problem?
%
%  \item What do we do about them?
%\end{itemize}

\section{Related Work}
\label{sec:related}

%Much literature on testing omits details about the structure of a
%test suite; this is reasonable and unsurprising because particulars
%about test suites are often immaterial to the results or discussions.
Denoting a group of test cases as a ``suite of test programs'' began around the
mid-1970's~\cite[p.~217]{brown:CSUR:1974}; similar terms include
``testcase dataset''~\cite{milleretal:ICRS:1975} and ``scenario,''
which an IEEE Standard defines as ``groups of test cases;
synonyms are script, set, or suite''~\cite[p.~10]{IEEE:829-1998}.
Treating test suites explicitly as \emph{mathematical sets} of tests dates at least
to Howden~\cite[p.~554]{howden:ToC:1975} and remains common in the literature.
The execution order of tests in a suite is usually addressed implicitly
or informally, suggesting that the potential of executing a given test
in different contexts is immaterial to those results: that is, test
independence is assumed.

%When a test suite is a set, the test suite passes if and only if all
%the test cases in it pass. 
%Combined with the unordered property of sets this suggests that the
%context in which each test is executed must have no
%effect on its result regardless of the ordering of execution.

%example~\cite[\emph{et alia}]{eldredetal:1978,harroldetal:OOPSLA:2001,staatsetal:ICSE:2011}.  

\subsection{Test Dependence}

In addition to the work by Kapfhammer and
Soffa~\cite{kapfhammeretal:FSE:2003},
there are a handful of categorical references that
acknowledge that tests can
be dependent based on context, suggesting 
ways to document or find situations where the independence
assumption fails to hold.  

The IEEE Standard for Software and System Test
Documentation (829-1998) \S 11.2.7, ``Intercase
Dependencies,'' says in its entirety: ``List the identifiers of
test cases that must be executed prior to this test
case. Summarize
the nature of the dependences''~\cite{IEEE:829-1998}.  The succeeding version of this
standard (829-2008) adds a single sentence: ``If
test cases are documented (in a tool or otherwise) in the order in
which they need to be executed, the Intercase Dependencies for most or
all of the cases may not be needed''~\cite{IEEE:829-2008}.

McGregor and Korson discuss interaction tests that
are intended to identify ``two methods that may directly or indirectly
cause each other to produce incorrect results'' and suggest constructing such
interaction tests by identifying the values shared via parameter passing
between methods
 that two or more test cases share~\cite[p~.69]{mcgregoretal:CACM:1994}.

Bergelson and Exman characterize a form of test dependence
explicitly: given two tests that each pass, the composite
execution of these tests may still
fail~\cite[p.~38]{bergelsonetal:EEE:2006}.  That is, if 
\suite{t_1} executed by itself passes and \suite{t_2} executed by itself passes,
executing the sequence \suite{t_1, t_2} in the same context may fail.

Some practitioners acknowledge test dependence as a possible, albeit low probability, event:
\begin{quote}
Unit testing \dots  
requires that we test the unit in isolation. That is, we
want to be able to say, \emph{to a very high degree of confidence\/} [emphasis added], that
any actual results obtained from the execution of test cases are
purely the result of the unit under test. The introduction of
other units may color our results~\cite{unit-test-def}.
\end{quote}
They further note that other tests, as well as stubs and drivers, are
other units that may ``interfere with the straightforward
execution of one or more test cases.''

A few approaches allow developers to annotate dependent tests and
provide supporting mechanisms to ensure that the test execution framework
respects those annotations.  DepUnit\footnote{\url{https://code.google.com/p/depunit/}}
allows developers to define soft and hard dependences. Soft dependences control
test ordering, while hard dependences in addition control whether specific tests are
run at all.  TestNG\footnote{\url{http://testng.org/}} is a testing framework intended to improve upon JUnit,
and allows dependence annotations and supports a variety of execution policies such as sequential execution
in a single thread, execution of a single test class per thread, etc.\
that respect these dependences.
What distinguishes our work from these approaches is that, while they allow dependences
to be made explicit and respected during execution, they do not help developers
\emph{identify} dependences.  A tool that finds dependences (Section~\ref{sec:tool}) could co-exist
with such frameworks by generating annotations for them.



%Testing frameworks
%%, and 
%%IBM's Rational tool family, and Microsoft's MSDN---
%provide mechanisms
%to help developers define the context for tests more effectively.
%JUnit\footnote{\url{http://junit.org}},
%for example, provides means to
%automatically execute methods (\code{setUp()} and \code{tearDown()} in JUnit
%3.x, and methods annotated with \code{@Before} and \code{@After} in
%JUnit 4.x) to help handle common setup and clean-up tasks between
%tests. Ensuring that these mechanisms are used properly, however, is
%beyond the scope of any framework.
%\todo{DN}{I stole some of the above paragraph from the intro.  We will have
%to accommodate that by removing one and forward/backward references or such.}

%\todo{JW}{During my first read, I'm not sure why this is here and how
%it relates to the previous discussion}\todo{DN}{You're right, I'm taking it
% out.  And both
%of these todos.}


%Characterizing test suites as ``collections'' of test cases is
%increasingly common, especially in descriptions of tools.  For example,
%the Javadoc for the JUnit \code{TestSuite} class includes: ``A TestSuite is a Composite
%of Tests. It runs a collection of test cases.''
%Sun's JUnit Primer says: ``A TestSuite contains a collection of
%tests\dots'', and similar
%terms are found in literature from IBM for the Rational tool family, from Microsoft's MSDN, etc.

%\todo{DN}{Removed the footnote and comments on fail/fail composing to pass.  I
%was wrong and I don't think it matters or is worth confusing things to cover
%it at this point in the paper.}
%\footnote{They
%also assert that if $T_{1}$ and $T_{2}$ each fail in isolation, that their composite
%execution will fail.  This is a variant of the test independence assumption,
%which we also believe is/show is false.  DN: Any examples of fail/fail to pass?
%\todo{KM}{David, I don't believe fail/fail can lead to pass with composition.
%The reason is the following: \\
%Let $\Gamma$ be the initial environment. \\
%T1 == fail implies that R(T1, $\Gamma$) = fail \\
%T2 == fail implies that R(T2, $\Gamma$) = fail \\
%So, whatever order you choose to run T1 and T2, you will at least get one
%failure (the first test that is run) because we know that: \\
%R(\{T1, T2\}, $\Gamma$) = \{fail, *\} and \\
%R(\{T2, T1\}, $\Gamma$) = \{fail, *\}.}}

\subsection{Test Prioritization}

%Test prioritization establishes ``an order for executing test cases in
%a prioritized manner that is most likely to detect software defects
%quickly''~\cite[p.~454]{Singh2001453}.  

Test prioritization seeks to reorder a test suite to detect
software defects more quickly, and is the example of downstream
testing tools that we focus on most closely both because it is
characteristic of the other tools (in the dimensions we address)
and also because of its focus on reordering (perhaps the most common
way to change the execution environment of a test).

Early work in test
prioritization~\cite{Wong:1997:SER:851010.856115,Rothermel:1999:TCP:519621.853398}
laid the foundation for the most commonly used problem definition:
consider the set of all permutations of a test suite and find the best
award value for an objective function over that
set~\cite{Elbaum:2000:PTC:347324.348910}.  The most common objective
functions favor permutations where more faults in the underlying
program  are found with running fewer tests.
%determine the number of tests from the permutations that need
%to be run to detect a set of faults in the underlying program.
A number of results carefully study various prioritization algorithms
empirically, most by Rothermel and colleagues,
spanning over a decade~\cite[\emph{et
alia}]{Rothermel:1999:TCP:519621.853398,Do:2010:ETC:1907658.1908088}.  These evaluations are based in part on the assumption that the set of faults in the underlying program is known beforehand; the possibility that test dependence may unmask additional faults in the program is not studied.
% \todo{DN}{I can't get the et alia above to look right -- where does that semicolon come from?  I'd like it to be [2,9, et alia] or such.
% In a pinch, just include the two numbers.}

Kim and Porter proposed a 
technique that uses the history of test cases run in prior
regression tests to prioritize those that have not yet run for
creating new 
regression test suites~\cite{Kim:2002:HTP:581339.581357}.  Whether
tests were executed is essential to the technique; the results
of specific tests are not.

Echelon defines a
heuristic that exploits both a mapping between tests and
executed program paths and also a binary differencing between two
program versions to select a subset of tests intended to quickly
identify program faults~\cite{Srivastava:2002:EPT:566172.566187}.
Test dependence is not considered in the approach.



Test independence is explicitly asserted as a requirement for
prioritization by Rummel et al.:
\begin{quote}
A test suite contains a tuple of tests \suite{T_1 $\ldots$ T_R} that execute in a specified order.  We require that each test is
independent so that there are no test execution ordering dependencies.  This requirement enables our prioritization algorithm to
re-order the tests in any sequence that maximizes the suite's
ability to isolate defects.  The assumption of test dependence
is acceptable because the JUnit test execution framework
provides \code{setUp} and \code{tearDown} methods that execute before
and after a test case and can be used to clear application state~\cite[p.~1500]{Rummel:2005:TPR:1066677.1067016}.
\end{quote}

%
%when re-executing it in the replay environment. that is the same
%related point as the test factoring work like david's note above.
%%also, 
%I suspect that Elbaum may point that out if he is reviewing this
%paper. I experienced that once, even getting reviews requesting to
%cite a much less related paper of his.



%\subsection*{Test Suites as Collections}


%and it is even the
%characterization of test suites used in Wikipedia (which also contradicts the IEEE Standard by stating that
%``[c]ollections of test cases are sometimes incorrectly termed a test plan, a test script, or even a test scenario.'')~\cite{wiki:test-suites}.
%It has been difficult---and seems unnecessary for our results---to identify precisely when ``collections''
%came into use for test suites.  It seems likely that this arose from the now-common
%terminology for containers that group multiple elements together: sets, lists,
%hash tables, arrays, etc.

\subsection{Syntactic and Semantic Test Dependencies}

\emph{Dependences} in testing are most often considered
to be syntactic dependences between program units, for example
methods calling other methods, and classes using other classes~\cite{bergelsonetal:EEE:2006,briandetal:SEKE:2002}. 
\emph{Syntactic} dependence here means that a unit \code{A} cannot be
compiled and executed without unit \code{B} being present. If we test
such a unit \code{A} without convincing ourselves first that \code{B}
is correct, a test failure for \code{A} is harder to interpret,
because it could just as well indicate a fault in \code{B}.

Zhang and Ryder extend this notion to 
\emph{semantic} dependences,
which is closer to our approach~\cite{zhangetal:TR:2006}. 
They use a notion of
``test outcome'' to determine whether or not syntactically dependent
classes or methods can influence each others results, and consider
only those that can to be semantically dependent.
They give an informal definition of what it means for the execution of a
test to influence the outcome of another test.  We define
this precisely, and we also define manifest test dependence in terms
of execution environments
and test execution order rather than in terms of code use.

Santelices et al.\ define a formal model of how changes might interact
at the source code level and present a technique for detecting such
interactions that arise at
run-time~\cite{Santelices:2010:PDR:1828417.1828487}.  In contrast to
our approach, they identify changes that interact rather than tests
that depend upon each other.

%There are results that identify dependences that
%may surface, for example, when there is aggressive testing of parameter
%settings by a single test case; one (of many) approaches uses
%bounded model checking to vary the parameters~\cite{Sullivan:2004:SAB:1013886.1007531}.



%\todo{DN}{Young says: ``If you are referring to the work I am familiar with, I think what has
%been treated in some depth is combinations of parameter settings in a
%single test case.  That's a sort of dependence as well (e.g., the
%parameters might enable a couple of features that interact in nasty
%ways), but it doesn't involve variations in the ordering of operations
%or test cases.''  I can't find anything on this, but I'm probably searching
%wrong.  If something finds a good citation or two, please include it and write
%something here.  If not, delete this entirely.}

Another kind of dependence helps address
the testing of configurable software, which can be combinatorial with respect to the
set of configurable options~\cite{Cohen97theaetg,Cohen:2003}.  
%In
%practice, there are often far fewer
%configurations that are used and thus should be tested.  This
%often structures the configuration space in a way that allows
%potential dependences to be explored more
%efficiently, as a test suite that binds multiple configuration option values
%can test all configurations that share those settings.
The dependences considered in this approach are not between tests, but rather within
the configuration option space.


%Another set of results searches for
%efficient (small) test suites that aggressively exercise potentially unexpected
%interactions---dependences---among the components of the
%program~\cite{Cohen97theaetg,Cohen:2003}.  These approaches are used for configurable software where
%in principle there are a combinatorial set of instances to test based on variations of the
%configuration options.  The objective is to temper the combinatorial blow-up by identifying
%components sharing some configurable option values; for example, 
%if a block of code is included in such system instances, a test that exercises that block
%can be used as a proxy for interactions between the shared options.
%
%These approaches tend to use covering arrays as a way to determine which of
%the dependencies among components are/are not exercised.  In the case of configurable
%software, there may be a combinatorial number of interactions to be considered.
%These approaches focus on generating a set of effective tests based on program interactions,
%rather than our focus on identifying dependent tests based on ordering, environment, and the
%results of the tests.
%
%
%In principle it could be combinatorial, and the idea of covering
%arrays is that most of the interactions that matter involve just a
%small number of choices.  If there are things that break only for a
%single setting of parameters A, B, C, D, E, F, G, H, we're hosed $A!-(B but
%if something breaks whenever B has value 1 and G has value 2, then
%something like covering arrays has a chance.  



%
%The research we propose is basic research that impacts most
%aspects of testing, ranging from (automatic) test generation through
%regression testing, test case selection and ending with
%considerations on the right test granularity. 
%Similar to the distinguished paper of Staats et
%al.~\cite{staatsetal:ICSE:2011}, we propose to give a rigorous
%foundation to an important aspect of software testing that is
%present but rarely examined in detail in current research.

\input{theory}

\section{Manifestations}
\label{sec:examples}

\input{example-table}

Dependent tests reach beyond theory and appear in real-world programs.  
In some cases, they are intentional, developers are aware of them and
document them, but in other cases they are inadvertent. 
Test dependence can cause problems, not only when test suites are reordered,
but even when they are
executed in the intended order.
This section presents concrete examples of test dependence found in
well-known open source programs. Figure~\ref{fig:example-summary}
summarizes the projects we studied and the results: The table
summarizes the number of tests in the suites produced by the
developers (\emph{MT}), the number of tests we generated automatically
with Randoop (\emph{AT}), and the corresponding numbers of dependent
tests in those test suites (\emph{MTD} and \emph{ATD}, respectively). 
The discussion of the examples in this section is distinguished by
the problems caused by test dependence (\emph{Kind}): when faults are masked because
tests make incorrect assumptions about the global environment (Section~\ref{sec:mask}); 
when tests do not
respect required initialization protocols (Section~\ref{sec:examples:initialization}); and when
undocumented test dependence leads to spurious bug reports (Section~\ref{sec:spurious}).
We also describe dependent tests in an automatically-generated test
suite (Section~\ref{sec:autogen}).
While this list---and associated set of examples---certainly is not exhaustive, it shows that there are
several classes of dependence-related problems that have practical
relevance.



\subsection{Masking Faults}\label{sec:mask}

\emph{Masking} is a particularly perplexing problem caused by
dependence.
The negative effect of masking is that it hides a fault in the
program, \emph{exactly} when the test suite is executed in its default
order. 
%So while manifest dependences can reveal such a problem, the
%underlying fault is in the program and affects first-order testing and
%use of the program.
%In its simplest form, masking occurs when parts of a program or tests assume that
%global state has correctly been initialized before these parts can
%ever execute. When this assumption is incorrect, because
%initialization is not implemented correctly, the interactions of
%different parts of the program might jointly modify the global state
%in ways that lead to intricate and subtle faults.
Masking occurs when a test case $t$ (a) \emph{should}
reveal a fault, (b) only does so when executed in a specific environment
$\env_R$, but (c) tests executed before $t$ in a test suite always
generate environments different from
$\env_R$.
%To express this more
More precisely and without loss of generality, assume any
environment with only a single variable. Then let $T =
\suite{t_1,\dots,t_n}$ be the test suite, and let $t_i, 1 < i \leq n$
be the test that should reveal the fault in environment $\env_R$. A
dependency $t_k \prec t_i, k < i$ masks the fault if
$\exec{\suite{t_1,\dots,t_{i-1}}}{\env_0} \neq \env_R$.

The following two examples illustrate masking in
practice.

\paragraph{CLI: A Long-Standing Bug}

\begin{figure}
% \lstset{language=Java,numbers=left}
%\lstset{language=Java}
\lstset{belowskip=0ex,escapechar={@},numbers=left,numberstyle=\small\ttfamily}
\begin{lstlisting}
public final class OptionBuilder {
  @\itshape\color{red}
  private static String argName;@
  
  private static void reset() {
    ...
    @\itshape\color{red}argName = "arg";@
    ...
  }
   
  public static Option create(String opt){
    Option option = 
      new Option(opt, description);
    ...
    option.setArgName(argName);
    @\itshape\color{red}OptionBuilder.reset();@
    return option;
  }
}
\end{lstlisting}
\caption{Fault-related code from \code{Option\-Build\-er.java}}
\label{fig:option_builder}
\end{figure}

A straightforward example of fault masking occurs in the Apache CLI
library.\footnote{\url{http://commons.apache.org/cli/}}
Two test cases fail when run in isolation:
\code{test13666} and \code{test\-Op\-tion\-With\-out\-Short\-For\-mat2} in test
classes \code{Bugs\-Test} and \code{Help\-For\-mat\-ter\-Test},
respectively.

A detailed study of the code under test revealed that both 
tests fail due to the same hidden dependence. The fault is located in 
\code{OptionBuilder.java} and is caused by not initializing a global
variable early enough.
Figure~\ref{fig:option_builder} shows code that
illustrates the fault. 
%
By default,
\code{argName} is initialized to \code{null} (line 2), and only set to
its intended default value \code{"arg"} by the \code{create()} method
via calling \code{reset()} (line 15). 
Consequently, if clients of CLI do not explicitly initialize the value of
\code{argName}, the first option created will have \code{null} rather
than \code{"arg"} as its argument name.
%In CLI, there are two types of options: options with and without
%argument names. If an option without argument is created first,
%this fault will not lead to a failure, because the \code{null} value
%will be ignored. Consecutive calls to \code{create()} can rely on
%\code{reset()} to establish the desired default value.

Both dependent tests
% \code{test13666} and \code{test27635} (or \code
% {test\-Op\-tion\-With\-out\-Short\-For\-mat2}) 
can reveal this fault, since they create an option with 
the default argument as the first thing in their execution. However,
in the default order of test execution, 
%the test classes \code{BugTest} and \code{Help\-For\-mat\-ter\-Test} both
%contain other 
tests that create options with explicit arguments execute \emph{before} 
these dependent tests.
% \code{test13666}
% and \code{test27635} respectively. 
%Thus, when the tests in these classes are 
%executed in order, the tests executed before \code{test13666}
%and \code{test27635} call \code{create()} 
Thus, the tests that are executed before call \code{create()} at least once, which
sets the default \code{argName} value, thus masking the fault.

%\todo{JW}{The following paragraph is not really necessary here. We
%want to illustrate how these things happen. If we have time and space,
%I think a general discussion section about the relevance of dependence
%might benefit from this, though}
%
%\todo{SZ}{I am on the side of keeping the following text. It is very
%impressive about the effect of dependent tests, making the whole
%test dependence story stronger.}

This fault is reported in the bug
database several times,\footnote{\url{https://issues.apache.org/jira/browse/CLI-26} \url{https://
issues.apache.org/jira/browse/CLI-186} \url{https://issues.apache.org/jira/browse/
CLI-187}} starting on March 13, 2004 (CLI-26). The report is marked as resolved
\emph{three years} later on March 15, 2007, but is then reopened as CLI-186 on
July 31, 2009. On this report, one of the developers commented:
\begin{quote}
I reproduced the issue, it requires a dedicated test case since it is tied to the initialization 
of a static field in OptionBuilder.
\end{quote}
Despite the realization that a dedicated test is required, no such
test was ever created.
About one month later, the bug is duplicated as CLI-187, and the
actual fix happens one 
year later on June 19, 2010, about six years after the bug was first reported (and four years
total on the open-issue list).
%total ``awareness'' of the fault
%of years. %The fix consists of adding the following code to \code{OptionBuilder.java}:

%We associated these dependent tests with a bug first reported by a
%user in 2004, marked as ``resolved''---but not actually resolved---in 2007,
%reopened in 2009 by another user, and finally identified and
%fixed in 2010.  This bug sat on the shelf for about six years and
%would have been identified much earlier and much more easily by
%considering test dependence.

\newcommand{\jodatime}{JodaTime\xspace}
\paragraph{\jodatime: Complex interactions that mask faults}
\label{sec:jodatime}
\input{jodatime}

\subsection{Poor Test Construction}\label{sec:examples:initialization}

Based on our interaction with the \jodatime developers, this last
dependence does not
mask a fault in the program.  Instead, it represents a less severe consequence of test
dependence that suggest that a test, or a test suite, 
has been constructed poorly in some dimension.  While test dependences that mask faults
correspond to a defect
in the program source, these dependences correspond to defects in the test code.
%
%In contrast to the previous section
%where the dependences led to defects in the program source, this section concerns defects
%in the test source.

%In some sense, dependences that are due to missing initialization are
%the dual to dependences that mask faults.  Both reveal problems in source code.
%However, masked faults reside in the program source, while incorrect
%initialization is a fault that resides in the test suite.

The test dependences presented in this section arise due to incorrect initialization
of program state by one or more tests. In the first case,
%
%The following two examples show two common patterns where incorrect
%initialization leads to test dependence.
%The first example is probably the most common. 
tested program code relies on a
global variable that is a part of the environment, but the test does
not properly initialize it.  In the second case, a test should but
does not call
an initialization function before later invocations to a complex library.
This flaw in the test code is masked because the default test suite execution
order includes other tests that initialize the library.  The defect is
inconsequential until and unless the flawed test is reordered, either manually or by
a downstream tool, to execute before any other initializing test.

%The second example employs a common pattern for complex
%libraries that requires a call to an initialization function before
%any other part of the library can be used.
%In both cases, other tests perform the required setup, and because
%they occur before the dependent tests in the normal execution order,
%no tests fail under normal circumstances.

\paragraph{Crystal: Global Variables Considered Harmful}
\input{crystal}

\paragraph{XML Security: Global Initialization}

%Are these test dependences realistic, or part of the modifications SIR
%made? by SZ: they are realistic, we use the original version without
%any modification from SIR people
\input{xmlsecurity}

\subsection{Spurious Bug Reports and Bug Fixes}\label{sec:spurious}
Sometimes developers introduce dependent tests intentionally because it is
easier, more efficient or more convenient to write unit tests for some modules
in that way~\cite{kapfhammeretal:FSE:2003, whittakeretal:2012}.
%DB-testing}.
Even though the developers are aware of these instances
when they create them, this knowledge can get lost, 
and other people who are not aware of these dependences can get confused 
when they run a subset of the test suite that manifests the
dependences.

As a result, they
might report bugs backed by the failing tests, although this is exactly the expected
behavior. If the dependence is not documented clearly and
correctly, it can take a considerable amount of time to work out that
these reported failures are spurious. Or worse, the developers may try
to fix a bug that is not there.

\paragraph{Eclipse SWT: Causing Spurious Bug Reports}
\input{eclipse}

\subsection{Dependence in Auto Generated Tests}
\label{sec:autogen}
\input{beanutils}

%\section{How does the theory relate to our examples}



\section{Algorithm and Implementation}
\label{sec:algorithm-tool}

In this section we present an algorithm and a prototype tool to detect dependent
tests.
In the worst case, a naive, exhaustive search would execute all $n!$
permutations of the test suite to detect dependent tests. While this
is not feasible for realistic $n$, our approximate algorithm uses 
our intuition that many dependences can be found by running only short subsequences of
test suites, and introduces a bound $k$ on the length
of subsequences. That effectively bounds the execution time to
$O(n^k)$, which for small $k$ is tractable. At the same time, our
prototype tool and the experiments we conducted with it, suggest that
many dependences can be found for small $k$.

\subsection{Algorithm}
\label{sec:algorithm}
\input{algorithm}

\subsection{Tool Implementation}
\label{sec:tool}
\input{tool}

%\subsection{Practical Considerations}
%\label{sec:practical}
%\input{practical}

\section{Conclusions}
\label{sec:questions}

\input{openquestions}

\subsection*{Acknowledgments} Bilge Soran was a participant in the project
that led to the initial result.  Yuriy Brun and Colin Gordon provided advice about
the formal notation.  Reid Holmes and Laura Inozemtseva identified the initial \jodatime dependence.  Mark Grechanik, Adam Porter, Michal
Young, and Reid Holmes provided timely and insightful comments on a draft.

\bibliographystyle{plain}
\bibliography{references}

%\section{Unused text snippets}
%\input{snippets}

\end{document}
% vim:wrap:wm=8:bs=2:expandtab:ts=4:tw=70:

