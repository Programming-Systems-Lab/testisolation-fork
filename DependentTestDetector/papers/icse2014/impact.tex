
%\enlargethispage{5pt}

\begin{table}
\centering
\setlength{\tabcolsep}{0.25\tabcolsep}
\begin{tabular}{|l|l|}
%\toprule
\hline
\textbf{Label} & \textbf{Technique Description} \\
\hline
T1 & Randomized ordering \\
T3 & Prioritize on coverage of statements \\
T4 & Prioritize on coverage of statements not yet covered\\
T5 & Prioritize on coverage of methods\\
T7 & Prioritize on coverage of functions not yet covered \\
%\bottomrule
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Five test prioritization techniques used
to assess the impact of dependent tests. These five
techniques are introduced in Table 1
of~\cite{Elbaum:2000:PTC:347324.348910}. (We use
the same labels as in~\cite{Elbaum:2000:PTC:347324.348910}. We did not
implement the other 9 test prioritization techniques
introduced in~\cite{Elbaum:2000:PTC:347324.348910}, since
they require a fault history that is not
available for our subject programs.)
}
\tinyrelax
\label{tab:testprio}
\end{table}



\begin{table}
\centering
% \setlength{\tabcolsep}{1.25\tabcolsep}
\begin{tabular}{|l|c|c|c|c|c|}
%\toprule
\hline
\textbf{Subject Program} & T1 & T3 & T4 & T5 & T7 \\
\hline
\jt& 0 & 0 & 1 & 0 & 0\\
XML Security& 0 & 0 & 0 & 0 & 0 \\
Crystal& 12 & 11 & 16 & 11 & 12 \\
Synoptic& 0 & 0 & 0 & 0 & 0 \\
%\bottomrule
\hline
\textbf{Total} & 12 & 11 & 17 & 11 & 12\\
\hline
%\textbf{Total}& &  & &  \\ 
%\hline
\end{tabular}
\caption{Differences in test results between original and prioritized
  human-written unit test suites.
Each cell shows the number of tests
that do not return the same results as they do when executed
in the default, unprioritized order.
}
\smallrelax
\label{tab:testprioresult}
\end{table}


%  LocalWords:  LOC Joda b609d7d66d T1 T3 T4 T7 unprioritized

\subsubsection{The Impact on Test Prioritization}
\label{sec:impact}

We implemented five test prioritization techniques~\cite{Elbaum:2000:PTC:347324.348910} (summarized in Table~\ref{tab:testprio}) and
applied them to the human-written test suites of our subject programs.


For each test prioritization algorithm, we counted the number
of dependent tests that return different results (pass or fail) in
the prioritized order than they do when executed in the
unprioritized order. Table~\ref{tab:testprioresult} summarizes
the results.

The dependent tests in our subject programs interfere with
\textit{all} five test prioritization techniques in
Table~\ref{tab:testprio}.
%\todo{we need to edit the sentence below, since a reviewer has
%strong opinion about the word "assume"}
This is because all these techniques
implicitly assume that there are no test dependences in
the input test suite. Violation of this assumption, as
happened in real-world test suites, can cause the prioritized suite to fail
even though the original suite passed.

We did not evaluate the effect of test dependence on metrics such as 
APFD~\cite{Rothermel:1999:TCP:519621.853398}; there is no point optimizing
such a metric at the cost of false positives or false negatives.


%One possible to remedy this problem is to 

%as  in the
%4 human-written test suites that can
%theoretically affect the results of these two techniques.
%For test selection, we manually checked whether
%there exists a subsequence of tests
%that do not return the same results
%when executed in the \textit{same} order
%as in the original test suite. 

%For test prioritization, we manually check whether
%there exists a possibly-\textit{reordered}
%subsequence of tests that do not return the
%same results as in the original test suite.

%As a result, 26 out of 29 dependent tests can
%%besides the synoptic test, and 2 jodatime tests
%affect the results of test selection,
%and all 29 dependent tests can affect the results
%of test prioritization.

