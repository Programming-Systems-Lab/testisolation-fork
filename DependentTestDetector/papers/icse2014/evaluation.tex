\section{Empirical Evaluation}
\label{sec:evaluation}


\newcommand{\jt}{Joda-Time\xspace}

\newcommand{\jfreecharttests}{2234\xspace}%change the total num
\newcommand{\jodatimetests}{3875\xspace}
\newcommand{\xmlsecuritytests}{108\xspace}
\newcommand{\crystaltests}{75\xspace}
\newcommand{\synoptictests}{118\xspace}
\newcommand{\totaltests}{4176\xspace}

\newcommand{\jfreechartautotests}{2946\xspace}
\newcommand{\jodatimeautotests}{2639\xspace}
\newcommand{\xmlsecurityautotests}{665\xspace}
\newcommand{\crystalautotests}{3198\xspace}
\newcommand{\synopticautotests}{2467\xspace}
\newcommand{\totalautotests}{8969\xspace}

\input{subject-table}
\input{example-table}

%We evaluated two aspects of \ourtool's
%effectiveness, answering the following
%research questions:
Our evaluation answers the following research questions:

\vspace{-1mm}

\begin{enumerate}
\vspace{-1mm}
\item How many dependent tests can each detection
algorithm detect in
real-world programs (Section~\ref{sec:detectedtests})?

\item How long does each algorithm in \ourtool take to detect dependent
tests (Section~\ref{sec:performance})?

\item Which algorithm is the most cost-effective in detecting
dependent tests (Section~\ref{sec:algcomparison})?
%\item How does \ourtool's effectiveness compare to an alternative
%approach based on test execution order randomization
%(Section~\ref{sec:random})?

\item Can dependent tests interfere with downstream testing techniques
such as test prioritization (Section~\ref{sec:impact})?

\end{enumerate}

\subsection{Subject Programs}


Table~\ref{tab:subjects} lists the programs and
tests used in our evaluation. We used these subject
programs because they have been developed for
a considerable amount of time (3--10 years) and each
of them includes a well-written unit test suite.

\jt~\cite{jodatime} is an open source
date and time library. It is a mature project that
has been under active development
for ten years. XML Security~\cite{xmlsecurity},
included in the SIR repository~\cite{sir},
is a component library implementing XML signature and encryption
standards. 
Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and
identifies textual, compilation, and behavioral conflicts.
Synoptic~\cite{synoptic} is a tool to mine a finite state
machine model representation of a system from logs.
All of the subject programs' test suites are designed to be executed in
a single JVM, rather than requiring separate processes per test case~\cite{vmvm}.

Given the increasing importance of automated test generation
tools~\cite{PachecoLET2007, ZhangSBE2011, Csallner:2004, fraseretal:ISSTA:2011},
we also want to investigate dependent tests in automatically-generated
test suites. For each subject program, we use
Randoop~\cite{PachecoLET2007}, an automated
test generation tool, to create a suite of 5,000 tests.
Randoop automatically drops lexically-redundant tests~\cite[Sec.~III.E]{RobinsonEPAL2011}
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.

We discarded the automatically-generated test suite of
\jt, since many tests in it are non-deterministic ---
they depend on the current time.


\subsection{Evaluation Procedure}

We evaluated each algorithm 
on both the human-written test suite 
and the automatically-generated test suite
of each subject program in Table~\ref{tab:subjects}.

%We run the three algorithms proposed
%in Section~\ref{sec:detecting} on both
%human-written and automatically-generated test suites
%of each subject program.

We ran the randomized algorithm \smalltrialnum, \mediumtrialnum,
and \trialnum times on each test suite, and recorded
the total number of detected dependent tests and time cost
for each setting. The choice of \trialnum times is based
on a practical guideline for using randomized algorithms
in software engineering, as summarized in~\cite{Arcuri:2011}.
%
For the exhaustive $k$-bounded algorithm
and the \dependenceaware{} $k$-bounded algorithm,
we use isolated execution ($k = 1$) and
pairwise execution ($k = 2$). The choice of $k$ is
based on the results of our empirical
study (Section~\ref{sec:study}) that a small $k$
can find most realistic dependent tests.

We provided \ourtool with a list of 39 ``dependence-free'' fields
for the 4 subject programs. This manual step cost
about 30 minutes in total.

%\edit{say a few sentences here about the manual part, such as
%the approximate manual time cost in
%listing the immutable fields.}

We examined each output dependent test manually to make
sure the test dependence is not caused by non-deter\-min\-istic
factors, such as multi-threading.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

Table~\ref{tab:results} summarizes the number of detected
dependent tests and the time cost for each algorithm
in \ourtool.

\subsubsection{Detected Dependent Tests}
\label{sec:detectedtests}

%\todo{I rewrote the below paragraph.  Please review.}

\ourtool detected 29 human-written dependent tests (among which 27
dependent tests were previously unknown) and 1311
automatically-generated dependent tests.  A larger percentage (15\%) of
automatically-generated tests are dependent.  Developers' understanding of
the code, and their goals when writing the tests, help them build
well-structured tests that carefully initialize and destroy the shared
objects they may use.
By contrast,  most automated test generation tools are not ``state-aware'': the
generated tests often ``misuse'' APIs, such as not setting up
the environment correctly.  This misuse may
indicate that the tests are invalid; it may indicate weaknesses, poor
design, or fragility of the APIs; or it may indicate that the human-written
tests have failed to exercise some functionality.

%Among the 29 human-written dependent tests, 23 tests
%can be detected by running in isolation, 2 tests
%require 2 tests to manifest the dependence, and 4
%tests require 3 tests to manifest the dependence.

The root cause of all the detected dependent tests is improper access to
static fields. The XML Security and Crystal developers use more
static fields in the test code,
so there are relatively more dependent tests detected in them.
%This concurs with our findings in Section~\ref{sec:studyfindings}.
%\edit{Can we say anything
%  about the relative frequency of the three?}

The randomized algorithm is surprisingly effective in
detecting dependent tests. In our experiments, when run \trialnum times,
it identifies \textit{more} dependent tests and found all
dependent tests identified by the other two algorithms.
For the human-written
test suites, the randomized algorithm detects 4 more dependent
tests in \jt. These tests only
manifest when a sequence of three tests are run in a specified,
non-default order. Both exhaustive and \dependenceaware{} $k$-bounded
algorithms fail to detect these tests, because
they cannot scale to $k$=3 for 
\jt. Related, the randomized algorithm
detects more dependent
tests in the automatically-generated test suites,
because both the exhaustive and \dependenceaware{} $k$-bounded
failed to scale to $k$=2 for all automatically-generated test suites.

The \dependenceaware{} bounded algorithm found the same
number of dependent tests as the exhaustive bounded algorithm, except
that it missed one dependent test in XML Security's
automatically-generated test suite.
The dependent test was missed because \ourtool
did not track static field access in the \CodeIn{java.security} package
of the JDK, and Javari did not provide annotations for APIs
in that package.


\subsubsection{Performance of \ourtool}
\label{sec:performance}

The time cost of the reversing algorithm is very low, and
the time cost of the randomized algorithm 
is proportional to the run time of the suite and the number of runs.
Overall, the time cost is acceptable for practical use.
For example, the randomized algorithm took around 1.5 hours
to finish 1000 runs,  for \jt's human-written test
suite (\jodatimetests tests).
 
The time cost of running the exhaustive $k$-bounded algorithm
is prohibitive. The JVM initialization time is the main cost.
The exhaustive algorithm failed to
scale to one human-written test suite and all four automatically-generated
test suites when $k$=2, and failed to scale to all test suites
when $k$=3. The primary reason is the large
number of possible test permutations. 
For example, there are 15,011,750 size-2 permutations
for \jt's human-written test suite (\jodatimetests tests),
which would take approximately 58 days to finish.
%For example, running all
%Joda-Time's \jodatimetests human-written
%unit tests when $k$=2 requires running 15,011,750 test pairs, taking
%approximately 

Table~\ref{tab:results} gives an estimated time cost for each
test suite that an algorithm failed to scale to. For each test suite,
we randomly chose 1000 permutations from all
test permutations, executed them, and measured the average time cost
per permutation. Then, we multiple
the average cost by the total number of permutations to estimate
the time cost.

The \dependenceaware{} $k$-bounded algorithm ran about
an order of magnitude faster
than the exhaustive $k$-bounded algorithm,  when $k$=2.
%On average,
%it took 3.3$\times$ and 1.1$\times$ less time to run all
%human-written tests and automatically-generated tests, respectively, when $k$=1.
%The \dependenceaware{} algorithm took an order of magnitude
%less time to run both human-written tests and automatically-generated tests,
%The speedups are largely determined by performance on Joda-Time.
The \dependenceaware{} algorithm helps most when there are relatively many
tests, each one of them relatively small.




\subsubsection{Comparison of Algorithms}
\label{sec:algcomparison}

\todo{I edit the following paragraph}
We next discuss the tradeoffs between choosing different detection
algorithms in \ourtool. Executing
a test suite in its reverse order,
as implemented in the reversing algorithm, is a
simple and useful heuristic to detetect dependent tests. 
%
Although the randomized algorithm
detects the most dependent tests in our subject programs,
it has several limitations. First, there is no guarantee of how many
dependent tests the randomized algorithm can detect. A randomized
algorithm might even produce different results across different runs.
Second, there is no clear stopping criterion
for running the randomized algorithm in practice.
Thus, it can be hard for users
to know how many runs would be enough to find all dependent tests in a test suite.
Third, given an identified dependent test, users
need to inspect the tests executed before it and isolate a minimized
subsequence of tests (either
manually or using an assisting tool~\cite{Zeller:2002}) to understand the dependence root cause.

By contrast, both the exhaustive $k$-bounded and the depend\-ence-aware
$k$-bounded algorithms systematically search for dependent
tests of a given size and do not suffer from the above limitations.
However, the major limitation that prevents them being applied to a
large test suite is the time cost to
explore all possible test permutations.


%\emph{three} tests to manifest. While these are easy to reproduce, we
%did not check that our tool finds them, because the time needed to
%run our naive algorithm on Joda-Time with $k=3$ is measured in months.

\input{impact}

\subsection{Discussion}
\label{sec:expdiscussion}

%\subsubsection{Developers' Reactions}

\noindent \textbf{Developers' Reactions to Dependent Tests.}
We sent the identified human-written dependent tests to the
subject program developers, asking for their feedback.

One dependent test in \jt was previously known
and had already been fixed. \jt's
developers confirmed the other new dependent
tests, and thought that they are due to unintended interactions
in the design of the library.
%
The Crystal developers confirmed that all dependent tests
found in Crystal were not intentional and happened because of dependence
through global variables. The developers considered the
dependent tests undesirable and opened a bug report for
this issue~\cite{crystalbugreport}.
%
The dependent test in Synoptic was previously known.
The developers merged two related tests to fix
the dependent test.
%
%After receiving our reported dependent tests in XML-Security,
The SIR~\cite{sir} maintainers confirmed our reported dependent
tests in XML-Security, and accepted our
suggested patch to fix them. They also highlighted the practice
that tests should \textit{always} ``stand alone''
without dependency on other tests, and characterized that as
``test engineering 101''. 
%They also accepted our suggested
%patch to fix the dependent tests.


%\noindent
%\setlength{\tabcolsep}{0.1\tabcolsep}
%\begin{tabular}{|l|c|}
%\hline
%Downstream Testing Techniques& \#Affected Dependent Tests\\
%\hline
%Test Selection & \\
%Test Prioritization& \\
%\hline
%\end{tabular}

\vspace{1mm}
\noindent \textbf{Threats to Validity}
There are several threats to the validity of our evaluation.
First, the \subjnum open-source
programs and their test suites may not be
representative enough. 
%Thus, we cannot claim the results
%can be generalized to an arbitrary program.
However, these are the first \subjnum subject programs
we tried, and the fact that we found dependent tests
in all of them is suggestive.
Second, in this evaluation, we focus specifically on
the {manifest dependence} between \textit{unit tests}.
We did not investigate possible test dependence that may arise
in other types of tests, such as integration tests.
Third, due to the computational complexity of the general dependent test
detection problem, we do not yet have
empirical data regarding \ourtool's recall and how many dependent
tests exist in a test suite. 
Fourth, we only assessed the
impact of dependent tests on five test prioritization
techniques.
%, and we only evaluated the impact of dependent
%tests on prioritizing unit tests.
Using other test prioritization techniques
might achieve different results. 

%\todo{this paper focuses on manifest test dependence, what about
%potential test dependence, as well as some other cases in the
%study}

\vspace{1mm}

\noindent \textbf{Experimental Conclusions}
We have four chief findings. \textbf{(1)}
Dependent tests do exist in practice, both in
human-written and automatically-generated test suites.
%can contain substantially more dependent tests than a human-written
%test suite.
\textbf{(2)} Like the dependent tests
studied in Section~\ref{sec:study}, the identified
dependent tests in our subject programs reveal weakness
in a test suite rather than defects in the tested code.
\textbf{(3)} Dependent tests can interfere with
test prioritization techniques and cause unexpected output.
\textbf{(4)} 
%\todo{need to figure out a proper english sentence to say like:
%The dependent test detection problem is inherently
%complex, a smarter algorithm may not achieve great results.}
The randomized algorithm is the most cost-effective in
detecting dependent tests.
%, but it has no guarantee
%of the number of dependent tests it can detect.
%Testers
%can use this simple yet effective algorithm in practice.

%\todo{Idea:  how about combining the randomized and dependence-aware
%  algorithms?  That is, generate random orderings that violate dependences;
%  or generate random orderings and discard ones that do not violate
%  dependences.  This probably isn't a great idea, but a reader might think
%  of it.}

%  LocalWords:  Joda dependences multi 67GHz 4GB 2GB tradeoffs subsequence
