\section{Empirical Evaluation}
\label{sec:evaluation}



\newcommand{\jodatimetests}{3875\xspace}
\newcommand{\xmlsecuritytests}{108\xspace}
\newcommand{\crystaltests}{75\xspace}
\newcommand{\synoptictests}{118\xspace}
\newcommand{\totaltests}{4176\xspace}

\newcommand{\jodatimeautotests}{2639\xspace}
\newcommand{\xmlsecurityautotests}{665\xspace}
\newcommand{\crystalautotests}{3198\xspace}
\newcommand{\synopticautotests}{2467\xspace}
\newcommand{\totalautotests}{8969\xspace}

\input{subject-table}
\input{example-table}

%We evaluated two aspects of \ourtool's
%effectiveness, answering the following
%research questions:
Our evaluation answers the following research questions:

\vspace{-1mm}

\begin{enumerate}
\vspace{-1mm}
\item How many dependent tests can each detection
algorithm detect in
real-world programs (Section~\ref{sec:detectedtests})?

\item How long does each algorithm in \ourtool take to detect dependent
tests (Section~\ref{sec:performance})?

\item Which algorithm is the most cost-effective one in detecting
dependent tests (Section~\ref{sec:algcomparison})?
%\item How does \ourtool's effectiveness compare to an alternative
%approach based on test execution order randomization
%(Section~\ref{sec:random})?
\end{enumerate}

\subsection{Subject Programs}


Table~\ref{tab:subjects} lists the programs and
tests used in our evaluation.

Joda-Time~\cite{jodatime} is an open source
date and time library. It is a mature project that
has been under active development
for more than eight years. XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. XML Security is included in
the SIR repository, and has been used widely
as a subject program in the software testing community.
Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and
precisely identifies and reports on textual,
compilation, and behavioral conflicts.
Synoptic~\cite{synoptic} is a tool to mine a finite state
machine model representation of a system from logs.

Each subject program has a human-written JUnit test suite.
In addition, for each subject program, we use
Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to create a suite of 5,000 tests.
Randoop automatically drops textually-redundant tests 
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.


\subsection{Evaluation Procedure}

We evaluate each algorithm 
on both the human-written test suite 
and the automatically-generated test suite
of each subject program in Table~\ref{tab:subjects}.

%We run the three algorithms proposed
%in Section~\ref{sec:detecting} on both
%human-written and automatically-generated test suites
%of each subject program.

We run the randomized algorithm \smalltrialnum, \mediumtrialnum,
and \trialnum times on each test suite, and record
the total number of detected dependent tests and time cost
for each setting. The choice of \trialnum times is based
on the a practical guideline of using randomized algorithm
in software engineering, as summarized in~\cite{Arcuri:2011}.
%
For the exhaustive $k$-bounded algorithm
and the dependence-aware $k$-bounded algorithm,
we use isolated execution ($k = 1$), and
pairwise execution ($k = 2$). The choice of $k$ is
based on the results of our empirical
study (Section~\ref{sec:study}) that a small $k$
can find most realistic test dependences.

\edit{say a few sentences here about the manual part, such as
the approximate manual time cost in
listing the immutable fields.}

Each output dependent test is examined manually to make
sure the test dependence is not caused by non-deterministic
factors, such as multi-threading.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

Table~\ref{tab:results} summarizes the number of detected
dependent tests and the time cost by each algorithm
in \ourtool.

\subsubsection{Detected Dependent Tests}
\label{sec:detectedtests}

A total number of human-written 29 dependent tests and 1311
automatically-generated dependent tests have been detected
by \ourtool. The percentage of dependent tests
in the automatically-generated test suites is significantly
higher than in the human-written test suites. We identified
two possible reasons: first, developers usually have deep
knowledge about the intended purpose of a program when writing tests
for it. Such expertise helps them build relatively well-structured
tests, e.g., tests that carefully initialize and destroy the
shared objects they may use. Second, 
 it is challenging for automated test generation tools to understand
 a program's intended purpose as well as how
specific parts of the program depend on the environment.
Thus, automated tools often do not explicitly generate code that sets up the
environment correctly.  In addition, to cover more
program states, automated tools often incrementally
builds tests on top of the program state after executing
other tests. This could further make a test's result depend on another.


%If, at the same time, other tests are
%generated that as a side e?ect create the needed environment, test dependence ensues



The randomized algorithm is surprisingly effective in
detecting dependent tests. After running \trialnum times,
it identifies \textit{more} dependent tests than the other
two algorithms can find. For the human-written
test suites, the randomized algorithm detects 2 more dependent
tests in Joda-Time. These two tests only
manifest when a sequence of three tests are run in a specified,
non-default order. Both exhaustive and dependence-aware $k$-bounded
algorithm fail to detect these two tests, because
they cannot scale to $k$=3 for 
Joda-Time. Related, the randomized algorithm
detects more dependent
tests in the automatically-generated test suites, for
two reasons. First,
both the exhaustive and dependence-aware $k$-bounded
failed to scale to $k$=2 for all automatically-generated test suites;
and second, some detected dependent tests require executing more than 2 tests
to manifest.

%Compared to the randomized algorithm, On the other hand,

\subsubsection{Performance of \ourtool}
\label{sec:performance}

The time cost of the randomized algorithm 
is proportional to number of runs, and
varies across different subject programs.
 Overall, the time cost is acceptable for practical use.
For example, the randomized algorithm took around 1.5 hours
to finish 1000 runs,  for the largest human-written test
suite (\jodatimetests tests in Joda-Time).
 
The time cost of running the exhaustive $k$-bounded algorithm
is prohibitive. The exhaustive algorithm failed to
scale to one human-written test suite and all four automatically-generated
test suites when $k$=2, and failed to scale to all test suites
when $k$=3. The primary reason is due to the large
number of possible test permutations. 
For example, when $k$=2, there are 15,011,750 permutations
for Joda-Time's human-written test suite (\jodatimetests tests),
which take approximately 1400 hours (58 days) to finish.
%For example, running all
%Joda-Time's \jodatimetests human-written
%unit tests when $k$=2 requires running 15,011,750 test pairs, taking
%approximately 

Table~\ref{tab:results} gives an estimated time cost for each
test suite that an algorithm failed to scale to. For each test suite,
we randomly chose 1000 permutations from all
test permutations, executed them, and measured the average time cost
per permutation. Then, we multiple
the average cost with the total number of permutations to estimate
the time cost.

The dependence-aware $k$-bounded algorithm substantially improves
the efficiency of the exhaustive $k$-bounded algorithm. On average,
it took 3.3$\times$ and 1.6$\times$ less time to run all
human-written tests and automatically-generated tests, respectively, when $k$=1.
The dependence-aware algorithm took an order of magnitude
less time to run both human-written tests and automatically-generated tests,
when $k$=2.
For example, when $k$=2, the exhaustive algorithm took 11,927 seconds
to execute all 11,556 permutations for XML Security's human-written test suite.
By contrast, the dependence-aware algorithm
explored the field read and write information, and
only took 3,322 seconds to execute 2,647 permutations.





\subsubsection{Comparison of Algorithms}
\label{sec:algcomparison}

We next discuss the tradeoffs of choosing different detection
algorithms in \ourtool. Although the randomized algorithm
detects the most dependent tests in our subject programs,
it has several limitations that must be considered
in practice. First, there is no guarantee of how many
dependent tests that the randomized algorithm can detect. A randomized
algorithm might even produce different results across different runs,
and makes reproducing a dependent test harder.
Second, more importantly, there is no clear stop criteria
for running the randomized algorithm.
Thus, it can be hard for users
to know how many runs would be enough for a test suite.
Third, when a dependent test is identified, users
need to inspect tests executed
before the dependent test, and isolate a minimized
subsequence of tests (either
manually or using an assisting tool~\cite{Zeller:2002}) to understand the dependence root cause.

By contrast, both the exhaustive $k$-bounded and the dependence-aware
$k$-bounded algorithms systematically search for dependent
tests, and do not suffer from the above limitations.
However, the major limitation that prevents them being applied to a
large test suite might be the time cost to
explore all possible test permutations.


%\emph{three} tests to manifest. While these are easy to reproduce, we
%did not check that our tool finds them, because the time needed to
%run our naive algorithm on Joda-Time with $k=3$ is measured in months.



\subsection{Discussion}
\label{sec:expdiscussion}

%\subsubsection{Developers' Reactions}

\noindent \textbf{Developers' Reactions.}
We sent the identified human-written dependent tests to the
subject program developers, asking for their feedback.

One dependent test in Joda-Time was previously-known,
and had already been fixed. Joda-Time's
developers confirmed the other new dependent
tests, and thought that they are due to unintended interactions
in the design of the library.

The Crystal developers confirmed that all dependent tests
found in Crystal were not intentional and mostly likely
happened because of the potential dependency
caused by global variables. The developers treat the
dependencies as undesirable and opened a bug report for
this report issue\footnote{\url{https://code.google.com/p/crystalvc/issues/ detail?id=57}}.

The Synoptic developers merged two related tests to fix
the dependent tests.

%After receiving our reported dependent tests in XML-Security,
The SIR~\cite{sir} maintainers confirmed our reported dependent
tests in XML-Security, and accepted our
suggested patch to fix them. They also highlighted the practice
that tests should \textit{always} ``stand alone''
without dependency on other tests, and treated that as
``test engineering 101''. 
%They also accepted our suggested
%patch to fix the dependent tests.

\vspace{1mm}
\noindent \textbf{Threats to Validity}
There are several threats to validity in our evaluation.
First, the \subjnum open-source
programs and their test suites may not be
representative enough. Thus, we cannot claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
the {manifest dependence} between \textit{unit tests}.
We did not investigate possible test dependence that may arise
in other types of tests, such as integration tests
or system tests.
Third, due to the computational complexity of the general dependent test
detection problem, we do not yet have
empirical data of how many dependent
tests exist in a test suite and how many percentages of dependence tests
\ourtool can catch.  Giving a reasonable estimation is one of our future work.

%\todo{this paper focuses on manifest test dependence, what about
%potential test dependence, as well as some other cases in the
%study}

\vspace{1mm}

\noindent \textbf{Experimental Conclusions}
We have three chief findings: \textbf{(1)}
Dependent tests do exist in practice. However,
they may not be obvious to be identified
unless been explicitly searched for.
An automatically-generated test suite can contain
substantially more dependent tests than a human-written
test suite.
\textbf{(2)} Like the dependent tests
studied in Section~\ref{sec:study}, the identified
dependent tests in our subject programs reveal weakness
in a test suite rather than defects in the tested code.
And \textbf{(3)} In terms
of the number of detected dependent tests
and the time cost, the randomized algorithm is the
most cost-effective one.
%However, it does not
%have any guarantee in 


%  LocalWords:  Joda dependences multi 67GHz 4GB 2GB tradeoffs subsequence
