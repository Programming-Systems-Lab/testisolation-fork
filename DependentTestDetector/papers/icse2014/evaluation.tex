\section{Empirical Evaluation}
\label{sec:evaluation}

To show the effectiveness of our proposed
dependent test detection algorithms, we conducted
an evaluation on \todo{XXX} open-source programs (Table~\ref{tab:subjects}).
In our evaluation, we seek to answer the following research questions:

\begin{itemize}
\item \textbf{RQ1:} How effectively do our algorithms detect
dependent tests?
\item \textbf{RQ2:} How do the proposed static and dynamic program analyses
in the improved algorithm increase the algorithm efficiency?
\end{itemize}

\todo{check the research questions above? any more?}

\subsection{Subject Programs}

\input{subject-table}
\input{example-table}

Table~\ref{tab:subjects} summarizes the programs used in our evaluation.

JodaTime~\cite{jodatime} is an open source
date and time library intended to improve upon the weaknesses of the
date and time facilities provided by the standard JDK.
%written to enhance the capabilities provided by the standard
%JDK such as allowing multiple calendar systems. 
It is a mature project that has been under active development
for more than eight years.

XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. Each released
version of XML Security has a human-written JUnit test suite that
achieves fairly high statement coverage.


Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and precisely identifies and reports on textual, compilation, and behavioral conflicts.

Beanutils~\cite{beanutils}
is a library that provides services for collections of
Java beans. 

\todo{say why choose these subject programs}

\subsection{Evaluation Procedure}

\todo{including manual tests and auto-generated tests}

We used the prototype to detect dependent
tests in the subject programs in Table~\ref{tab:subjects} using isolated execution ($k = 1$)
and pairwise execution ($k = 2$).

While we believe that most test dependences can be found with small
$k$. This is in part because the set of dependent tests that can be
found with a bound $k$ is always a subset of the set of dependent
tests that can be found with any bound $k' > k$. Additionally, our
empirical study (Section~\ref{sec:study}) indicates that small $k$
find many dependences. 

%, while larger $k$ do not. However, in principle
%it is conceivable
%that any number of chain dependences with chains longer than any tried $k$ exist
%in all the libraries we analyzed.


For each subject program, we also used Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to generate 5,000 tests
for each program, and then drop what it considers to be redundant
tests. Then, we applied our prototype to detect dependent tests in these automatically-generated
test suites.

\todo{add more about how to identify dependent tests in the tool's output}

%The discussion of the examples in this section is distinguished by
%the problems caused by test dependence (\emph{Kind}): when faults are masked because
%tests make incorrect assumptions about the global environment (Section~\ref{sec:mask}); 
%when tests do not
%respect required initialization protocols (Section~\ref{sec:examples:initialization}); and when
%undocumented test dependence leads to spurious bug reports (Section~\ref{sec:spurious}).
%We also describe dependent tests in an automatically-generated test
%suite (Section~\ref{sec:autogen}).


\subsection{Results}

\todo{show the basic results here, i.e., the num
of dependent tests found. Its consequence is discussed
below.}

\subsubsection{RQ1: Algorithm Effectiveness}

All the dependent tests reported in Figure~\ref{fig:example-summary},
except for two dependent tests in JodaTime and the dependences in SWT, 
can already be found by isolated execution.
 
During manual bug diagnosis in JodaTime, we identified two test dependences that require
\emph{three} tests to manifest. While these are easy to reproduce, we
did not check that our tool finds them, because the time needed to
run our naive algorithm on JodaTime with $k=3$ is measured in months.

\todo{say this concurs with the study findings}

\subsubsection{RQ2: Improvement from Program Analyses}

\subsection{Dependence in Human-written Tests}

\todo{The following text needs to be re-organized.
Be consistent to the categories used in the study section}

Most of the test dependences we found in human-written
test suites arise due to incorrect initialization
of program state by one or more tests. Such test
dependence often suggests that a test, or a test suite, 
has been constructed poorly in some dimension. 
While test dependences that mask faults
correspond to a defect in the program source,
these dependences correspond to defects in the test code.

There are two common patterns where incorrect
initialization leads to test dependence.
The first example is probably the most common. 
The tested program code relies on a
global variable that is a part of the environment, but the test does
not properly initialize it.  In the second case, a test should but
does not call
an initialization function before later invocations to a complex library.
This flaw in the test code is masked because the default test suite execution
order includes other tests that initialize the library.  The defect is
inconsequential until and unless the flawed test is reordered, either manually or by
a downstream tool, to execute before any other initializing test.

We next show several examples to illustrate test dependence in human-written
test suites.

\newcommand{\jodatime}{JodaTime\xspace}
%\paragraph{\jodatime: Complex interactions that mask faults}
%\label{sec:jodatime}
%\input{jodatime}
\todo{here is a jodatime example commented out in the source, that dependence
masks faults. It is very verbose, should we discuss that in the paper.}

%\subsubsection{Poor Test Construction}\label{sec:examples:initialization}

%Based on our interaction with the \jodatime developers, this last
%dependence does not
%mask a fault in the program.  
%
%In contrast to the previous section
%where the dependences led to defects in the program source, this section concerns defects
%in the test source.

%In some sense, dependences that are due to missing initialization are
%the dual to dependences that mask faults.  Both reveal problems in source code.
%However, masked faults reside in the program source, while incorrect
%initialization is a fault that resides in the test suite.

%
%The following two examples show two common patterns where incorrect
%initialization leads to test dependence.
%The first example is probably the most common. 

%The second example employs a common pattern for complex
%libraries that requires a call to an initialization function before
%any other part of the library can be used.
%In both cases, other tests perform the required setup, and because
%they occur before the dependent tests in the normal execution order,
%no tests fail under normal circumstances.

\paragraph{Crystal: Global Variables Considered Harmful}
\input{crystal}

\paragraph{XML Security: Global Initialization}

%Are these test dependences realistic, or part of the modifications SIR
%made? by SZ: they are realistic, we use the original version without
%any modification from SIR people
\input{xmlsecurity}


\subsection{Dependence in Auto Generated Tests}
\label{sec:autogen}
\input{beanutils}

%\section{How does the theory relate to our examples}

\subsection{Experimental Discussion}

\subsubsection{Threats to Validity}

There are two major threats to validity in our evaluation.
First, the \todo{NUM} open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
dependence between unit tests.
\todo{only consider unit tests, no system tests.} 

\subsubsection{Conclusions}


We have XXX chief findings: \textbf{(1)}
\textbf{(2)}, \textbf{(3)} ...
The examples where dependence identified weaknesses in the tests themselves
are even less likely to be observed.  
Our prototype tool shows
the potential for revealing dependences, allowing
developers to observe them and make conscious decisions about how, or even whether,
to deal with the dependences. 
