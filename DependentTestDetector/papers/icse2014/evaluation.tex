\section{Empirical Evaluation}
\label{sec:evaluation}

To show the effectiveness of our proposed
dependent test detection algorithms, we conducted
an evaluation on \todo{XXX} open-source programs (Table~\ref{tab:subjects}).
In our evaluation, we seek to answer the following research questions:

\begin{itemize}
\item \textbf{RQ1:} How effectively do our algorithms detect
dependent tests?
\item \textbf{RQ2:} How do the proposed static and dynamic program analyses
in the improved algorithm increase the algorithm efficiency?
\end{itemize}

\todo{check the research questions above? any more?}
\todo{should we add human-written tests v.s. automatically-generated tests?
I prefer not, since it does not lead to much insight.}

\subsection{Subject Programs}

\input{subject-table}
\input{example-table}

Table~\ref{tab:subjects} summarizes the programs used in our evaluation.

JodaTime~\cite{jodatime} is an open source
date and time library intended to improve upon the weaknesses of the
date and time facilities provided by the standard JDK.
%written to enhance the capabilities provided by the standard
%JDK such as allowing multiple calendar systems. 
It is a mature project that has been under active development
for more than eight years.

XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. Each released
version of XML Security has a human-written JUnit test suite that
achieves fairly high statement coverage.


Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and precisely identifies and reports on textual, compilation, and behavioral conflicts.

\todo{Add Derby, and Synoptic}

\todo{say why choose these subject programs}

\subsection{Evaluation Procedure}

We use \ourtool to detect dependent
tests among the human-written test suite 
associated with each subject program in Table~\ref{tab:subjects}.

We also evaluate \ourtool on automatically-generated
test suites. For each subject program, we use
Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to generate 5,000 tests.
Randoop drops tests that it considers to be redundant,
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.
Then, we applied \ourtool to detect dependent tests
in these automatically-generated test suites.

For both human-written and automatically-generated
test suites, we use isolated execution ($k = 1$)
and pairwise execution ($k = 2$), since our empirical
study (Section~\ref{sec:study}) indicates that  a small $k$
can find most test dependences. 

Each output dependent test is examined manually to make
sure its different results are not caused by non-deterministic
factors, such as multi-threading.

%While we believe that most test dependences can be found with small
%$k$. This is in part because the set of dependent tests that can be
%found with a bound $k$ is always a subset of the set of dependent
%tests that can be found with any bound $k' > k$. 
%Additionally, our
%empirical study 

%, while larger $k$ do not. However, in principle
%it is conceivable
%that any number of chain dependences with chains longer than any tried $k$ exist
%in all the libraries we analyzed.


%The discussion of the examples in this section is distinguished by
%the problems caused by test dependence (\emph{Kind}): when faults are masked because
%tests make incorrect assumptions about the global environment (Section~\ref{sec:mask}); 
%when tests do not
%respect required initialization protocols (Section~\ref{sec:examples:initialization}); and when
%undocumented test dependence leads to spurious bug reports (Section~\ref{sec:spurious}).
%We also describe dependent tests in an automatically-generated test
%suite (Section~\ref{sec:autogen}).


\subsection{Results}

\todo{show the basic results here, i.e., the num
of dependent tests found. Its consequence is discussed
below.}

\subsubsection{RQ1: Algorithm Effectiveness}

\todo{How many} dependent tests reported in Table~\ref{fig:example-summary}
are found by isolated execution, and \todo{how many} dependent
tests reported in the same are found by pairwise execution.

\todo{does two algorithms find the same number of tests?}
 
\todo{say this concurs with the study findings}

Characteristics of the identified dependent tests are discussed
in Section~\ref{sec:humantest} and~\ref{sec:autotest} in details.

%During manual bug diagnosis in JodaTime, we identified two test dependences that require
%\emph{three} tests to manifest. While these are easy to reproduce, we
%did not check that our tool finds them, because the time needed to
%run our naive algorithm on JodaTime with $k=3$ is measured in months.

\subsubsection{RQ2: Improvement from Program Analyses}

\todo{How much improvement can we gain?}

\subsection{Dependence in Human-written Tests}
\label{sec:humantest}

\todo{The following text needs to be re-organized.
Be consistent to the categories used in the study section}

Most of the test dependences we found in human-written
test suites arise due to incorrect initialization
of program state by one or more tests. Such test
dependence often suggests defects in the
test code. 
%that a test, or a test suite, 
%has been constructed poorly in some dimension. 

%While test dependences that mask faults
%correspond to a defect in the program source,
%these dependences correspond to defects in the test code.

There are two common patterns where incorrect
initialization leads to test dependence.
The first pattern is probably the most common. 
The test code relies on a
global variable, but the test itself does
not properly initialize it.  In the second pattern,
a test should but does not call
an initialization function before later invocations to a complex library.
This flaw in the test code is masked because the default test suite execution
order includes other tests that initialize the library.  The defect is
inconsequential until and unless the flawed test is reordered, either manually or by
a downstream tool, to execute before any other initializing test.
We next example dependent tests for these two patterns.

%We next show several examples to illustrate test dependence in human-written
%test suites.


\newcommand{\jodatime}{JodaTime\xspace}
%\paragraph{\jodatime: Complex interactions that mask faults}
%\label{sec:jodatime}
%\input{jodatime}

%\todo{here is a jodatime example commented out in the source, that dependence
%masks faults. It is very verbose, should we discuss that in the paper.}

%\subsubsection{Poor Test Construction}\label{sec:examples:initialization}

%Based on our interaction with the \jodatime developers, this last
%dependence does not
%mask a fault in the program.  
%
%In contrast to the previous section
%where the dependences led to defects in the program source, this section concerns defects
%in the test source.

%In some sense, dependences that are due to missing initialization are
%the dual to dependences that mask faults.  Both reveal problems in source code.
%However, masked faults reside in the program source, while incorrect
%initialization is a fault that resides in the test suite.

%
%The following two examples show two common patterns where incorrect
%initialization leads to test dependence.
%The first example is probably the most common. 

%The second example employs a common pattern for complex
%libraries that requires a call to an initialization function before
%any other part of the library can be used.
%In both cases, other tests perform the required setup, and because
%they occur before the dependent tests in the normal execution order,
%no tests fail under normal circumstances.

%\paragraph{Crystal: Global Variables Considered Harmful}
%The latest release of Crystal contains 81 human-written unit tests. 
%Of those, 75 are fully automated, and 18 exhibit
%dependences.
\ourtool identified 18 dependent tests in Crystal.
All these dependent tests
are caused by incomplete initialization of the
environment when testing methods of three distinct classes
(\code{Data\-Source, Lo\-cal\-State\-Re\-sult, Con\-flict\-Daemon}).
In all cases, one test initializes the environment correctly, and all
other tests rely on that test executing first. 
Crystal developers confirmed that such test dependence was not
intentional and most likely happened because they were not
aware of the potential dependency caused by the use of global
variables. Since we pointed out this problem, the developers treat the
dependencies as undesirable and opened a bug report to have this issue
resolved.\footnote{\url{https://code.google.com/p/crystalvc/issues/detail?id=57}}


%\paragraph{XML Security: Global Initialization}
%Are these test dependences realistic, or part of the modifications SIR
%made? by SZ: they are realistic, we use the original version without
%any modification from SIR people
%\input{xmlsecurity}

The XML Security program is incorporated in the Soft\-ware-artifact
Infrastructure Repository (SIR)~\cite{sir}. \ourtool identified
3 dependent tests (\code{test\_Y1}, \code{test\_Y2}, and \code{test\_Y3}
in class \code{ExclusiveC14NInterop}), which share the same root
cause of the dependence: before any
method in the library can be used, the global initialization function 
\code{Init.init()} has to be called but is not called. The
\code{Init.init()} method initializes the static field that
each dependent test relies on.

For dependent tests in a human-written test suite, we speculate that
developers either simply forgot to
initialize the tests properly, or expected that these tests would always execute
in the order defined
in the test suite.

 %at least two out of the four versions contain dependent tests.
 %Specifically, in versions 1.0.4 and 1.0.5d2, \code{test\_Y1}, \code{test\_Y2}, and \code{test\_Y3}
%in class \code{ExclusiveC14NInterop} show dependent behavior.
%Since the dependences are the same in both versions, in the further
%discussion and in Figure~\ref{fig:example-summary}, we consider only
%version 1.0.4.



\subsection{Dependence in Auto Generated Tests}
\label{sec:autotest}

Dependent tests in an automatically-generated test suite
are more common.  As shown in Table~\ref{tab:results}, in most
projects, a large fraction of the remaining tests are dependent.
%while
%in some projects, there are almost no dependent tests.

\input{beanutils}

%\section{How does the theory relate to our examples}

\subsection{Experimental Discussion}

\subsubsection{Threats to Validity}

There are two major threats to validity in our evaluation.
First, the \todo{NUM} open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
dependence between unit tests.
\todo{only consider unit tests, no system tests.} 

\subsubsection{Conclusions}


We have XXX chief findings: \textbf{(1)}
\textbf{(2)}, \textbf{(3)} ...
The examples where dependence identified weaknesses in the tests themselves
are even less likely to be observed.  
Our prototype tool shows
the potential for revealing dependences, allowing
developers to observe them and make conscious decisions about how, or even whether,
to deal with the dependences. 
