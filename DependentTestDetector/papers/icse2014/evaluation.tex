\section{Empirical Evaluation}
\label{sec:evaluation}

To show the effectiveness of our proposed
dependent test detection algorithms, we conducted
an evaluation on \todo{XXX} open-source programs (Table~\ref{}).
In our evaluation, we seek to answer the following research questions:

\begin{itemize}
\item \textbf{RQ1:} How effectively do our algorithms detect
dependent tests?
\item \textbf{RQ2:} How do the proposed static and dynamic program analyses
in \todo{algorithm-name} improve the algorithm efficiency?
\end{itemize}

\subsection{Subject Programs}

\input{subject-table}
\input{example-table}

Figure~\ref{fig:example-summary}
summarizes the projects we studied and the results: The table
summarizes the number of tests in the suites produced by the
developers (\emph{MT}), the number of tests we generated automatically
with Randoop (\emph{AT}), and the corresponding numbers of dependent
tests in those test suites (\emph{MTD} and \emph{ATD}, respectively). 
The discussion of the examples in this section is distinguished by
the problems caused by test dependence (\emph{Kind}): when faults are masked because
tests make incorrect assumptions about the global environment (Section~\ref{sec:mask}); 
when tests do not
respect required initialization protocols (Section~\ref{sec:examples:initialization}); and when
undocumented test dependence leads to spurious bug reports (Section~\ref{sec:spurious}).
We also describe dependent tests in an automatically-generated test
suite (Section~\ref{sec:autogen}).
While this list---and associated set of examples---certainly is not exhaustive, it shows that there are
several classes of dependence-related problems that have practical
relevance.


\subsection{Evaluation Procedure}

\todo{including manual tests and auto-generated tests}

We used the prototype to verify the dependent tests reported by
users, developers, other researchers, and us, and to find new dependent
tests in the
example programs in Section~\ref{sec:examples} using isolated execution ($k = 1$)
and pairwise execution ($k = 2$).

All the dependent tests reported in Figure~\ref{fig:example-summary},
except for two dependent tests in JodaTime and the dependences in SWT, 
can already be found by isolated execution. Since we could not run the
test suite of SWT, we could not check these dependences with our tool.
During manual bug diagnosis in JodaTime, we identified two test dependences that require
\emph{three} tests to manifest. While these are easy to reproduce, we
did not check that our tool finds them, because the time needed to
run our naive algorithm on JodaTime with $k=3$ is measured in months.

While we believe that most test dependences can be found with small
$k$. This is in part because the set of dependent tests that can be
found with a bound $k$ is always a subset of the set of dependent
tests that can be found with any bound $k' > k$. Additionally, our
intuition and preliminary exploration seem to indicate that small $k$
find many dependences, while larger $k$ do not. However, in principle
it is conceivable
that any number of chain dependences with chains longer than any tried $k$ exist
in all the libraries we analyzed.

\subsection{Results}

\todo{show the basic results here, i.e., the num
of dependent tests found. Its consequence is discussed
below.}

\subsubsection{RQ1: Algorithm Effectiveness}

\subsubsection{RQ2: Improvement from Program Analyses}

\subsection{Causes of Dependent Tests}

\todo{The following text is too verbose. Note be
consistent to the categories used in the study section}


\newcommand{\jodatime}{JodaTime\xspace}
\paragraph{\jodatime: Complex interactions that mask faults}
\label{sec:jodatime}
\input{jodatime}

\subsubsection{Poor Test Construction}\label{sec:examples:initialization}

Based on our interaction with the \jodatime developers, this last
dependence does not
mask a fault in the program.  Instead, it represents a less severe consequence of test
dependence that suggest that a test, or a test suite, 
has been constructed poorly in some dimension.  While test dependences that mask faults
correspond to a defect
in the program source, these dependences correspond to defects in the test code.
%
%In contrast to the previous section
%where the dependences led to defects in the program source, this section concerns defects
%in the test source.

%In some sense, dependences that are due to missing initialization are
%the dual to dependences that mask faults.  Both reveal problems in source code.
%However, masked faults reside in the program source, while incorrect
%initialization is a fault that resides in the test suite.

The test dependences presented in this section arise due to incorrect initialization
of program state by one or more tests. In the first case,
%
%The following two examples show two common patterns where incorrect
%initialization leads to test dependence.
%The first example is probably the most common. 
tested program code relies on a
global variable that is a part of the environment, but the test does
not properly initialize it.  In the second case, a test should but
does not call
an initialization function before later invocations to a complex library.
This flaw in the test code is masked because the default test suite execution
order includes other tests that initialize the library.  The defect is
inconsequential until and unless the flawed test is reordered, either manually or by
a downstream tool, to execute before any other initializing test.

%The second example employs a common pattern for complex
%libraries that requires a call to an initialization function before
%any other part of the library can be used.
%In both cases, other tests perform the required setup, and because
%they occur before the dependent tests in the normal execution order,
%no tests fail under normal circumstances.

\paragraph{Crystal: Global Variables Considered Harmful}
\input{crystal}

\paragraph{XML Security: Global Initialization}

%Are these test dependences realistic, or part of the modifications SIR
%made? by SZ: they are realistic, we use the original version without
%any modification from SIR people
\input{xmlsecurity}


\subsubsection{Dependence in Auto Generated Tests}
\label{sec:autogen}
\input{beanutils}

%\section{How does the theory relate to our examples}

\subsection{Experimental Discussion}

\subsubsection{Threats to Validity}

There are two major threats to validity in our evaluation.
First, the \todo{NUM} open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
dependence between unit tests.
\todo{only consider unit tests, no system tests.} 

\subsubsection{Conclusions}


We have XXX chief findings: \textbf{(1)}
\textbf{(2)}, \textbf{(3)} ...
The examples where dependence identified weaknesses in the tests themselves
are even less likely to be observed.  
Our prototype tool shows
the potential for revealing dependences, allowing
developers to observe them and make conscious decisions about how, or even whether,
to deal with the dependences. 
