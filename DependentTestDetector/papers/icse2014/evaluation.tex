\section{Empirical Evaluation}
\label{sec:evaluation}



\newcommand{\jodatimetests}{3875\xspace}
\newcommand{\xmlsecuritytests}{108\xspace}
\newcommand{\crystaltests}{75\xspace}
\newcommand{\synoptictests}{118\xspace}
\newcommand{\jodatimeautotests}{2639\xspace}
\newcommand{\xmlsecurityautotests}{665\xspace}
\newcommand{\crystalautotests}{3198\xspace}
\newcommand{\synopticautotests}{2467\xspace}

\input{subject-table}
\input{example-table}

We evaluated two aspects of \ourtool's
effectiveness, answering the following
research questions:

\begin{enumerate}
\item How many dependent tests can \ourtool detect in
real-world programs by each detection algorithm (Section~\ref{sec:detectedtests})?
\item How long does each algorithm in \ourtool take to detect dependent
tests (Section~\ref{sec:performance})?

\item Which algorithm is the most cost-effective one in detecting
dependent tests (Section~\ref{sec:algcomparison})?
%\item How does \ourtool's effectiveness compare to an alternative
%approach based on test execution order randomization
%(Section~\ref{sec:random})?
\end{enumerate}

\subsection{Subject Programs}


Table~\ref{tab:subjects} summarizes the programs and
tests used in our evaluation.

JodaTime~\cite{jodatime} is an open source
date and time library. It is a mature project that
has been under active development
for more than eight years. XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. XML Security is included in
the SIR repository, and has been used widely
as a subject program in the software testing community.
Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and
precisely identifies and reports on textual,
compilation, and behavioral conflicts.
Synoptic~\cite{synoptic} is a tool to mine a finite state
machine model representation of a system from logs.

Each subject program has a human-written JUnit test suite.
In addition, for each subject program, we use
Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to create a suite of 5,000 tests.
Randoop automatically drops textually-redundant tests 
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.


\subsection{Evaluation Procedure}

We use \ourtool to detect dependent
tests among both the human-written test suite 
and the automatically-generated test suite
for each subject program in Table~\ref{tab:subjects}.


For both human-written and automatically-generated
test suites, we run the three algorithms proposed
in Section~\ref{sec:detecting} separately.

We run the randomzied algorithm repeatedly until no
new dependent tests are identified in the last
10 iterations.

When running the exhausitive $k$-bounded algorithm and dependence-aware
$k$-bounded algorithm,
we use isolated execution ($k = 1$),
pairwise execution ($k = 2$), and
3-wise execution ($k = 3$). The choice of $k$ is
based on the results of our empirical
study (Section~\ref{sec:study}) that a small $k$
can find most test dependences. 

%\todo{The experimental methodology might be different
%for the improved algorithm, which can incoporate user
%annoations, and is not restricted to k = 1, or 2}

%When comparing \ourtool with test execution order
%randomization, we use Random jUnit
%executor~\cite{randomjunit}, an existing tool
%that shuttles the whole test suite and executes the shuttled
%test suite. We execute each human-written
%and automatically-generated test suite with Random
%jUnit executor until reaching a plateau, i.e.,
%no more dependent tests are identified in the past
%10 executions.

Each output dependent test is examined manually to make
sure the test dependence is not caused by non-deterministic
factors, such as multi-threading.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7

\subsection{Results}

Table~\ref{tab:results} summarizes the detected
dependent tests by \ourtool.

\subsubsection{Detected Dependent Tests}
\label{sec:detectedtests}

\todo{How many dependent tests are detected by both
algorithms?}

\subsubsection{Performance of \ourtool}
\label{sec:performance}

\todo{show the time cost}
 
\todo{summarize the findings and show
whether this concurs with the study findings}

\subsubsection{Comparison of Algorithms}
\label{sec:algcomparison}

\todo{which one is the most cost-effective one, i speculate that it might be the randomized one}

%\emph{three} tests to manifest. While these are easy to reproduce, we
%did not check that our tool finds them, because the time needed to
%run our naive algorithm on JodaTime with $k=3$ is measured in months.



\subsection{Discussion}
\label{sec:expdiscussion}

\subsubsection{Developers' Reactions}

We sent the identified human-written dependent tests to the
subject program developers, asking for their feedback.

One dependent test in JodaTime was previoulsy-known,
and had already been fixed. JodaTime's
developers confirmed the other two new dependent
tests, and thought that they is due to interactions
that are not intended in the design of the library.

The Crystal developers confirmed that all dependent tests
found in Crystal were not intentional and mostly likely
happended because they were not aware of the potential dependency
caused by global variables. The developers treat the
dependencies as undesirable and opened a bug report for
this report issue\footnote{\url{https://code.google.com/p/crystalvc/issues/ detail?id=57}}.

The Synoptic developers merged two related tests to fix
the dependent tests.

After receiving our reported dependent tests in XML-Security,
the SIR~\cite{sir} maintainers 
confirmed that tests should \textit{always} ``stand alone''
without dependency on other tests, and treated that as
``test engineering 101''. They are working on fixing the reported
test dependence.

\subsubsection{Threats to Validity}

There are several threats to validity in our evaluation.
First, the \subjnum open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
the {manifest dependence} between \textit{unit tests}.
It is unclear how test dependence will arise
in other types of tests, such as integration tests
or system tests.
Third, due to the computational complexity of the general dependent test
detection problem, we do not yet have
empirical data of how many dependent
tests exist in a test suite and how many percentages of dependence tests
\ourtool can catch.  Giving a reasonable estimation is one of our future work.

%\todo{this paper focuses on manifest test dependence, what about
%potential test dependence, as well as some other cases in the
%study}

\subsubsection{Experimental Conclusions}


We have three chief findings: \textbf{(1)}
Dependent tests are not obvious to be identified
unless been explicitly searched for.
An automatically-generated test suite can contain
substantially more dependent tests than a human-written
test suite.
\textbf{(2)} Like the dependent tests
studied in Section~\ref{sec:study}, the identified
dependent tests in our subject programs reveal weakness
in a test suite rather than defect in the tested code.
And \textbf{(3)} In terms
of the number of detected dependent tests
and time cost, the randomized algorithm is the
most cost-effective one.
%However, it does not
%have any guarantee in 

