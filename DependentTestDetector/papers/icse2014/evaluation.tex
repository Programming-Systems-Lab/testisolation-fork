\section{Empirical Evaluation}
\label{sec:evaluation}



\newcommand{\jodatimetests}{3875\xspace}
\newcommand{\xmlsecuritytests}{108\xspace}
\newcommand{\crystaltests}{75\xspace}
\newcommand{\synoptictests}{118\xspace}
\newcommand{\totaltests}{4176\xspace}

\newcommand{\jodatimeautotests}{2639\xspace}
\newcommand{\xmlsecurityautotests}{665\xspace}
\newcommand{\crystalautotests}{3198\xspace}
\newcommand{\synopticautotests}{2467\xspace}
\newcommand{\totalautotests}{8969\xspace}

\input{subject-table}
\input{example-table}

%We evaluated two aspects of \ourtool's
%effectiveness, answering the following
%research questions:
Our evaluation answers the following research questions:

\vspace{-1mm}

\begin{enumerate}
\item How many dependent tests can each detection
algorithm detect in
real-world programs (Section~\ref{sec:detectedtests})?

\item How long does each algorithm in \ourtool take to detect dependent
tests (Section~\ref{sec:performance})?

\item Which algorithm is the most cost-effective one in detecting
dependent tests (Section~\ref{sec:algcomparison})?
%\item How does \ourtool's effectiveness compare to an alternative
%approach based on test execution order randomization
%(Section~\ref{sec:random})?
\end{enumerate}

\subsection{Subject Programs}


Table~\ref{tab:subjects} lists the programs and
tests used in our evaluation.

JodaTime~\cite{jodatime} is an open source
date and time library. It is a mature project that
has been under active development
for more than eight years. XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. XML Security is included in
the SIR repository, and has been used widely
as a subject program in the software testing community.
Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and
precisely identifies and reports on textual,
compilation, and behavioral conflicts.
Synoptic~\cite{synoptic} is a tool to mine a finite state
machine model representation of a system from logs.

Each subject program has a human-written JUnit test suite.
In addition, for each subject program, we use
Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to create a suite of 5,000 tests.
Randoop automatically drops textually-redundant tests 
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.


\subsection{Evaluation Procedure}

We evaluate each algorithm 
on both the human-written test suite 
and the automatically-generated test suite
for each subject program in Table~\ref{tab:subjects}.

%We run the three algorithms proposed
%in Section~\ref{sec:detecting} on both
%human-written and automatically-generated test suites
%of each subject program.

We run the randomzied algorithm \smalltrialnum, \mediumtrialnum,
and \trialnum times on each test suite, and record
the total number of detected dependent tests and time cost
for each setting. The choice of \trialnum times is based
on the a practical guideline of using randomized algorithm
in software engineering, as summarized in~\cite{Arcuri:2011}.
%
For the exhausitive $k$-bounded algorithm
and the dependence-aware $k$-bounded algorithm,
we use isolated execution ($k = 1$), and
pairwise execution ($k = 2$). The choice of $k$ is
based on the results of our empirical
study (Section~\ref{sec:study}) that a small $k$
can find most realistic test dependences.

\todo{say a few sentences about the manual part, such as
listing the immutable fields. say the approximate manual time used here.}

Each output dependent test is examined manually to make
sure the test dependence is not caused by non-deterministic
factors, such as multi-threading.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7.

\subsection{Results}

Table~\ref{tab:results} summarizes the number of detected
dependent tests and the time cost by each algorithm
in \ourtool.

\subsubsection{Detected Dependent Tests}
\label{sec:detectedtests}

A total number of 29 dependent tests are detected
from all human-written test suites, and 1311
dependent tests are detected from all automatically-generated
test suites. The percentage of dependent tests
in automatically-generated test suites is significantly
higher than in human-written test suites. We identified
two possible reasons: first, developers usually have deep
knowledge the intended purpose of a program when writing tests
for it. Such expertise helps them to build relatively well-structured and coherent
tests, e.g., tests that carefully initialize and destroy the
shared objects they may use. Second, 
 it is challenging for automated test generation tools to understand
 a program's intended purpose as well as how
specific parts of the program depend on the environment.
Thus, automated tools often do not explicitly generate code that sets up the
environment correctly.  In addition, to cover more
program states, automated tools often incrementally
builds tests on top of the program state after executing
other tests. This could further make a test's result depend on another.


%If, at the same time, other tests are
%generated that as a side e?ect create the needed environment, test dependence ensues



The randomized algorithm is surpringly effective in
detecting dependent tests. After running \trialnum times,
it identifies \textit{more} dependent tests than the other
two algorithms can find. For human-written
tests, the randomized algorithm detects 2 more dependent
tests in the JodaTime program. These two tests only
manifest when a sequence of three tests are run in a specified,
non-default order. Both exhaustive and dependence-aware $k$-bounded
algorithm fail to detect these two tests, because
they can not scale to $k$=3 for the
JodaTime program. Related, the randomized algorithm
detects significantly more dependent
tests in the automatically-generated test suites, for
two reasons. First,
both the exhaustive and dependence $k$-bounded
failed to scale to $k$=2 for 3 automatically-generated test suites;
and second, some detected dependent tests requires executing more than 2 tests
to manifest.

%Compared to the randomized algorithm, On the other hand,

\subsubsection{Performance of \ourtool}
\label{sec:performance}

The time cost of the randomized algorithm 
varies across different subject programs, and
is proportional to number of runs. Overall, the
time cost is acceptable for practical use.
For example, the randomized algorithm took around 1.5 hours
to finish 1000 runs,  for the largest human-written test
suite (\jodatimetests tests in JodaTime).
 
The time cost of running the exhaustive $k$-bounded algorithm
is prohibitive. The exhaustive algorithm failed to
scale to one human-written test suite and three automatically-generated
test suites when $k$=2, and failed to scale to all test suites
when $k$=3. The primary reason is due to the extremely large
number of all possible tuples. For example, running all JodaTime's \jodatimetests human-written
unit tests when $k$=2 requires running 15,011,750 test pairs, costing
approximately 1400 hours (58 days). 

The dependence-aware $k$-bounded algorithm helps improve
the efficiency of \todo{more results here.}

Table~\ref{tab:results} gives an estimated time cost for each
test suite that an algorithm failed to scale to. For each test suite,
we randomly chose 1000 samples from all possible
test tuples, executed them, and measured the time cost. Then,
we calcuated the average time cost of each sample, and multiples
the average cost with the number of all samples.



\subsubsection{Comparison of Algorithms}
\label{sec:algcomparison}

We next discuss the tradeoffs of choosing different detection
algorithms. Although the randomized algorithm
detects the most dependent tests in our subject programs,
it has several shortcomings that must be considered
in practice. First, there is no guarantee of
the randomized algorithm's results. A randomized
algorithm might even produce different results across runs,
and makes reproducing a dependent test harder.
Second, more importantly, there is no stop criteria
of using the randomized algorithm in shuffling and
running the test suite. Thus, it is hard for users
to know how many runs would be enough for a test suite.
Third, when a dependent test is identified, users
may need to inspect every test that is executed
before the dependent test, and isolate  a minimized
subsequence of tests (either
manually or using an assisting tool) to understand the dependence root cause.

By contrast, both exhaustive $k$-bounded and dependence-aware
$k$-bounded algorithms do not suffer from the above shortcomings.
One major factor that may prevent them being applied to a
large test suite might be the time cost for systematically
exploring all possible test tuples.


%\emph{three} tests to manifest. While these are easy to reproduce, we
%did not check that our tool finds them, because the time needed to
%run our naive algorithm on JodaTime with $k=3$ is measured in months.



\subsection{Discussion}
\label{sec:expdiscussion}

\subsubsection{Developers' Reactions}

We sent the identified human-written dependent tests to the
subject program developers, asking for their feedback.

One dependent test in JodaTime was previoulsy-known,
and had already been fixed. JodaTime's
developers confirmed the other two new dependent
tests, and thought that they is due to interactions
that are not intended in the design of the library.

The Crystal developers confirmed that all dependent tests
found in Crystal were not intentional and mostly likely
happended because they were not aware of the potential dependency
caused by global variables. The developers treat the
dependencies as undesirable and opened a bug report for
this report issue\footnote{\url{https://code.google.com/p/crystalvc/issues/ detail?id=57}}.

The Synoptic developers merged two related tests to fix
the dependent tests.

%After receiving our reported dependent tests in XML-Security,
The SIR~\cite{sir} maintainers confirmed our reported dependent
tests in XML-Security. They highlighted the practice
that tests should \textit{always} ``stand alone''
without dependency on other tests, and treated that as
``test engineering 101''. They also accepted our suggested
patch to fix the dependent tests.

\subsubsection{Threats to Validity}

There are several threats to validity in our evaluation.
First, the \subjnum open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
the {manifest dependence} between \textit{unit tests}.
We did not investigate possible test dependence that may arise
in other types of tests, such as integration tests
or system tests.
Third, due to the computational complexity of the general dependent test
detection problem, we do not yet have
empirical data of how many dependent
tests exist in a test suite and how many percentages of dependence tests
\ourtool can catch.  Giving a reasonable estimation is one of our future work.

%\todo{this paper focuses on manifest test dependence, what about
%potential test dependence, as well as some other cases in the
%study}

\subsubsection{Experimental Conclusions}


We have three chief findings: \textbf{(1)}
Dependent tests do exist in real-world test suites.
They may not be obvious to be identified
unless been explicitly searched for.
An automatically-generated test suite can contain
substantially more dependent tests than a human-written
test suite.
\textbf{(2)} Like the dependent tests
studied in Section~\ref{sec:study}, the identified
dependent tests in our subject programs reveal weakness
in a test suite rather than defect in the tested code.
And \textbf{(3)} In terms
of the number of detected dependent tests
and the time cost, the randomized algorithm is the
most cost-effective one.
%However, it does not
%have any guarantee in 

