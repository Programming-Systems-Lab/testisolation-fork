\section{Empirical Evaluation}
\label{sec:evaluation}

\newcommand{\jodatimetests}{3875\xspace}
\newcommand{\xmlsecuritytests}{108\xspace}
\newcommand{\crystaltests}{75\xspace}
\newcommand{\synoptictests}{xx\xspace}
\newcommand{\jodatimeautotests}{2639\xspace}
\newcommand{\xmlsecurityautotests}{665\xspace}
\newcommand{\crystalautotests}{3199\xspace}
\newcommand{\synopticautotests}{2467\xspace}

We evaluated two aspects of \ourtool's
effectiveness, answering the following
research questions:

\begin{enumerate}
\item How many dependent tests can \ourtool detect in
real-world programs by each detection algorithm (Section~\ref{sec:detectedtests})?
\item How long does each algorithm in \ourtool take to detect dependent
tests (Section~\ref{sec:performance})?
%\item How does \ourtool's effectiveness compare to an alternative
%approach based on test execution order randomization
%(Section~\ref{sec:random})?
\end{enumerate}

\subsection{Subject Programs}

\input{subject-table}
\input{example-table}

Table~\ref{tab:subjects} summarizes the programs and
tests used in our evaluation.

JodaTime~\cite{jodatime} is an open source
date and time library. It is a mature project that
has been under active development
for more than eight years. XML Security~\cite{xmlsecurity}
is a component library implementing XML signature and encryption
standards. XML Security is included in
the SIR repository, and has been used widely
as a subject program in the software testing community.
Crystal~\cite{crystal} is a tool that
pro-actively examines developers' code and
precisely identifies and reports on textual,
compilation, and behavioral conflicts.
Synoptic~\cite{synoptic} is a tool to mine a finite state
machine model representation of a system from logs.

Each subject program has a human-written JUnit test suite.
In addition, for each subject program, we use
Randoop~\cite{PachecoLET2007}, a state-of-the-art automated
test generation tool, to create a suite of 5,000 tests.
Randoop automatically drops textually-redundant tests 
and outputs a subset of the generated tests as
shown in Table~\ref{tab:subjects}.


\subsection{Evaluation Procedure}

We use \ourtool to detect dependent
tests among both the human-written test suite 
and the automatically-generated test suite
for each subject program in Table~\ref{tab:subjects}.


For both human-written and automatically-generated
test suites, we run the three algorithms proposed
in Section~\ref{sec:detecting} separately.

We run the randomzied algorithm repeatedly until no
new dependent tests are identified in the last
10 iterations.

When running the exhausitive $k$-bounded algorithm and dependence-aware
$k$-bounded algorithm,
we use isolated execution ($k = 1$),
pairwise execution ($k = 2$), and
3-wise execution ($k = 3$). The choice of $k$ is
based on the results of our empirical
study (Section~\ref{sec:study}) that a small $k$
can find most test dependences. 

%\todo{The experimental methodology might be different
%for the improved algorithm, which can incoporate user
%annoations, and is not restricted to k = 1, or 2}

%When comparing \ourtool with test execution order
%randomization, we use Random jUnit
%executor~\cite{randomjunit}, an existing tool
%that shuttles the whole test suite and executes the shuttled
%test suite. We execute each human-written
%and automatically-generated test suite with Random
%jUnit executor until reaching a plateau, i.e.,
%no more dependent tests are identified in the past
%10 executions.

Each output dependent test is examined manually to make
sure the test dependence is not caused by non-deterministic
factors, such as multi-threading.

Our experiments were run on a 2.67GHz Intel Core PC
with 4GB physical memory (2GB was allocated for the JVM),
running Windows 7

\subsection{Results}

Table~\ref{tab:results} summarizes the detected
dependent tests by \ourtool.

\subsubsection{Detected Dependent Tests}
\label{sec:detectedtests}

\todo{How many dependent tests are detected by both
algorithms?}

\subsubsection{Performance of \ourtool}
\label{sec:performance}

\todo{show the time cost}

%\subsubsection{Comparing with Test Execution Order\\ Randomization}
%\label{sec:random}

%compare with~\cite{randomjunit}



 
\todo{summarize the findings and show
whether this concurs with the study findings}


%During manual bug diagnosis in JodaTime, we identified two test dependences that require
%\emph{three} tests to manifest. While these are easy to reproduce, we
%did not check that our tool finds them, because the time needed to
%run our naive algorithm on JodaTime with $k=3$ is measured in months.



\subsection{Discussion}

\subsubsection{Threats to Validity}

There are two major threats to validity in our evaluation.
First, the \todo{NUM} open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
dependence between unit tests.
\todo{only consider unit tests, no system tests.} 
Third, due to the computational complexity of the general dependent test
detection problem, it is difficult to know precisely how many dependent
tests exist in a test suite. Thus, we do not
yet have empirical data that shows how many percentages of dependence tests
\ourtool can catch.  Giving a reasonable estimation is one of our future work.
\todo{this paper focuses on manifest test dependence, what about
potential test dependence, as well as some other cases in the
study}

\subsubsection{Experimental Conclusions}


We have XXX chief findings: \textbf{(1)}
\textbf{(2)}, \textbf{(3)} ...
The examples where dependence identified weaknesses in the tests themselves
are even less likely to be observed.  
Our prototype tool shows
the potential for revealing dependences, allowing
developers to observe them and make conscious decisions about how, or even whether,
to deal with the dependences. 
