\section{Empirical Evaluation}
\label{sec:examples}

To show the effectiveness of our proposed
dependent test detection algorithms, we conducted
an evaluation on \todo{XXX} open-source programs (Table~\ref{}).
In our evaluation, we seek to answer the following research questions:

\begin{itemize}
\item \textbf{RQ1:} How effectively do our algorithms detect
dependent tests?
\item \textbf{RQ2:} How do the proposed static and dynamic program analyses
in \todo{algorithm-name} improve the algorithm efficiency?
\end{itemize}

\subsection{Subject Programs}

\input{example-table}

Figure~\ref{fig:example-summary}
summarizes the projects we studied and the results: The table
summarizes the number of tests in the suites produced by the
developers (\emph{MT}), the number of tests we generated automatically
with Randoop (\emph{AT}), and the corresponding numbers of dependent
tests in those test suites (\emph{MTD} and \emph{ATD}, respectively). 
The discussion of the examples in this section is distinguished by
the problems caused by test dependence (\emph{Kind}): when faults are masked because
tests make incorrect assumptions about the global environment (Section~\ref{sec:mask}); 
when tests do not
respect required initialization protocols (Section~\ref{sec:examples:initialization}); and when
undocumented test dependence leads to spurious bug reports (Section~\ref{sec:spurious}).
We also describe dependent tests in an automatically-generated test
suite (Section~\ref{sec:autogen}).
While this list---and associated set of examples---certainly is not exhaustive, it shows that there are
several classes of dependence-related problems that have practical
relevance.


\subsection{Evaluation Procedure}

\todo{including manual tests and auto-generated tests}

We used the prototype to verify the dependent tests reported by
users, developers, other researchers, and us, and to find new dependent
tests in the
example programs in Section~\ref{sec:examples} using isolated execution ($k = 1$)
and pairwise execution ($k = 2$).

All the dependent tests reported in Figure~\ref{fig:example-summary},
except for two dependent tests in JodaTime and the dependences in SWT, 
can already be found by isolated execution. Since we could not run the
test suite of SWT, we could not check these dependences with our tool.
During manual bug diagnosis in JodaTime, we identified two test dependences that require
\emph{three} tests to manifest. While these are easy to reproduce, we
did not check that our tool finds them, because the time needed to
run our naive algorithm on JodaTime with $k=3$ is measured in months.

While we believe that most test dependences can be found with small
$k$. This is in part because the set of dependent tests that can be
found with a bound $k$ is always a subset of the set of dependent
tests that can be found with any bound $k' > k$. Additionally, our
intuition and preliminary exploration seem to indicate that small $k$
find many dependences, while larger $k$ do not. However, in principle
it is conceivable
that any number of chain dependences with chains longer than any tried $k$ exist
in all the libraries we analyzed.

\subsection{Results}

\todo{show the basic results here, i.e., the num
of dependent tests found. Its consequence is discussed
below.}

\subsubsection{RQ1: Algorithm Effectiveness}

\subsubsection{RQ2: Improvement from Program Analyses}

\subsection{Consequences of Dependent Tests}

\todo{The following text is too verbose. Note be
consistent to the categories used in the study section}

\subsubsection{Masking Faults}\label{sec:mask}

\emph{Masking} is a particularly perplexing problem caused by
dependence.
The negative effect of masking is that it hides a fault in the
program, \emph{exactly} when the test suite is executed in its default
order. 
Masking occurs when a test case $t$ (a) \emph{should}
reveal a fault, (b) only does so when executed in a specific environment
$\env_R$, but (c) tests executed before $t$ in a test suite always
generate environments different from
$\env_R$.
%To express this more
More precisely and without loss of generality, assume any
environment with only a single variable. Then let $T =
\suite{t_1,\dots,t_n}$ be the test suite, and let $t_i, 1 < i \leq n$
be the test that should reveal the fault in environment $\env_R$. A
dependency $t_k \prec t_i, k < i$ masks the fault if
$\exec{\suite{t_1,\dots,t_{i-1}}}{\env_0} \neq \env_R$.

The following two examples illustrate masking in
practice.

\paragraph{CLI: A Long-Standing Bug}

\begin{figure}
% \lstset{language=Java,numbers=left}
%\lstset{language=Java}
\lstset{belowskip=0ex,escapechar={@},numbers=left,numberstyle=\small\ttfamily}
\begin{lstlisting}
public final class OptionBuilder {
  @\itshape\color{red}
  private static String argName;@
  
  private static void reset() {
    ...
    @\itshape\color{red}argName = "arg";@
    ...
  }
   
  public static Option create(String opt){
    Option option = 
      new Option(opt, description);
    ...
    option.setArgName(argName);
    @\itshape\color{red}OptionBuilder.reset();@
    return option;
  }
}
\end{lstlisting}
\caption{Fault-related code from \code{Option\-Build\-er.java}}
\label{fig:option_builder}
\end{figure}

A straightforward example of fault masking occurs in the Apache CLI
library~\cite{cli}.
Two test cases fail when run in isolation:
\code{test13666} and \code{test\-Op\-tion\-With\-out\-Short\-For\-mat2} in test
classes \code{Bugs\-Test} and \code{Help\-For\-mat\-ter\-Test},
respectively.

A detailed study of the code under test revealed that both 
tests fail due to the same hidden dependence. The fault is located in 
\code{OptionBuilder.java} and is caused by not initializing a global
variable early enough.
Figure~\ref{fig:option_builder} shows code that
illustrates the fault. 
%
By default,
\code{argName} is initialized to \code{null} (line 2), and only set to
its intended default value \code{"arg"} by the \code{create()} method
via calling \code{reset()} (line 15). 
Consequently, if clients of CLI do not explicitly initialize the value of
\code{argName}, the first option created will have \code{null} rather
than \code{"arg"} as its argument name.
%In CLI, there are two types of options: options with and without
%argument names. If an option without argument is created first,
%this fault will not lead to a failure, because the \code{null} value
%will be ignored. Consecutive calls to \code{create()} can rely on
%\code{reset()} to establish the desired default value.

Both dependent tests
% \code{test13666} and \code{test27635} (or \code
% {test\-Op\-tion\-With\-out\-Short\-For\-mat2}) 
can reveal this fault, since they create an option with 
the default argument as the first thing in their execution. However,
in the default order of test execution, 
%the test classes \code{BugTest} and \code{Help\-For\-mat\-ter\-Test} both
%contain other 
tests that create options with explicit arguments execute \emph{before} 
these dependent tests.
% \code{test13666}
% and \code{test27635} respectively. 
%Thus, when the tests in these classes are 
%executed in order, the tests executed before \code{test13666}
%and \code{test27635} call \code{create()} 
Thus, the tests that are executed before call \code{create()} at least once, which
sets the default \code{argName} value, thus masking the fault.


This fault is reported in the bug
database several times,\footnote{\url{https://issues.apache.org/jira/browse/CLI-26} \url{https://
issues.apache.org/jira/browse/CLI-186} \url{https://issues.apache.org/jira/browse/
CLI-187}} starting on March 13, 2004 (CLI-26). The report is marked as resolved
\emph{three years} later on March 15, 2007, but is then reopened as CLI-186 on
July 31, 2009. On this report, one of the developers commented:
\begin{quote}
I reproduced the issue, it requires a dedicated test case since it is tied to the initialization 
of a static field in OptionBuilder.
\end{quote}
Despite the realization that a dedicated test is required, no such
test was ever created.
About one month later, the bug is duplicated as CLI-187, and the
actual fix happens one 
year later on June 19, 2010, about six years after the bug was first reported (and four years
total on the open-issue list).

\newcommand{\jodatime}{JodaTime\xspace}
\paragraph{\jodatime: Complex interactions that mask faults}
\label{sec:jodatime}
\input{jodatime}

\subsubsection{Poor Test Construction}\label{sec:examples:initialization}

Based on our interaction with the \jodatime developers, this last
dependence does not
mask a fault in the program.  Instead, it represents a less severe consequence of test
dependence that suggest that a test, or a test suite, 
has been constructed poorly in some dimension.  While test dependences that mask faults
correspond to a defect
in the program source, these dependences correspond to defects in the test code.
%
%In contrast to the previous section
%where the dependences led to defects in the program source, this section concerns defects
%in the test source.

%In some sense, dependences that are due to missing initialization are
%the dual to dependences that mask faults.  Both reveal problems in source code.
%However, masked faults reside in the program source, while incorrect
%initialization is a fault that resides in the test suite.

The test dependences presented in this section arise due to incorrect initialization
of program state by one or more tests. In the first case,
%
%The following two examples show two common patterns where incorrect
%initialization leads to test dependence.
%The first example is probably the most common. 
tested program code relies on a
global variable that is a part of the environment, but the test does
not properly initialize it.  In the second case, a test should but
does not call
an initialization function before later invocations to a complex library.
This flaw in the test code is masked because the default test suite execution
order includes other tests that initialize the library.  The defect is
inconsequential until and unless the flawed test is reordered, either manually or by
a downstream tool, to execute before any other initializing test.

%The second example employs a common pattern for complex
%libraries that requires a call to an initialization function before
%any other part of the library can be used.
%In both cases, other tests perform the required setup, and because
%they occur before the dependent tests in the normal execution order,
%no tests fail under normal circumstances.

\paragraph{Crystal: Global Variables Considered Harmful}
\input{crystal}

\paragraph{XML Security: Global Initialization}

%Are these test dependences realistic, or part of the modifications SIR
%made? by SZ: they are realistic, we use the original version without
%any modification from SIR people
\input{xmlsecurity}

\subsubsection{Spurious Bug Reports and Bug Fixes}\label{sec:spurious}
Sometimes developers introduce dependent tests intentionally because it is
easier, more efficient or more convenient to write unit tests for some modules
in that way~\cite{kapfhammeretal:FSE:2003, whittakeretal:2012}.
%DB-testing}.
Even though the developers are aware of these instances
when they create them, this knowledge can get lost, 
and other people who are not aware of these dependences can get confused 
when they run a subset of the test suite that manifests the
dependences.

As a result, they
might report bugs backed by the failing tests, although this is exactly the expected
behavior. If the dependence is not documented clearly and
correctly, it can take a considerable amount of time to work out that
these reported failures are spurious. Or worse, the developers may try
to fix a bug that is not there.

\paragraph{Eclipse SWT: Causing Spurious Bug Reports}
\input{eclipse}

\subsubsection{Dependence in Auto Generated Tests}
\label{sec:autogen}
\input{beanutils}

%\section{How does the theory relate to our examples}

\subsection{Experimental Discussion}

\subsubsection{Threats to Validity}

There are two major threats to validity in our evaluation.
First, the \todo{NUM} open-source
programs and their test suites may not be
representative enough. Thus, we can not claim the results
can be generalized to an arbitrary program.
Second, in this evaluation, we focus specifically on
dependence between unit tests.
\todo{only consider unit tests, no system tests.} 

\subsubsection{Conclusions}


We have XXX chief findings: \textbf{(1)}
\textbf{(2)}, \textbf{(3)} ...
The examples where dependence identified weaknesses in the tests themselves
are even less likely to be observed.  
Our prototype tool shows
the potential for revealing dependences, allowing
developers to observe them and make conscious decisions about how, or even whether,
to deal with the dependences. 
