Subject: ISSTA 2014 notification for paper 91
From: ISSTA 2014 <issta2014@easychair.org>
To: "Michael D. Ernst" <mernst@cs.washington.edu>
Date: Sun, 30 Mar 2014 21:14:05 +0200

Dear Michael D.,

Thank you for your submission to the ISSTA 2014 conference.  The
program committee evaluated and discussed the 128 submitted papers.
We are happy to inform you that your submission:

Empirically Revisiting the Test Independence Assumption

was accepted for inclusion in the ISSTA 2014 conference program.
Congratulations!

Each paper went through a thorough review process that involved at
least three reviewers.  This year we received a record number of
submissions, and the quality of submissions has been very high.  The
program committee had a difficult time selecting from the many very
good papers submitted.  Overall, only 36 papers out of 128 submissions
were accepted (<29% acceptance rate).

The reviews of your paper are enclosed.  Please make sure to take into
account the comments of the reviewers when preparing the camera-ready
version of your paper (in addition to carefully proof-checking it for
typos and other minor problems).  You will receive a subsequent email
containing general directions for submitting the final camera-ready
copy of your paper, which is due on June 6, 2014.

We strongly encourage you to submit the artifacts related to your
paper to the Artifact Evaluation track.  Please see the instructions
at http://issta2014.org/artifacts.html

For your paper to be published in the ISSTA 2014 conference
proceedings, at least one of the authors of the paper must register
for the conference and confirm that she/he will present the paper in
person.   Please confirm by having one of the authors reply to this
message that: (1) you are willing to submit a camera-ready paper, and
(2) one of the authors will present the paper at the conference.

We look forward to seeing you at ISSTA 2014 in San Jose, CA!

Please also consider submitting to one of colocated events, doctoral
symposium, and/or tool demo.  The deadlines are in April and more info
is available from http://issta2014.org/

Best Regards,

Corina Pasareanu and Darko Marinov
ISSTA 2014 General and PC Chair


----------------------- REVIEW 1 ---------------------
PAPER: 91
TITLE: Empirically Revisiting the Test Independence Assumption
AUTHORS: Sai Zhang, Darioush Jalali, Jochen Wuttke, Kivanc Muslu, Wing Lam, Michael D. Ernst and David Notkin


----------- REVIEW -----------
This paper formalizes the test independence assumption, studies the
characteristics of real dependence tests in Java programs, and
proposes and evaluates three algorithms for finding dependent tests.
Findings include the fact that test independence violations typically
involve a small number of tests, are often due to shared static
variables, most often indicate problems in the test code, affect test
prioritization algorithms, and that a randomized algorithm performs
best on an initial study involving four Java programs.

This is a well-written paper, which makes for an engaging read.  The
problem is clearly introduced, and shown to be prevalent and important
in practice.  The requirement for tests to be independent is widely
acknowledged but rarely questioned, so it was surprising to see that
it is often violated.  While the empirical studies in the paper are
too limited to demonstrate how many violations of this kind exist,
they convincingly show that this is more than an isolated incident,
and techniques relying on test independence should question this
assumption.  However, the paper does not demonstrate that test 
independence has a (significant) negative impact on these techniques,
and this is a weakness of the paper. 

Finally, I thought the paper does a very good job presenting the 
different types of problems that can occur and the different ways 
in which developers handle these problems.

The paper focuses on Java code and unit tests, and I think this should
be explicitly mentioned/discussed at the beginning of Section 2. Java
and JUnit have several specific characteristics, such as the use of a
single JVM to run all tests and the communication via static
fields. It would be also interesting to speculate the kind of problems
that one would see in other types of programs, e.g., C code with only
system-level tests.

I was wondering how effective it would be to simply run the
fully-reversed permutation (n, n-1, ..., 1).  I think this could be a
competitive heuristic.

What are textually-redundant tests?  I didn't understand how the
filtering of Randoop tests was done.

I was happy to see that the dependent tests are made available
to the community.


----------------------- REVIEW 2 ---------------------
PAPER: 91
TITLE: Empirically Revisiting the Test Independence Assumption
AUTHORS: Sai Zhang, Darioush Jalali, Jochen Wuttke, Kivanc Muslu, Wing Lam, Michael D. Ernst and David Notkin


----------- REVIEW -----------
Dependent tests are tests that may function differently when run in different 
orders, due to interactions between the tests themselves.   A number of 
current techniques for facilitating testing may function differently in
the presence of dependent tests, but the effects of dependency on those
techniques has not been rigorously studied.  The paper examines the incidence 
of dependent tests, and proposes algorithms with which to detect them.  
An empirical study is used to evaluate the algorithms.

This is an interesting paper on a topic that hasn't been sufficiently studied, 
and that clearly may have ramifications for testing research.  My comments and 
suggestions for improvements pertain primarily (with one exception) to presentational 
issues, though there are a couple of larger issues.

One larger issue involves the fourth research question and the way it is addressed 
in 6.3.4.   Here, the authors note that they implemented five prioritization 
techniques, and for each one, counted the number of dependent tests that return 
different results in prioritized orders from those return in unprioritized orders.  
A small number of tests (from 1 to 6) are found to be dependent, 6 of 7 on 
one program.  From this the authors conclude that ".... these techniques implicitly 
assume that there are no test dependences.... Violation of this assumption...
causes undesired output." I have two points on this.  First, this is too strongly 
stated given the small number of violations and programs on which they occur.  
Second, the prioritization techniques studied are meant to increase rate of 
fault detection, usually measured by APFD.   There is no data presented here to
indicate that the dependent tests caused the techniques to compute worse APFD values 
than they otherwise would, thus there is no evidence to support a claim that 
dependence has, in this case, "caused undesired output".   Moreover, it may even 
be the fact that, due to test scheduling, the resulting techniques produce *better* 
APFD values.   The current section won't suffice.   I suggest moving it into an 
earlier portion of the paper, and using it as motivation to support the claim 
that prioritization techniques **might** be affected, and qualify it appropriately 
given the small number of occurrences.

A second larger issue I want to make pertains to the claim that prioritization
techniques "assume" independence.  I don't believe that they do; at least, this
isn't an appropriate word to use.  Prioritization techniques are heuristics, and 
there are many things that may cause them to produce suboptimal results.  For example, 
coverage-based prioritization techniques use coverage from prior versions to estimate 
coverage on a version to be tested.  The problem with the word "assume" as used by 
the authors, both throughout the paper, is that it carries a connotation of "requires".
This simply isn't the case.  Prioritization techniques can operate with or without 
dependent tests being present.  The real issue is, will the presence of the tests 
cause them to perform less well.   

The authors should be more careful in discussing not just prioritization techniques, 
but all of the techniques mentioned in the second paragraph of the intro.   In fact, 
that paragraph in particular should be carefully edited, it makes it sound as if all 
the techniques mentioned there *require* independence, calling the "assumption" "critical".
In no way is this assumption necessarily "critical" for prioritization, as discussed above.
Moreover, whether it is critical for most of the other techniques will depend on an
empirical study focusing on the actual effects ignoring dependence has on those techniques.

The foregoing comments in no way lessen the worth of this paper's contribution,
overall, and I certainly agree that the effects of dependence need to be assessed.
In fact, if the authors stop being so unduly strong up front about the effects, 
they'll leave themselves room for quite a bit of additional, interesting research.

This leads me to one other point.  The intro and related work sections also
claim that the issues involving dependence have not been mentioned in
regression test selection papers.   This isn't fully accurate.  RTS techniques that 
are safe are described as such because of a specific theoretical background that 
includes a requirement that test cases be deterministic (a requirement that 
"controlled regression" testing be performed.)  (See Rothermel and Harrold, 
TOSEM 1996, "Analyzing Regression Test Selection Techniques").  Controlled regression 
testing precludes dependence among tests, because such dependencies render test 
results non-deterministic.  Thus, the theory implies that in the presence of such 
tests, RTS techniques can't be asserted to be safe.  The term "test dependence" is 
not explicitly used in the foundational papers on safety, but it is certainly 
subsumed by the larger interest in control, so saying that the issue has been 
neglected in that sub-area isn't correct.  Papers on safe RTS are typically quite 
specific about this assumption, and specific about the fact that in the absence 
of the assumption holding, safety is not possible.  The results in this paper 
(the paper this review is about) are important in that connection as they show 
that dependence effects may exist more often in practice than has been thought, 
and important in the sense that they may lead to techniques to improve the prospects 
for safety by detecting dependences, but this doesn't mean that the issues related 
to dependence simply "haven't been mentioned".   For example, if the authors look 
more closely at their reference 40 (Section 2.3 of the paper), an RTS paper, they'll 
find that the paper discusses the need for deterministic test runs to ensure safety.
Deterministic runs necessarily require test independence, so I would say the need
for that is being acknowledge, even though that particular effect is not being
mentioned specifically.

On page 3, left, para on "Root cause".  When I first read this, I was confused 
as to the authors' intent.  In what way are tests making "inappropriate" 
accesses to shared static variables?  What exactly is an inappropriate access, 
and "inappropriate for what".  Arguably, if tests reveal *faults* in the SUT 
due to accesses made, in those tests, to shared variables, then as tests, 
they're actually pretty *good* tests.  Later, the definitions make it more 
clear what sort of things the authors mean by dependencies and how they are 
defining them.  But it could be more clear here.

This leads to my next comment though.  I don't think the paper is quite 
balanced enough in its discussion of the issues related to dependent tests.   
I think there are plenty of good reasons to often prefer independent tests, 
but it isn't so black and white, and a more balanced presentation will 
actually make it clear that the topic is much richer.

For example, in Section 1 para 2, we read that (unequivocally) "all test 
cases should be independent".  Of course there are many reasons to favor 
independence as the paper later shows.  But there are still times when 
dependence is not required, as in non-deterministic testing which seeks 
to expose interaction effects by trying different permutations of things.   
It's not appropriate to be so unequivocal here.

As another example, page 4 left bottom refers to "improper static variable 
accesses" as a cause for dependence.  What is meant by "improper" here?   
Again, if there are static variable accesses that cause program failures 
and if the tests reveal those, how is this unequivocally improper?

Put another way, I could argue that simply asserting that test dependence 
is bad, and must be detected and removed, might lead us down the road to 
techniques that remove fault-detection power from test suites, by flagging 
cases such as variable accesses and other interactions.  The results in
this paper can't as of yet assert that such an argument would necessarily
be wrong.  Dependence among tests has not yet been shown to be the devil 
incarnate, and the tradeoffs should be discussed in a more balanced manner.

As another example, Page 3 right bottom discusses masking as a consequence of 
dependent tests, and its deleterious effects.  But, dependent tests can also
*expose* faults by revealing behaviors that haven't been seen or considered,
a result supported by the author's reference 45.

Of course, another way to deal with "dependencies" among tests that may 
be useful in testing is to concatenate those tests into a single test.  
I.e. if tests A and B have dependencies then perhaps two tests, AB and BA, 
should be created.  This retains the power of the tests to detect interactions.
This notion could actually help increase the usefulness of the authors'
approach.  One needn't restrict the use of the approach to efforts to detect 
and *remove* dependencies, one might use it to detect certain potentially
useful dependencies and redesign test suites to be more effective.

Abstract sentence 3, "techniques... do not justify that assumption".  Techniques
can't justify assumptions, I think you mean to say simply that the paper involved
don't discuss the assumption.  But again, as noted above, I don't think "assume"
is the right word.

Table 4 caption line 2, seems like some missing text here.


----------------------- REVIEW 3 ---------------------
PAPER: 91
TITLE: Empirically Revisiting the Test Independence Assumption
AUTHORS: Sai Zhang, Darioush Jalali, Jochen Wuttke, Kivanc Muslu, Wing Lam, Michael D. Ernst and David Notkin


----------- REVIEW -----------
The paper presents a study on dependent  tests. The study is conducted in two parts: The first one  analyzes 5 software  issues tracking systems  to collect 53 reports about dependent tests.  Then this reports were analyzed to single out the time of  manifestation, the root cause and the repair actions. Based on the findings of this  first study three dependent test detection algorithms are proposed: a randomized one, a exhaustive bounded one and  a dependence-aware one. The three algorithms are all incomplete but exhibit different efficiency, a further study is then dedicated to assess the quality of these algorithms in detecting dependent tests on 4 software systems. A last evaluation is on the effect of dependence tests on prioritization techniques.

The paper is build around the axiom  that normally independence of tests is assumed however this assumption is clearly not valid and the paper contribution is to demostrate this. 

Indeed the paper seems to also demonstrate that is well known among developers that dependent tests may arise as witnessed by the reports in the tracking systems and by the feedback of the developers on the findings of he DTDector tool. 

Also the origin of these dependencies seem to be known by developers and indeed this knowledge is the source for the definition of the dependence-aware bounded algorithm. Some of the finding of the first study seem to me hard to generalize or at least not convincing. If on one side due to the NP-complexity of  the problem it is reasonable to think of bounded algorithms, it is hard to believe that there is any rationale in the findings that most dependencies were manifested after 2 tests. 
The paper is very well written and carefully explains all the steps, however i am not convinced of the relevance of such findings nor I am of the relevance of the proposed tool. 
It is not clear to me what in practice developers will get from this study and tool that they did not already know and deal with. It may very well be possible that the DTDector tool might provide some help but this seems to me quite marginal.  Also in terms of contribution at the academic level the definition  of dependent tests and  the accompanying  definitions are quite straightforward. To this respect Def. 4 should include a reference to the program (tests exist in the scope of a program) and a notion of what an environment is would be beneficial (the sentence "the set of possible environments" means nothing as is). Moreover the formalization  and all the analysis are confined to determinisic tests with no self-dependence.

