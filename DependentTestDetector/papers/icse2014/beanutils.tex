\newcommand{\pub}{\texttt{Prop\-er\-ty\-Utils\-Bean}}
\newcommand{\fhm}{\texttt{Fast\-Hash\-Map}}
\newcommand{\cub}{\texttt{ConvertUtilsBean}}

As shown in Table~\ref{tab:results}, in most
projects, a large fraction of the remaining tests are dependent, while
in some projects, there are almost no dependent tests.

Take the Beanutils program as an example, Randoop generates a test suite consisting of
2692 unit tests for a recent release of Beanutils (version 1.8.3).
The prototype tool we outline in Section~\ref{sec:impl}
detected 299 dependent tests in this test
suite.

After a close inspection of the automatically generated test code, we found
the primary reason for the dependencies is missing initialization 
(cf.~Sec.~\ref{sec:examples:initialization}).
%is the unintended program state before test execution.
Specifically, 248 tests attempt to retrieve values from a cache before
anything has been added to the cache. This particular dependence could be
fixed by adding a single line of setup code to each test.
Most of the other dependencies could be fixed with similarly low effort, too.
However, this particular fix requires understanding of at least part
of the program semantics, which is a feat beyond the abilities of
current test generation tools.
%and fully automatically.

%implicitly assume a particular program state (
%the caching state) when executing in the generated order. In Beanutils,
%\pub{} --- which can be
%accessed as a singleton --- contains a global cache from \texttt{Class} to
%\fhm{}. These 248 tests retrieve the value for \cub{}
%from this cache and assert that the result is not \texttt{null}, without adding
%anything to the cache first. However, some other generated tests call
%\texttt{Prop\-er\-ty\-Utils.get\-Prop\-er\-ty\-Editor\-Class}, which adds
%a \{\cub{}, \fhm{}\} pair to the cache internally. As a result, the former tests
%fail when run in isolation (since the cache is empty and it returns null), however
%pass when run within the whole test suite. Reasons for the remaining 51 dependent
%tests are similar, which we omit here for brevity.


Given the high ratio of dependent tests in the automatically generated
test suite, we speculate that the following two phenomena could be
reasons for this.

First, developers usually know a lot about the intended purpose of a
program when they write tests for it. This knowledge helps them to
build well-structured and coherent test suites.
Automated tools, on the other hand, have no such knowledge. One
possible consequence of this is illustrated by the example: the
automated tool does not understand the cache protocol and thus does
not know that it must add values to the cache first. 

%unit
%tests, programmers tend to put logically-related code in the same unit test to test certain software functionality. By contrast, automated test generation tools are often not aware of the underlying program structure nor the test execution environment when creating new ones. In particular, random test
%generation tools like Randoop invokes tested methods
%with little guidance. Thus, a generated test is more likely to depend on the
%execution of others.

%are more likely to ``interleave''
%with each other, such as, invoking the same static method mutating program states.
%\todo{JW}{I do not understand this explanation at all. What has this
%interleaving to do with dependences?}

Second, it is often hard for automated tools to understand that
specific parts of the code depend on the environment, and thus may not
explicitly generate code that sets up the environment correctly. If,
at the same time, other tests are generated that as a side effect
create the needed environment, test dependence ensues.

%Second, test frameworks like JUnit offer constructs \code{@Before}
%(\code{@After}) to permit programmers to abstract common execution environment
%construction (de-construction) code for each unit test. Such mechanism prevents
%dependent tests exhibiting to some extent. However, to the best of our knowledge,
%most automated test generation tools do not leverage
%such mechanism to enforce generated unit tests to execute in an intended environment.

%\todo{JW}{While this is true, I'm not sure if this is the right plave
%to put it}
% \textbf{Kivanc has investigated this, but i can not find the email now.}
% \todo{KM:}{ This is not completely true. I just have the stack trace for the
% first dependency which seems to be due to a mistake in caching. However, I
% don't know if this is a bug, or any information about other dependencies.}



On the other hand, test dependence in automatically generated test suites is 
even more troublesome than in human-written suites. The reason for this is
that all automated test generation tools we are aware of produce tests
that are hard to read for humans, are undocumented, and their intent
cannot easily be gleaned from naming conventions and other aids
developers normally use. While there is some work to alleviate this problem, it
still remains difficult to determine whether a failed test points to a bug
in the program or a dependent test~\cite{fraseretal:ISSTA:2011}.

%We already showed some evidence that test dependence is not uncommon
%in human-written tests. Given the increasing importance of
%automatically generated tests, we also wanted to at least get a
%glimpse of what is happening in that area.
%As a very preliminary, and by no means exhaustive or conclusive
%investigation, we applied Randoop to all the projects for which the
%source was readily available (this excludes SWT).

%Why this strong division happens, and whether the differences between
%the programs can be used to derive guidelines for better testing is an
%interesting question left to future work.

%\todo{JW}{I no longer think the following paragraph is true}
%This is at once surprising and troublesome. It is surprising, because
%in our experience test dependence occurs either because it is too much
%hassle to write proper test setup code for every single test, or
%because developers are not aware that global state is relevant to the
%code that is being tested. The first point should not at all be
%relevant to automated techniques, as the effort of generating boiler
%plate code is negligible compared to the cost of figuring out useful
%parts of the code to test. The second aspect is fairly well amenable
%to static analysis. Thus overall, there is no reason why automated
%tools could not avoid test dependence altogether.

