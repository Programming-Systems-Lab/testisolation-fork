\section{Introduction}

Informally, \emph{dependent tests} produce different test results when
executed in different contexts. %, while \emph{independent tests} produce
%the same test results regardless of execution order.  
It is easy to
construct an example of dependence between two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to both tests passing, while running \code{B} and then
\code{A} leads to either or both tests failing---the order
of applying the tests, in this case, changes the execution context.

%~\cite{KapfhammerS03}
%Chays:2000:FTD:347324.348954,
%Gray:1994:QGB:191843.191886}, 
\todo{find a place to emphase unit tests}

Definitions in the testing literature are generally clear that the
conditions under which a test is executed may affect its result.  The
importance of context in testing has been explored in some depth in
some domains including databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,
kapfhammeretal:FSE:2003}, with results about test
generation, test adequacy criteria, etc., and mobile
applications~\cite{Wang:2007:AGC}.
For the database domain, Kapfhammer and Soffa formally
define and distinguish independent test suites from those that are
\emph{non-restricted\/} and thus ``can capture more of an application's
interaction with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations of test
adequacy''~\cite[p.~101]{kapfhammeretal:FSE:2003}.

At the same time, there is little focus on the core
issues of
test dependence itself.
Is this because test dependence does
not arise in practice (beyond domains such as databases)?  Is it because, even if-and-when it does arise, there
are few if any repercussions?  Is it because it is difficult to
notice if-and-when it arises?
\todo{note, the above questions must be recalled and discussed after
presentating the study and evaluation. do not forget.}

\subsection{Manifest Test Dependence}

To explore these questions, we consider a narrow characterization
of test dependence that:
\begin{itemize}
\item Adopts the results of the default, usually implicit,
  order of execution of a test suite as the \emph{expected results}. 
\item Asserts \emph{test dependence\/} when there is a possibly
  reordered subsequence of the original test suite that, when
  executed, has at least one test result that differs from the
  expected result for that test.  
\end{itemize}
That is, we focus on a \emph{manifest\/} perspective of test dependence,
requiring a \emph{concrete\/} variant of the test suite that
\emph{dynamically\/} produces different results from the expected.  Our
definition differs from that of Kapfhammer and Soffa by considering
test results rather than program and database states.
Considering only manifest test dependences allows
us to more easily situate this research in the empirical domain (Section~\ref{sec:formaldiscussion}).



\subsection{Test Execution Environment}

Varying execution environments is the unsurprising central
cause of test dependence. Specifically, when a
test is executed in different environments---global variables
with different values, differences in the file system, differences in
data obtained from web services, etc.---it has the potential to return
a different result.  
As shown in Sections~\ref{sec:study} and~\ref{sec:evaluation}, most of the dependences we found
ultimately stem from incorrect or incomplete initialization
of the program environment.

Why does this happen? Especially given frameworks such as
JUnit that facilitate the process of clean setup by providing means to
automatically execute methods (\code{setUp()} and \code{tearDown()} in JUnit
3.x, and methods annotated with \code{@Before} and \code{@After} in
JUnit 4.x) that should handle all common setup and clean-up between
test cases. 

It appears that the answer is that developers are as likely
to make mistakes when writing tests as when they are writing other code.
And while frameworks make it easier to get environment setup right, 
they cannot ensure that it is done properly. 
Like with other code, this means that tests in some cases will have
unintended and unexpected behaviors.  And as programs increase in complexity,
so may tests, which may increase the frequency of such problems in tests,
which may in turn increase the frequency of test dependence.
The situation is exacerbated by the fact that global variables shared by tests
%These tests
%shared global variables, and the test results varied depending on the
%values stored in these variables.  
%That is, relevant execution conditions---specifically, pertinent parts
%of the implicit \emph{environment} comprising global variables, the file system, operating system services, etc.---were neglected.
are usually buried deep in the program code, and the assertions
do not directly check them,
but rather check values that have been computed from
them. In many non-trivial real-world programs, this
deep nesting effectively hides potential dependencies from developers,
and they may only become aware of them when a subtle bug leads them
there or a test fails suspciously.

%Therefore, we explicitly
%distinguish potential test dependences (Definition~\ref{def:dependency})---those that could cause a variation in test suite results 
%under \emph{some} environment and order---and manifest test
%dependences (Definition~\ref{def:manifest})---those that are guaranteed to cause a
%variation in test suite results under a \emph{specific} environment and order.  


\subsection{Repercussions of Dependent Tests}

%We have identified a number of substantive examples of test suites
%from fielded programs that manifest dependences.
%in fielded programs. 
%with test suites that manifest test dependence.  

%In this paper, we seek to understand dependent tests and their repercussions
%in two ways. 

After analyzing \todo{xx} real-world, confirmed dependent
tests from existing bug repositories (Section~\ref{sec:study}), we identified three
major categories of repercussions that can arise due to the presence of dependent tests.
First, most test suites that unexpectedly contain dependent tests \emph{conceal
weaknesses in the test suite} itself. Exposing
dependent tests can detect code smells and identify situations
where some tests do not perform proper initialization.
Second, a test suite containing test dependences can lead
to \emph{spurious bug reports}. As an example, a dependent test in
eclipse SWT took the developers
more than a month to realize that the test dependences were intentional,
allowing them to close the bug report without a change to the system.
Third, even worse, test suites that unexpectedly contain dependent tests can
\emph{mask faults in a program}. Specifically, executing a test suite in the
default order does not expose the fault, whereas
executing the same test suite in a different order does. We found 
a bug masked two dependent tests
for 3 years in the Apache CLI library~\cite{cli} (Section~\ref{}).

%Second, guided by the findings of our study, we design two algorithms
%to detect manifest dependent tests. By applying our algorithms
%to \todo{xx} open-source programs and their test suites, we 
%found a large number of unknown dependent tests, .

Another important impact of dependent test is for downstream testing
techniques, in which the underlying test execution context can
unexpectedly change is when a tool or technique that takes a test
suite as input is used.  Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite that will guarantee some properties during
regression testing)~\cite{harroldetal:OOPSLA:2001},
test prioritization techniques (that reorder the
input to increase the rate of fault detection)~\cite{Elbaum:2000:PTC:347324.348910},
test parallelization techniques (that schedule the input for execution across multiple
CPUs), test generation techniques~\cite{PachecoLET2007, SPLAT},
test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which take system tests as
input and create unit tests and differential unit tests,
respectively), experimental debugging techniques (such as Delta Debugging~\cite{Zeller:2002},
which requires running a set of tests repeatedly), etc. 

Many such downstream testing techniques implicitly assume that
there are no test dependences in the input test suite.  Our concern is
that this assumption can cause \emph{delayed problems} in the face
of latent test dependence in the input.  As an
example, test selection may report a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.

%Of these techniques, we are most concerned about
%those that modify the organization of test suites, rather than the tests
%they contain.

\subsection{Contributions}

At its heart, this paper addresses and questions
conventional wisdom about the test independence assumption. 
This paper makes the following contributions:

\begin{itemize}

  \item \textbf{Study.} We describe a study of \todo{NUM} real-world
  dependent tests from well-known bug repositories to show that test dependence
  arises in practice. Our study indicates that test dependence can have
  potentially costly repercussions, and can be hard to identify unless
  explicitly searched for (Section~\ref{sec:study}).

\item \textbf{Formalization.} We provide a formalization of test dependence
  in terms of test suites as ordered sequences of tests and explicit execution
  environments for test suites, which enables reasoning about test dependence
  as well as a proof that finding \emph{manifest} dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).

  \item \textbf{Algorithms.} We present two algorithms
  to detect dependent tests (Section~\ref{sec:detecting}). The first
  basic algorithm exhaustively executes all possible $k$-tests for
  a bounding parameter $k$ to identify tests exhibiting different results.
  The second improved algorithm uses static and dynamic program analyses
  to reduce the search space. \todo{check the above}

  \item \textbf{Evaluation.} We implemented our algorithms in a prototype
  tool, and applied it to \todo{NUM} real-world subject programs. Our
  tool detected a large number of important test dependences that have not
  been discovered before, showing that on average \todo{xx}\% of the human-written
  tests are dependent and \todo{xx}\% of the automatically-generated tests are dependent
   (Section~\ref{sec:evaluation}).
  Finally, we discuss a set of open questions and other possible impacts of dependent
  tests in Section~\ref{sec:discussion}.
\end{itemize}


