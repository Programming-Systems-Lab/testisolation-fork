\section{Introduction}

%\todo{Is ``dependent test'' a term that can be applied to one test in
%  isolation?  Or is a pair of tests dependent on one another if the result
%  of one of them changes if the other one is run first?  The paper doesn't
%  make this clear anywhere in sections 1 and 2, and this leads to some
%  confusion for the reader.}

%Informally, a \emph{dependent test} produces different test
%results when executed in different environments. 
Consider a test suite containing two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to \code{A} passing, while running \code{B} and then
\code{A} leads to \code{A} failing. We call \code{A}
an \textit{order-dependent} test (in the context of this test suite), since its result depends on
whether it runs after \code{B} or not.



In a test suite, all the test cases should be independent:
no test should affect any other test's result, and
running the tests in any order should produce the same test results.
%Practitioners are well aware of test dependence:  coding
%guidelines~\cite{unit-test-def,Massol:2003} and
%standards~\cite{IEEE:829-1998,IEEE:829-2008} say to avoid or document it,
%and tools support those goals~\cite{junitordering,depunit,testng, easymock, randomjunit}.
%%\todo{Cite a mocking framework}.
%Researchers are also aware of test
%dependence~\cite{Csallner:2004, Steimann:2013, Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,kapfhammeretal:FSE:2003,Wang:2007:AGC, Samimi:2013:DM}.
%%\todo{Cite research about creating mocks.}
%Nonetheless, much testing research and practice
%assumes test independence;
%this includes techniques for test selection~\cite{harroldetal:OOPSLA:2001,RenCR2006},
%test prioritization~\cite{Elbaum:2000:PTC:347324.348910},
%and test parallelization~\cite{Misailovic:2007}.
%% These are not offected:
%%, test factoring~\cite{Saff:2005}, and test carving~\cite{Elbaum:2006}.
%theoretical results, algorithms, and tools may behave unexpectedly
%in the presence of dependent tests.
%A test suite that contains dependent tests affects the applications
%of these techniques.
%These techniques produce incorrect results if run on a test suite that contains dependent tests. 
%\todo{The claim of the above sentence is too strong, and may irritate readers, given we do not have strong evidence.
%I would tone down it as: dependent tests can
%affect the results of these techniques. Further, the ``correctness''
%of a test selection/prioritization technique is decided by whether
%a test has been selected or not, rather than whether the test should
%maintain the same resutls or not.}
The assumption of of test independence 
is important so that tests behave consistently
as designed. In addition, 
many techniques assume test independence, including test
prioritization~\cite{Elbaum:2000:PTC:347324.348910, Kim:2002:HTP:581339.581357, Rummel:2005:TPR:1066677.1067016, Srivastava:2002:EPT:566172.566187, Jiang:2009:ART},
test selection~\cite{harroldetal:OOPSLA:2001, Orso:2004:SRT,
Briand:2009:ART, Zhang:2012:RMT, Nanda:2011:RTP, hsu09may},
test execution~\cite{Kim:2013:OUT, Misailovic:2007, SPLAT},
test factoring~\cite{Saff:2005, Wu:2010:LRV}, test carving~\cite{Elbaum:2006},
and experimental debugging techniques~\cite{Zeller:2002,
Steimann:2013, Zhang:2013:IMF}.
%often implicitly assume no test dependences in
%a test suite. 
\todo{check the following sentencee, people may ask
is the above paper list representative enough? be aware.}
\todo{I think it would be more compelling for this section to only cite
  papers that do not mention but implicitly assume test independence.  The
  current writing sounds like you might be cherry-picking:  why this set of
  papers?  Are the ratios representative?  But if you just give a list (as
  long as possible) of papers that don't assume it, then that makes the
  point you want to, and no one will misinterpret the list as trying to be
  representative.}
However, this critical assumption is
rarely questioned, investigated, or even mentioned:
in the above paper list,
4 papers explicitly assumed test independence~\cite{Rummel:2005:TPR:1066677.1067016, Orso:2004:SRT, harroldetal:OOPSLA:2001, Steimann:2013},
14 papers did not mention but implicitly
assumed test independence~\cite{Elbaum:2000:PTC:347324.348910,
Jiang:2009:ART, Srivastava:2002:EPT:566172.566187,
Zhang:2012:RMT, Misailovic:2007, Elbaum:2006, Saff:2005,
Zeller:2002, Briand:2009:ART, Wu:2010:LRV, Kim:2013:OUT,
Zhang:2013:IMF, Nanda:2011:RTP, hsu09may 
},
1 paper considered violation of the test independence
assumption as a threat to validity~\cite{SPLAT},
and 1 paper acknowledged the potential dependences
between tests~\cite{Kim:2002:HTP:581339.581357}. 
Anecdotally, researchers have told us that test dependence
is not a significant concern.
We wish to investigate the validity of this unverified conventional wisdom,
in order to understand whether test dependence arises in practice, 
the repercussions of dependent tests, and how to 
detect dependent tests.

%is generally ignored.
%Is this acceptable, because test dependence does
%not arise in practice?
%Is it because even when test dependence arises, there are few
%negative repercussions?
%Is it because no one has studied this problem or thought to examine it?
%Is it because the problem is important but is too hard to analyze or understand?

\subsection{Manifest Test Dependence}

%To explore these questions, 
This paper focuses on test
dependence that manifests as a
difference in test result (i.e., passing or failing) as determined by the testing oracle.
We adopt the results of the default
%% True, but a distraction.
% , usually implicit,
order of execution of a test suite as the
expected results; these are the results that a developer sees when running
the suite in the standard way. A test is dependent when there exists a possibly
reordered subsequence of the original test suite, in which
the test's result (determined by its existing testing
oracles) differs from its expected result in the
original test suite.
%
That is, manifest test dependence
%we focus on a \emph{manifest} perspective of test dependence,
requires a concrete order of the test suite that
produces {different} results than expected.  
%
%



This paper uses \textit{dependent test} as a shorthand for
\textit{manifest order-dependent test}
unless otherwise noted.
A single test may consist of setup and teardown
code, multiple statements, and multiple assertions
distributed through the test.



%\subsection{Causes of Dependent Tests}

\subsection{Causes and Repercussions}
%\todo{I merged the two original subsections into the following one. looks better?}
%\todo{also some text editing below}
%\subsection{Repercussions of Dependent Tests}
\label{sec:intro-repercussions}

%\todo{Fold this section into Section~\ref{sec:intro-repercussions}??}

Test dependence results from interactions with other tests,
as reflected in the execution environment.
Tests may make \textit{implicit} assumptions about their
execution environment -- values of global variables,
contents of files, etc. A dependent test
manifests when another alters the execution
environment in a way that invalidates those assumptions.

%A test has the potential to yield
%different test results when executed in different environments
%--- global variables with different values, differences in the file system, etc.

Why does this happen?
%As
%suggested by the principle of unit testing~\cite{Greiler:2013:SAT, Massol:2003},
Each test ought to initialize (or mock) the execution environment
and/or any resources it will use.
Likewise, after test execution, it should reset the
execution environment and external resources
to avoid affecting other tests' execution.
However, developers sometimes
%are as likely to
make mistakes when writing tests as when they are writing other code.
Even though frameworks such as
JUnit provide ways to set up the environment for a test execution and clean
up the environment afterward,
they cannot ensure that it is done
properly. This means that tests, like other code,
will have unintended and unexpected behaviors in some cases.
%And
%as programs increase in complexity, so may tests, which may
%increase the frequency of such problems in tests, which may
%in turn increase the frequency of test dependence.
%\edit{check the grammar of the above sentence}



%\todo{Where does this paragraph belong?  Here?}
%This principle is adopted and confirmed by many
%real-world developers (Sections~\ref{sec:study} and~\ref{sec:expdiscussion}).
%When it needs to interact with the execution environment,
%it should mock or carefully resetting external resources
%
%Ideally, each test should not depend on its environment, because it
%initializes any resources it will use 
%Likewise, the test should not modify its environment, because of mocks or
%resetting resources after test execution. 
%




% Our study of \dtnum real-world, confirmed dependent
% tests 
% % from \repnum software issue tracking systems
% (Section~\ref{sec:study}) identified two 
Here are three consequences of the fact that a dependent
test gives different results depending on when it is executed
during testing.

\textbf{(1)}
Dependent tests can
\emph{mask faults in a program}. Specifically, executing a test suite in the
default order does not expose the fault, whereas
executing the same test suite in a different order does. 
% We found a
One bug~\cite{clibug} in the Apache CLI library~\cite{cli}
was masked by two dependent tests
for 3 years (Section~\ref{sec:repercussion}).

\textbf{(2)}
Test dependences can lead to \emph{spurious bug reports}.
When a dependent test fails, it usually represents
%\edit{change to use the word of "represents", good? using
%"reveals" may sound like dependent test is good to improve code quality}
a weakness in the test
suite (such as failure to perform proper initialization) rather than a bug
in the program. 
When a test should pass but
fails after reordering due to the dependence,
people who are not aware of the dependence can get confused
and might report bugs.
% about the failing test,
%even though this is exactly the intended behavior.
%Programmers made these errors even though frameworks such as
%JUnit provide ways to set up the environment for a test execution and clean
%up the environment afterward.
%
As an example, the Eclipse developers
investigated a bug report~\cite{eclipsebug} in SWT for
more than a month before realizing that the 
bug report was invalid and was caused by test dependences
(i.e., a test should pass, but it failed when a user
ran tests in a different order).
%were intentional,
%allowing them to close the bug report without a change to the system.
%


%Second, guided by the findings of our study, we design two algorithms
%to detect manifest dependent tests. By applying our algorithms
%to \todo{xx} open-source programs and their test suites, we 
%found a large number of unknown dependent tests, .

\textbf{(3)}
Dependent tests can \textit{interfere with downstream testing
techniques} that change a test suite and thereby change a test's execution environment.
Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite to run during
regression testing)~\cite{harroldetal:OOPSLA:2001, Orso:2004:SRT,
Briand:2009:ART, Zhang:2012:RMT, Nanda:2011:RTP, hsu09may},
test prioritization techniques (that reorder the
input to discover defects sooner)~\cite{Elbaum:2000:PTC:347324.348910, Kim:2002:HTP:581339.581357, Rummel:2005:TPR:1066677.1067016, Srivastava:2002:EPT:566172.566187, Jiang:2009:ART},
test parallelization techniques (that schedule the input tests for execution across multiple
CPUs)~\cite{Misailovic:2007}, test execution techniques~\cite{Kim:2013:OUT},
test factoring~\cite{Saff:2005, Wu:2010:LRV} and test carving~\cite{Elbaum:2006} (which
convert large system tests into smaller unit tests),
%test generation (which re-executes suites as it builds them up)~\cite{PachecoE2005,RobinsonEPAL2011},
experimental debugging techniques (such as Delta
Debugging~\cite{Zeller:2002, Steimann:2013, Zhang:2013:IMF} and mutation
analysis~\cite{Zhang:2012:RMT, Schuler:2009:EMT, Zhang:2013:FMT},
which run a set of tests repeatedly), etc. 
Most of these downstream testing techniques implicitly assume that
there are no test dependences in the input test suite. Violation of
this assumption, as we show happens in practice, can cause unexpected
output. %\todo{change erroneous to different?} 
As an example, test prioritization may produce a reordered sequence
of tests that do not
return the same results as they do when executed in
the default order. Section~\ref{sec:impact}
provides empirical evidence to show that
dependent tests do affect the output of five test prioritization
techniques.


\subsection{Contributions}
\label{sec:contributions}

This paper addresses and questions
conventional wisdom about the test independence assumption. 
This paper makes the following contributions:

\begin{itemize}

  \item \textbf{Study.} We describe a study of \dtnum real-world
  dependent tests from \repnum software issue tracking
  systems to characterize dependent tests that
  arise in practice.  Test dependence can have
  potentially non-trivial repercussions and can be hard to identify
  (Section~\ref{sec:study}).

\item \textbf{Formalization.} We formalize test dependence
  in terms of test suites as ordered sequences of tests and explicit execution
  environments for test suites.  The formalization enables reasoning about test dependence
  as well as a proof that finding manifest dependent tests is an NP-complete
  problem (Section~\ref{sec:formalism}).

  \item \textbf{Algorithms.} We present three algorithms
  to detect dependent tests:
  one randomized, one exhaustive bounded, and one that prunes the search
  space using dynamic analyses.
  All three algorithms are \emph{sound} but \emph{incomplete}:
  every dependent test they identify is real, but the algorithms
  do not guarantee to find all dependent tests (Section~\ref{sec:detecting}). 
  %\edit{check above when the algorithm section is written}

  \item \textbf{Evaluation.} We implemented our algorithms in a prototype
  tool, called \ourtool (Section~\ref{sec:impl}).
  \ourtool detected 27 previously-unknown dependent tests in human-written
  unit tests in \subjnum real-world subject programs.
  % (and even more in automatically-generated tests).
  The developers confirmed all of these as
  undesired (Section~\ref{sec:evaluation}).

  %\item \textbf{Assessment.} 
  \item \textbf{Impact Assessment.} We implemented five test prioritization
  techniques and evaluated them on \subjnum subject programs
  that contain dependent tests. The results show that all
  five test prioritization techniques are affected by dependent tests
  (Section~\ref{sec:evaluation}).

  % \textit{every} subject program we studied, from both  and automatically-generated
  % unit tests (Section~\ref{sec:evaluation}).
  %been discovered before, showing that on average \todo{xx}\% of the human-written
  %unit tests are dependent and \todo{xx}\% of the automatically-generated
  %unit tests are dependent
  %Finally, we discuss a set of open questions and other possible impacts of dependent
  %tests in Section~\ref{sec:discussion}.
\end{itemize}


%  LocalWords:  Kapfhammer Soffa subsequence SWT CLI NUM dependences

%  LocalWords:  teardown
