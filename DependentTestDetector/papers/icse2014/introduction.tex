\section{Introduction}

\todo{Is ``dependent test'' a term that can be applied to one test in
  isolation?  Or is a pair of tests dependent on one another if the result
  of one of them changes if the other one is run first?  The paper doesn't
  make this clear anywhere in sections 1 and 2, and this leads to some
  confusion for the reader.}

Informally, \emph{dependent tests} produce different test results when
executed in different contexts. An example is two tests \code{A}
and \code{B}, where running \code{A} in isolation leads
to \code{A} passing, while running \code{B} and then
\code{A} leads to \code{A} failing. We call \code{A}
a dependent test, since its result depends on
whether it runs after \code{B} or not.

%and then \code{B} leads
%to both tests passing, while running \code{B} and then
%\code{A} leads to one or both tests failing.

\edit{I changed the above example based on the comments below.}

\todo{Would a better example focus on just one test \code{B}:  it produces
  a different result (succeed/fail) depending on whether test \code{A} is run
  before it or not.  I'm not sure whether this would be an improvement or
  not; it perhaps depends on the intended definition of ``dependent test'',
  which is not clear.}


%~\cite{KapfhammerS03}
%Chays:2000:FTD:347324.348954,
%Gray:1994:QGB:191843.191886}, 
\todo{find a place to emphasize unit tests}

Test independence is a common assumption in testing research and practice:
theoretical results, algorithms, and tools all fail or behave unexpectedly
in the presence of dependent tests.
However, this critical assumption is rarely questioned or investigated.
\todo{Probably revise the below list.}
Is this acceptable, because test dependence does
not arise in practice?
Is it acceptable because even when test dependence arises, there are few
negative repercussions?
Is it because the problem is important but no one has thought to examine it?
Is it because the problem is important but is too hard to analyze or understand?
\todo{note, the above questions must be recalled and discussed after
presenting the study and evaluation. do not forget.}

\subsection{Manifest Test Dependence}

To explore these questions, we consider a narrow characterization
of test dependence that:
\begin{itemize}
\item Adopts the results of the default, usually implicit,
  order of execution of a test suite as the \emph{expected results}. 
\item Asserts \emph{test dependence} when there is a possibly
  reordered subsequence of the original test suite that, when
  executed, has at least one test result that differs from the
  expected result for that test.  
\end{itemize}
That is, we focus on a \emph{manifest} perspective of test dependence,
requiring a \emph{concrete} variant of the test suite that
\emph{dynamically} produces different results from the expected.  

\edit{Three places particularly require consistence: (1)
test dependence reveals \todo{defect, or flaws, or smells}
in test code, (2) test dependence can mask \todo{bug, defect,
or fault}, and (3) test dependence, or dependent tests. seems
these 2 phrases are used interchangably.
In addition, should it be test dependence, or test
dependences?}

\todo{We need a consistent set of terminology.  I like the following, 
adapted from \\
http://en.wikipedia.org/wiki/Dependability:
\begin{itemize}
\item
 Defect:
      A flaw, failing, or imperfection in a system, such as an
      incorrect design, algorithm, or implementation.  Also known as a
      fault or bug.  Typically caused by a human mistake.
\item
 Error:
      An error is a discrepancy between the intended behavior of a
      system and its actual behavior inside the system boundary.  Not
      detectable without assert statements and the like.
\item
 Failure:
      A failure is an instance in time when a system displays
      behavior that is contrary to its specification.
\end{itemize}
but if we use something else (which would be fine), we should define the terms.}


\subsection{Test Execution Environment}

Test dependence results from interactions with the execution environment.
Specifically, when a
test is executed in different environments --- global variables
with different values, differences in the file system, etc. --- it has the
potential to yield
a different result.  

Ideally, each test does not depend on its environment, because it
initializes any resources it will use and/or uses mocked resources.
Likewise, the test does not modify its environment, because of mocks or
resetting resources after test execution.

In practice, developers are as likely
to make mistakes when writing tests as when they are writing other code.
As shown in Sections~\ref{sec:study} and~\ref{sec:evaluation}, most of the dependences we found
stem from incorrect or incomplete initialization
of the program environment.
Programmers made these errors even though frameworks such as
JUnit provide ways to set up the environment for a test execution and clean
up the environment afterward.

% that facilitate the process of clean setup by providing means to
% automatically execute methods (\code{setUp()} and \code{tearDown()} in JUnit
% 3.x, and methods annotated with \code{@Before} and \code{@After} in
% JUnit 4.x) that should handle all common setup and clean-up between
% test cases. 


% And while frameworks make it easier to get environment setup right, 
% they cannot ensure that it is done properly. 
% Like with other code, this means that tests in some cases will have
% unintended and unexpected behaviors.  And as programs increase in complexity,
% so may tests, which may increase the frequency of such problems in tests,
% which may in turn increase the frequency of test dependence.
% The situation is exacerbated by the fact that 

Test dependence is usually caused by
global variables shared by tests.
%These tests
%shared global variables, and the test results varied depending on the
%values stored in these variables.  
%That is, relevant execution conditions --- specifically, pertinent parts
%of the implicit \emph{environment} comprising global variables, the file system, operating system services, etc. --- were neglected.
Buried deep in the program code, and the assertions
do not directly check the global variables,
but rather check values that have been computed from
them.
Developers don't notice the dependencies until they are deep in debugging a
subtle bug or test failure.

%Therefore, we explicitly
%distinguish potential test dependences (Definition~\ref{def:dependency}) --- those that could cause a variation in test suite results 
%under \emph{some} environment and order --- and manifest test
%dependences (Definition~\ref{def:manifest}) --- those that are guaranteed to cause a
%variation in test suite results under a \emph{specific} environment and order.  


\subsection{Repercussions of Dependent Tests}

%We have identified a number of substantive examples of test suites
%from fielded programs that manifest dependences.
%in fielded programs. 
%with test suites that manifest test dependence.  

%In this paper, we seek to understand dependent tests and their repercussions
%in two ways. 

Our study of \dtnum real-world, confirmed dependent
tests 
% from \repnum software issue tracking systems
(Section~\ref{sec:study}) identified three
major consequences of dependent tests.
%
First, dependent tests usually indicate a
\emph{weaknesses in the test suite}. Exposing
dependent tests can detect code smells and identify situations
where some tests do not perform proper initialization.
%
Second, test dependences can lead
to \emph{spurious bug reports}. As an example, the Eclipse developers
investigated a dependent test~\cite{eclipsebug} in SWT for
more than a month before realizing that the test dependences were intentional,
allowing them to close the bug report without a change to the system.
%
Third, dependent tests can
\emph{mask faults in a program}. Specifically, executing a test suite in the
default order does not expose the fault, whereas
executing the same test suite in a different order does. We found 
a bug~\cite{clibug} that had been masked by two dependent tests
for 3 years in the Apache CLI library~\cite{cli} (Section~\ref{sec:repercussion}).

%Second, guided by the findings of our study, we design two algorithms
%to detect manifest dependent tests. By applying our algorithms
%to \todo{xx} open-source programs and their test suites, we 
%found a large number of unknown dependent tests, .

Dependent tests also interfere with downstream testing
techniques that change a test's execution context.
Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite to run during
regression testing)~\cite{harroldetal:OOPSLA:2001},
test prioritization techniques (that reorder the
input to discover defects sooner)~\cite{Elbaum:2000:PTC:347324.348910},
test parallelization techniques (that schedule the input tests for execution across multiple
CPUs), test generation techniques~\cite{PachecoLET2007, SPLAT},
test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which take system tests as
input and create unit tests and differential unit tests,
respectively), experimental debugging techniques (such as Delta Debugging~\cite{Zeller:2002},
which requires running a set of tests repeatedly), etc. 

Many such downstream testing techniques implicitly assume that
there are no test dependences in the input test suite.  Violation of
this assumption, as we show happens in practice, can cause \emph{delayed problems} in the face
of latent test dependence in the input.  As an
example, test selection may produce a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.

%Of these techniques, we are most concerned about
%those that modify the organization of test suites, rather than the tests
%they contain.

\subsection{Contributions}

At its heart, this paper addresses and questions
conventional wisdom about the test independence assumption. 
This paper makes the following contributions:

\begin{itemize}

  \item \textbf{Study.} We describe a study of \dtnum real-world
  dependent tests from \repnum software issue tracking
  systems to show that test dependence
  arises in practice. Our study identifies common
  characteristics of dependent tests, and
  it indicates that test dependence can have
  potentially costly repercussions and can be hard to identify unless
  explicitly searched for (Section~\ref{sec:study}).

\item \textbf{Formalization.} We formalize test dependence
  in terms of test suites as ordered sequences of tests and explicit execution
  environments for test suites.  The formalization enables reasoning about test dependence
  as well as a proof that finding \emph{manifest} dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).

  \item \textbf{Algorithms.} We present two algorithms
  to detect dependent tests (Section~\ref{sec:detecting}). The first
  basic algorithm exhaustively executes all possible $k$-tests for
  a bounding parameter $k$ to identify tests exhibiting different results.
  The second improved algorithm uses static and dynamic program analyses
  to reduce the search space. \todo{check the above}
  \todo{Say something brief about the guarantees.  In particular, the
    algorithms find test dependences but don't guarantee to find all test
    dependences.}

  \item \textbf{Evaluation.} We implemented our algorithms in a prototype
  tool, and applied it to \todo{NUM} real-world subject programs. Our
  tool detected a large number of important test dependences that have not
  been discovered before, showing that on average \todo{xx}\% of the human-written
  tests are dependent and \todo{xx}\% of the automatically-generated tests are dependent
   (Section~\ref{sec:evaluation}).
  Finally, we discuss a set of open questions and other possible impacts of dependent
  tests in Section~\ref{sec:discussion}.
\end{itemize}



%  LocalWords:  Kapfhammer Soffa subsequence SWT CLI NUM
