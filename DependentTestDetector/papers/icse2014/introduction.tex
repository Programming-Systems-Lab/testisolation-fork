\section{Introduction}

Informally, \emph{dependent tests} produce different test results when
executed in different contexts. %, while \emph{independent tests} produce
%the same test results regardless of execution order.  
It is easy to
construct an example of dependence between two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to both tests passing, while running \code{B} and then
\code{A} leads to either or both tests failing---the order
of applying the tests, in this case, changes the execution context.

%~\cite{KapfhammerS03}
%Chays:2000:FTD:347324.348954,
%Gray:1994:QGB:191843.191886}, 

Definitions in the testing literature are generally clear that the
conditions under which a test is executed may affect its result.  The
importance of context in testing has been explored in some depth in
some domains including databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,
kapfhammeretal:FSE:2003}, with results about test
generation, test adequacy criteria, etc., and mobile
applications~\cite{Wang:2007:AGC}.
For the database domain, Kapfhammer and Soffa formally
define and distinguish independent test suites from those that are
\emph{non-restricted\/} and thus ``can capture more of an application's
interaction with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations of test
adequacy''~\cite[p.~101]{kapfhammeretal:FSE:2003}.

At the same time, there is little focus on the core
issues of
test dependence itself.
Is this because test dependence does
not arise in practice (beyond domains such as databases)?  Is it because, even if-and-when it does arise, there
are few if any repercussions?  Is it because it is difficult to
notice if-and-when it arises?

\subsection{Manifest Test Dependence}

To explore these questions, we consider a narrow characterization
of test dependence that:
\begin{itemize}
\item Adopts the results of the default, usually implicit,
  order of execution of a test suite as the \emph{expected results}. 
\item Asserts \emph{test dependence\/} when there is a possibly
  reordered subsequence of the original test suite that, when
  executed, has at least one test result that differs from the
  expected result for that test.  
\end{itemize}
That is, we focus on a \emph{manifest\/} perspective of test dependence,
requiring a \emph{concrete\/} variant of the test suite that
\emph{dynamically\/} produces different results from the expected.  Our
definition differs from that of Kapfhammer and Soffa by considering
test results rather than program and database states.
As we discuss later, considering only manifest test dependences allows
us to more easily situate this research in the empirical domain.

%: (a) given the lack of
%attention to test dependence, reporting solely on potential but
%unrealized dependence, that is, ``false positives'', might be of little value; and
%(b) computational advantages arise from computing manifest rather than
%potential dependence.

\subsection{Examples and Repercussions}

We have identified a number of substantive examples of test suites
from fielded programs that manifest dependences.
%in fielded programs. 
%with test suites that manifest test dependence.  
We examined
six projects and found in their human-written test suites a total
of 75 dependent tests ($1.4 \%$). For the same set of
programs, we also generated test suites automatically using
Randoop~\cite{PachecoLET2007} and found that on average $14 \%$ of
the generated tests are dependent.

%Further, considering the increasing importance of
%automated test generation techniques, we wanted to get an impression
%of whether test dependence occurs in automatically generated test
%suites. We applied Randoop to the same
%set of programs and 

By analyzing these examples of test dependence, we identified three
categories of problems that can arise due to the presence of dependent tests.
First, test suites that unexpectedly contain dependent tests can
\emph{mask faults in a program}.  We present examples where
executing a test suite in the default order does not expose the fault, whereas
executing the same test suite in a different order does.
Second, test suites that unexpectedly contain dependent tests can \emph{conceal
weaknesses in the test suite} itself.  We present examples where exposing
dependent tests can identify situations where some tests do not perform
proper initialization.
Third, a test suite containing undocumented test dependences can lead
to \emph{spurious bug reports}.  We present an example where it took the developers
more than a month to realize that the test dependences were intentional,
allowing them to close the bug report without a change to the system.

%In practice, test suites that are thought to include only independent
%tests but that manifest dependence can cause problems including:
%\begin{itemize}
%\item masking faults in the program that are not exposed in one execution order but that are
%in another order; 
%\item exhibiting unexpected
%results if reordered (for instance, by downstream testing techniques such as
%test selection or prioritization), a likely indication of poor test construction; and,
%\item reporting of spurious bugs.
%% if the tests are intended to be dependent but the dependence
%%is undocumented. \todo{DN}{I'm thinking of removing this bullet from the intro.
%%It's a bit different, in that the test writers understand the dependence, of course.
%%I'm just afraid that it'll increase confusion instead of clarifying things.}
%\end{itemize}

%As an example of the first category, the JodaTime library
%(Section~\ref{sec:jodatime}) defines a complex caching system.  Its
%test suite includes tests that check the rather complicated function that normalizes
%object states into cache keys.
%However, an unexpected test dependence between two tests masks a bug in this code. The default
%test execution order exercises an unintended path for one test because an object is
%already cached due to a previously executed test; the fault is exposed if that object
%is not initially cached, as would happen
%if the two tests
%are executed in the reverse order.
%Examples of the other categories are
%described in Section~\ref{sec:examples}.

%Test suites with unknown dependent tests may also exhibit unexpected
%results if reordered by downstream testing techniques such as
%test selection or prioritization  In addition, undocumented
%but desired test dependence can
%lead to spurious bug reports.

\subsection{Test Execution Environment}

Our examples highlight varying execution environments as the
unsurprising central cause of test dependence. Specifically, when a
test is executed in different environments---global variables
with different values, differences in the file system, differences in
data obtained from web services, etc.---it has the potential to return
a different result.  
%
%Changing a test suite's execution order can
%increase the potential to change the execution environment for a given test:
%different tests may be executed before that given test, and they may
%produce an environment that may cause the test to have a different result.
Most of the dependences we see in our
examples ultimately stem from incorrect or incomplete initialization
of the program environment.

Why does this happen? Especially given frameworks such as
%
%It is justified to argue that developers 
%should take the utmost care to initialize their tests correctly and completely. In principle,
%such setup code could be part of each individual test case. Frameworks
JUnit that facilitate the process of clean setup by providing means to
automatically execute methods (\code{setUp()} and \code{tearDown()} in JUnit
3.x, and methods annotated with \code{@Before} and \code{@After} in
JUnit 4.x) that should handle all common setup and clean-up between
test cases. 

It appears that the answer is that developers are as likely
to make mistakes when writing tests as when they are writing other code.
And while frameworks make it easier to get environment setup right, 
they cannot ensure that it is done properly. 
%The frameworks provide effective mechanisms for setting test execution
%environments, but they do not ensure that these mechanisms are used properly.
Like with other code, this means that tests in some cases will have
unintended and unexpected behaviors.  And as programs increase in complexity,
so may tests, which may increase the frequency of such problems in tests,
which may in turn increase the frequency of test dependence.
%And the more complex a programs structure
%is, the more likely it is that some initialization of global variables
%will be forgotten. By identifying test dependence
%as a more broadly discussed issue, and by providing algorithms and
%tools for identifying test dependences, we hope to reduce their


%Yet no framework can ensure that these methods are used
%correctly. 
%Since we are most interested in practical issues, we argue that
%developers are as likely to make mistakes when writing tests, as they
%are when writing code. 
%\todo{mark it red to avoid been overlooked}{frequency and their cost.}

%\subsection{Downstream Testing Tools}

Another situation in which the underlying test execution context can
unexpectedly change is when a tool or technique that takes a test
suite as input is used.  Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite that will guarantee some properties during
regression testing)~\cite{harroldetal:OOPSLA:2001}, test prioritization techniques (that reorder the
input to increase the rate of fault detection)~\cite{Elbaum:2000:PTC:347324.348910}, test parallelization
techniques (that schedule the input for execution across multiple
CPUs), test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which take system tests as
input and create unit tests and differential unit tests,
respectively), etc. 

Of these techniques, we are most concerned about
those that modify the organization of test suites, rather than the tests
they contain.
Many such downstream testing techniques implicitly assume that
there are no test dependences in the input test suite.  Our concern is
that this assumption can cause \emph{delayed problems} in the face
of latent test dependence in the input.  As an
example, test selection may report a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.
%In essence, these tools may have an unstated precondition---''the input
%must have no test dependences''---that may not be checked or satisfied in some cases.
%An alternative would be for the tools to detect and eliminate dependences.
%
%If this selection happened for regression testing, developers may be
%led to investigate only modified and newly added code to find the
%fault. But it is possible that the fault lies
%elsewhere and has not been discovered due to the dependence in the
%test suite. \todo{KM}{I don't buy this. Once you have the failing test, it
%should not be very hard for a reasonable developer to find the real reason
%behind the failure.}

%
%The two fundamental operations that such tools can apply to test
%suites are sub-suite selection, and suite reordering. Our
%formalization in Section~\ref{sec:formalism} and the examples in
%Section~\ref{sec:examples} show that both these operations can
%produce test suites that exhibit manifest dependences, because all
%these operations lead to tests being executed in potentially different
%environments. While there are some differences between selection and
%re-ordering, these are not significant for the following discussion. 
%Therefore, in Section~\ref{sec:related} we consider only
%related work in test prioritization as a representative of this
%entire class of techniques. %, especially due to its concern with reordering.
%

%The value of studying manifest dependence lies in the fact that
%it can impact second
%order techniques, such as test prioritization or parallelization.
%One premise of such techniques is usually that executing tests in any
%order or in parallel will produce the same results. Therefore, we
%consider as test dependence, effects that cause the results of tests
%to differ when they are executed in different environments.
%Such test dependence may arise when the test results rely on a particular
%context that may unexpectedly differ from one execution order to
%another (Figure~\ref{fig:downstream}).  
%For example, if test \code{A} assumes that a global variable has been
%initialized by some other test, executing test \code{A} before those
%tests may cause different test results.
%Conversely, tests are independent when
%they either do not rely on context at all, or assure the correct
%context before executing.



% \todo{KM}{I don't think what this paragraph says is true. I recommend cutting
% it (the above paragraph also contains what it tries to say).} Our examples also
% show extensive use of test infrastructure that can reduce test dependence: in
% JUnit\footnote{\url{http://www.junit.org}}, \code{setUp()} and
% \code{tearDown()} as well as \code{@before} and \code{@after}
% annotations help developers significantly but allowing them to more
% easily establish the execution environment.  However, these and
% related features are mechanisms only and are not intended to, and do
% not, enforce any policies to ensure that developers use the
% mechanisms consistently and effectively.
% 
% Why do some tests allow varying execution environments?  Can't this
% be easily avoided?
% We contend
% that this is for the same reasons that developers still create bugs,
% still don't always initialize program variables, still don't always check
% array lengths before indexing, etc.  By identifying test dependence
% as a more broadly discussed issue, and by providing algorithms and
% tools for identifying test dependences, we hope to reduce their
% frequency and their cost.

\subsection{Contributions}

At its heart, this paper addresses and questions conventional wisdom about the
test independence assumption.  It is intended to balance a precise 
characterization of test dependence with substantive empiric examples and concerns. The contributions of the paper include:
\begin{itemize}
%  \item Evidence from the literature that test independence is broadly assumed but rarely addressed (Section~\ref{sec:related}).
  \item A precise formalization of test dependence in terms of test suites as ordered sequences
  of tests and explicit execution environments for test suites, which enables reasoning about test dependence
  as well as a proof that finding \emph{manifest} dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).
  \item Examples from fielded software of test suites where manifest test
  dependences lead to identifiable concerns with the underlying programs or test suites (Section~\ref{sec:examples}).
  \item Motivation for and presentation of our initial approximate algorithms and a supporting tool for identifying test dependences (Section~\ref{sec:algorithm-tool}).
\end{itemize}

%: in practice, it does not always hold and, 
%this can cause problems. 
%We provide evidence
%from fielded software that shows that, while perhaps uncommon, test
%dependence arises in practice. 
Although we provide evidence that test dependence unexpectedly arises in practice,  
a broad
study of how often test dependences arise, and the costs that these may lead to,
is beyond the scope of this paper. 
We conclude the paper with a set of open questions addressing this and
other possible concerns in Section~\ref{sec:questions}.

%While superficially straightforward, reasoning about test dependence
%%and the potential causes and consequences of test dependence is
%is intricate and non-trivial. We introduce a formalism to help
%understand and reason about test dependence.  The two key aspects
%of the formalism are (a) defining
%test
%suites as ordered sequences of tests and (b) making explicit the
%context in which tests are run. The formalism provides a precise
%basis for defining test dependence, for proving the (NP-hard) complexity
%of determining if a suite can manifest test dependences, and for
%algorithms that can efficiently identify important classes of dependences.
%
%
%may arise due to the context 
%in which tests are
%executed (Section~\ref{sec:formalism} formalizes context). Hence, the
%notion and use of context is the second fundamental part of our
%formalism.


%We raise awareness of the problems caused by the test
%  independence assumption but demonstrating through theory and
%  manifestations of test dependence that the consequences can be
%  severe. 
%  \item We define a formalism to reason about test suites as sequences
%  and show how dependences arise in theory and practice.
%  \item We lay a foundation for efficient heuristic algorithms to
%  detect dependences in existing test suites and show with some
%  examples that heuristics rather than exhaustive algorithms already
%  have signigificant benefits.


%\todo{JW}{I couldn't fit this into the rewritten intro. I might like
%to use in in Sec 4 or 5}
%The two ways
%  of altering context that we address here are \emph{isolation} and
%  \emph{ordering}.  By isolation, we mean executing each test in a
%  test suite separately: for example, in a different instance of JUnit
%  or in a different virtual machine.  This isolates, and may provide a
%  different context for a test, by ensuring that the initial context
%  is reinitialized for each test.  In contrast, most conventional
%  approaches execute tests in a sequence in the same context, giving
%  (for example) the second test an execution context that can in
%  principle depend in part on how the first test may have modified the
%  context.  By ordering (which as we show in
%  Section~\ref{sec:formalism} is strictly more general than
%  isolation), we mean that the sequence in which tests in a test suite
%  are executed can be varied.  A different ordering of test execution
%  can cause a given test to execute in a different context and,
%  perhaps, provide a different result.
%

%\begin{itemize}
%  \item Why do we think knowing about dependencies is important?
%
%Give a neat, clear, if constructed example.
%
%\item Are they a real problem?
%
%  \item What do we do about them?
%\end{itemize}
