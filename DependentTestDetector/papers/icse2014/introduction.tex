\section{Introduction}

%\todo{Is ``dependent test'' a term that can be applied to one test in
%  isolation?  Or is a pair of tests dependent on one another if the result
%  of one of them changes if the other one is run first?  The paper doesn't
%  make this clear anywhere in sections 1 and 2, and this leads to some
%  confusion for the reader.}

%Informally, a \emph{dependent test} produces different test
%results when executed in different environments. 
Consider a test suite containing two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to \code{A} passing, while running \code{B} and then
\code{A} leads to \code{A} failing. We call \code{A}
an \textit{order-dependent} test (in the context of this test suite), since its result depends on
whether it runs after \code{B} or not.



In a unit test suite, all the test cases should be independent:
no test should affect any other test's result, and
running the tests in any order should produce the same test results.
Practitioners are well aware of test dependence:  coding
guidelines~\cite{unit-test-def,Massol:2003} and
standards~\cite{IEEE:829-1998,IEEE:829-2008} say to avoid or document it,
and tools support those goals~\cite{junitordering,depunit,testng}\todo{Cite
  a mocking framework}.  Researchers are also aware of test
dependence~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,kapfhammeretal:FSE:2003,Wang:2007:AGC}.\todo{Cite research about creating mocks.}
Nonetheless, much testing research and practice
assumes test independence;
this include techniques for test selection~\cite{harroldetal:OOPSLA:2001,RenCR2006},
test prioritization~\cite{Elbaum:2000:PTC:347324.348910},
and test parallelization~\cite{Misailovic:2007}.
%% These are not offected:
%%, test factoring~\cite{Saff:2005}, and test carving~\cite{Elbaum:2006}.
%theoretical results, algorithms, and tools may behave unexpectedly
%in the presence of dependent tests.
These techniques produce incorrect results if run on a test suite that contains dependent tests.
The critical assumption of test independence is rarely questioned or investigated.
We wish to investigate the validity of this unverified conventional wisdom,
in order to understand whether test dependence arises in practice, 
the repercussions of dependent tests, and how to 
detect dependent tests.

%is generally ignored.
%Is this acceptable, because test dependence does
%not arise in practice?
%Is it because even when test dependence arises, there are few
%negative repercussions?
%Is it because no one has studied this problem or thought to examine it?
%Is it because the problem is important but is too hard to analyze or understand?

\subsection{Manifest Test Dependence}

%To explore these questions, 
This paper focuses on test
dependence that manifests as a
difference in test outcome as determined by the test oracle.
We adopt the results of the default
%% True, but a distraction.
% , usually implicit,
order of execution of a test suite as the
expected results. A test is dependent when there is a possibly
reordered subsequence of the original test suite, in which
the test's result (determined by its existing testing
oracles) differs from its expected result in the
original test suite.

That is, manifest test dependence
%we focus on a \emph{manifest} perspective of test dependence,
requires a concrete order of the test suite that
produces {different} results than expected.  
%
We work with real, existing test suites to determine the practical impact
and prevalence of dependent tests.
%
(One alternative would be to consider tests dependent if reordering could
affect any internal computation or heap value; but these internal details
do not affect the test outcome.  Another alternative would be to ask
whether it is possible to write a new dependent test for an existing
program; but the answer to this question is trivially ``yes''.)


This paper uses \textit{dependent test} as a shorthand for
\textit{manifest order-dependent test}
unless otherwise noted.
%
A single test may consist of setup and teardown
code, multiple statements, and multiple assertions
%% True, but a distraction
% (the oracle)
distributed
through the test.


%As we discuss later, considering only manifest test dependences allows
%us to more easily situate this research in the empirical domain.
%\todo{manifest is more challenging}
%test results rather than program and database states.



\subsection{Causes of Dependent Tests}

\todo{Fold this section into Section~\ref{sec:intro-repercussions}??}

Test dependence results from interactions with other tests, as reflected
in the execution environment.
A test has the potential to yield
different test results when executed in different environments --- global variables
with different values, differences in the file system, etc.
%% I don't see how this supports our main argument.
% Testing oracles usually do not directly check all global variables,
% but rather check values that have been computed from
% them. Developers don't notice the dependencies until they are deep in debugging a
% subtle bug or test failure.


\todo{Where does this paragraph belong?  Here?}
As a principle of unit testing~\cite{Massol:2003}, each unit
test should properly initialize (or mock) the execution environment
and/or any resources it will use.
Likewise, after test execution, it should reset the
execution environment and external resources
to avoid affecting other tests' execution.
%This principle is adopted and confirmed by many
%real-world developers (Sections~\ref{sec:study} and~\ref{sec:expdiscussion}).
%When it needs to interact with the execution environment,
%it should mock or carefully resetting external resources
%
%Ideally, each test should not depend on its environment, because it
%initializes any resources it will use 
%Likewise, the test should not modify its environment, because of mocks or
%resetting resources after test execution. 
%
However, in practice,
developers are as likely
to make mistakes when writing tests as when they are writing other code.



\subsection{Repercussions of Dependent Tests}
\label{sec:intro-repercussions}

% Our study of \dtnum real-world, confirmed dependent
% tests 
% % from \repnum software issue tracking systems
% (Section~\ref{sec:study}) identified two 
Here are three consequences of the fact that a dependent test gives different
results depending on when it is executing during testing.

\textbf{(1)}
Dependent tests can
\emph{mask faults in a program}. Specifically, executing a test suite in the
default order does not expose the fault, whereas
executing the same test suite in a different order does. 
% We found a
One bug~\cite{clibug} in the Apache CLI library~\cite{cli}
was been masked by two dependent tests
for 3 years (Section~\ref{sec:repercussion}).

\textbf{(2)}
Test dependences can lead to \emph{spurious bug reports}.
When a dependent test fails, it usually reveals a weakness in the test
suite (such as failure to perform proper initialization) rather than a bug
in the program.
Programmers made these errors even though frameworks such as
JUnit provide ways to set up the environment for a test execution and clean
up the environment afterward.
%
As an example, the Eclipse developers
investigated a bug report~\cite{eclipsebug} in SWT for
more than a month before realizing that the 
bug report was invalid and was caused by test dependences
(i.e., a test should pass, but it failed when a user
ran tests in a different order).
%were intentional,
%allowing them to close the bug report without a change to the system.
%


%Second, guided by the findings of our study, we design two algorithms
%to detect manifest dependent tests. By applying our algorithms
%to \todo{xx} open-source programs and their test suites, we 
%found a large number of unknown dependent tests, .

\textbf{(3)}
Dependent tests can interfere with downstream testing
techniques that change a test suite and thereby change a test's execution environment.
Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite to run during
regression testing)~\cite{harroldetal:OOPSLA:2001},
test prioritization techniques (that reorder the
input to discover defects sooner)~\cite{Elbaum:2000:PTC:347324.348910},
test parallelization techniques (that schedule the input tests for execution across multiple
CPUs), test execution techniques~\cite{Misailovic:2007, SPLAT},
test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which
convert large system tests into smaller unit tests),
test generation (which re-executes suites as it builds them up)~\cite{PachecoE2005,RobinsonEPAL2011},
experimental debugging techniques (such as Delta
Debugging~\cite{Zeller:2002} and mutation analysis\todo{add citation},
which run a set of tests repeatedly), etc. 
Most of these downstream testing techniques implicitly assume that
there are no test dependences in the input test suite. Violation of
this assumption, as we show happens in practice, can cause erroneous
output.  As an
example, test selection may produce a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.


\subsection{Contributions}
\label{sec:contributions}

This paper addresses and questions
conventional wisdom about the test independence assumption. 
This paper makes the following contributions:

\begin{itemize}

  \item \textbf{Study.} We describe a study of \dtnum real-world
  dependent tests from \repnum software issue tracking
  systems to characterize dependent tests that
  arises in practice.  Test dependence can have
  potentially costly repercussions and can be hard to identify
  (Section~\ref{sec:study}).

\item \textbf{Formalization.} We formalize test dependence
  in terms of test suites as ordered sequences of tests and explicit execution
  environments for test suites.  The formalization enables reasoning about test dependence
  as well as a proof that finding manifest test dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).

  \item \textbf{Algorithms.} We present three algorithms
  to detect dependent tests (Section~\ref{sec:detecting}):
  one randomized, one exhaustive bounded, and one that prunes the search
  space based on dynamic dependences.
  All three algorithms are \emph{sound} but \emph{incomplete}:
  every dependent test they identify is real, but the algorithms
  do not guarantee to find all dependent tests. 
  %\edit{check above when the algorithm section is written}

  \item \textbf{Evaluation.} We implemented our algorithms in a prototype
  tool, called \ourtool (Section~\ref{sec:impl}).
  \ourtool detected 27 previously-unknown dependent tests in human-written
  unit tests in \subjnum real-world subject programs.
  % (and even more in automatically-generated tests).
  The developers confirmed all of these as
  undesired.

  % \textit{every} subject program we studied, from both  and automatically-generated
  % unit tests (Section~\ref{sec:evaluation}).
  %been discovered before, showing that on average \todo{xx}\% of the human-written
  %unit tests are dependent and \todo{xx}\% of the automatically-generated
  %unit tests are dependent
  %Finally, we discuss a set of open questions and other possible impacts of dependent
  %tests in Section~\ref{sec:discussion}.
\end{itemize}


%  LocalWords:  Kapfhammer Soffa subsequence SWT CLI NUM dependences

%  LocalWords:  teardown
