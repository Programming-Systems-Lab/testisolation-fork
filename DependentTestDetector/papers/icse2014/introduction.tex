\section{Introduction}

%\todo{Is ``dependent test'' a term that can be applied to one test in
%  isolation?  Or is a pair of tests dependent on one another if the result
%  of one of them changes if the other one is run first?  The paper doesn't
%  make this clear anywhere in sections 1 and 2, and this leads to some
%  confusion for the reader.}

Informally, a \emph{dependent test} produces different test
results when executed in different contexts. Consider a test
suite containing two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to \code{A} passing, while running \code{B} and then
\code{A} leads to \code{A} failing. We call \code{A}
a dependent test (in the context of this suite), since its result depends on
whether it runs after \code{B} or not.


In a test suite, all tests should be independent, and
running the tests in any order should produce the same test results.
Test independence is a common assumption in testing research and practice:
theoretical results, algorithms, and tools all fail or behave unexpectedly
in the presence of dependent tests.
However, this critical assumption is rarely questioned or investigated. Test
independence is generally ignored.
Is this acceptable, because test dependence does
not arise in practice?
Is it acceptable because even when test dependence arises, there are few
negative repercussions?
Is it because no one has noticed this problem or thought to examine it?
Is it because the problem is important but is too hard to analyze or understand?
\todo{donot forget to recall above questions in the paper later.}

\subsection{Manifest Test Dependence}

This paper focuses on test dependence that manifests as a \textit{user-visible}
difference. We adopt the results of the default, usually
implicit, order of execution of a test suite as the
\textit{expected results}. A test is dependent when there is a possibly
reordered subsequence of the original test suite, in which
the test's result (determined by its existing testing
oracles) \textit{differs} from its expected result in the
original test suite.

That is, we focus on a \emph{manifest} perspective of test dependence,
requiring a \emph{concrete} order of the test suite that
produces \emph{different} and \emph{user-visible} results from the expected.  
%
We work with real, existing test suites rather than wondering
whether there could be any hypothetical suite 
or environment that can lead to different program states.

In the following sections of this paper, we use \textit{dependent tests}
to represent \textit{manifest dependent tests} for brevity
unless explicitly mentioned.


%As we discuss later, considering only manifest test dependences allows
%us to more easily situate this research in the empirical domain.
%\todo{manifest is more challenging}
%test results rather than program and database states.



\subsection{Causes of Dependent Tests}


Test dependence results from interactions with other tests, as reflected
in the execution environment.
Specifically, when a
test is executed in different environments --- global variables
with different values, differences in the file system, etc: --- it has the
potential to yield
a different result.  

Ideally, each test should not depend on its environment, because it
initializes any resources it will use and/or uses mocked resources.
Likewise, the test should not modify its environment, because of mocks or
resetting resources after test execution. However, in practice,
developers are as likely
to make mistakes when writing tests as when they are writing other code.
As shown in Sections~\ref{sec:study} and~\ref{sec:evaluation}, most of the dependences we found
stem from incorrect or incomplete initialization
of the program environment.
Programmers made these errors even though frameworks such as
JUnit provide ways to set up the environment for a test execution and clean
up the environment afterward.



Test dependence is usually caused by global variables shared by tests.
Assertions in the program code
do not directly check the global variables,
but rather check values that have been computed from
them.
Developers don't notice the dependencies until they are deep in debugging a
subtle bug or test failure.


\subsection{Repercussions of Dependent Tests}


Our study of \dtnum real-world, confirmed dependent
tests 
% from \repnum software issue tracking systems
(Section~\ref{sec:study}) identified two 
major consequences of dependent tests.
%
First, dependent tests usually indicate a
\emph{weaknesses in the test suite}. Exposing
dependent tests can detect test code flaws and identify situations
where some tests do not perform proper initialization.
Test dependences can lead
to {spurious bug reports}. As an example, the Eclipse developers
investigated a dependent test~\cite{eclipsebug} in SWT for
more than a month before realizing that the test dependences were intentional,
allowing them to close the bug report without a change to the system.
%
Second, dependent tests can
\emph{mask faults in a program}. Specifically, executing a test suite in the
default order does not expose the fault, whereas
executing the same test suite in a different order does. We found 
a bug~\cite{clibug} that had been masked by two dependent tests
for 3 years in the Apache CLI library~\cite{cli} (Section~\ref{sec:repercussion}).

%Second, guided by the findings of our study, we design two algorithms
%to detect manifest dependent tests. By applying our algorithms
%to \todo{xx} open-source programs and their test suites, we 
%found a large number of unknown dependent tests, .

Dependent tests also interfere with downstream testing
techniques that change a test's execution context.
Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite to run during
regression testing)~\cite{harroldetal:OOPSLA:2001},
test prioritization techniques (that reorder the
input to discover defects sooner)~\cite{Elbaum:2000:PTC:347324.348910},
test parallelization techniques (that schedule the input tests for execution across multiple
CPUs), test generation techniques~\cite{PachecoLET2007, SPLAT},
test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which
convert large system tests into smaller unit tests),
experimental debugging techniques (such as Delta Debugging~\cite{Zeller:2002},
which requires running a set of tests repeatedly), etc. 

Many such downstream testing techniques implicitly assume that
there are no test dependences in the input test suite.  Violation of
this assumption, as we show happens in practice, can cause \emph{delayed problems} in the face
of latent test dependence in the input.  As an
example, test selection may produce a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.


\subsection{Contributions}
\label{sec:contributions}

This paper addresses and questions
conventional wisdom about the test independence assumption. 
This paper makes the following contributions:

\begin{itemize}

  \item \textbf{Study.} We describe a study of \dtnum real-world
  dependent tests from \repnum software issue tracking
  systems to show that test dependence
  arises in practice. Our study identifies common
  characteristics of dependent tests, and
  it indicates that test dependence can have
  potentially costly repercussions and can be hard to identify unless
  explicitly searched for (Section~\ref{sec:study}).

\item \textbf{Formalization.} We formalize test dependence
  in terms of test suites as ordered sequences of tests and explicit execution
  environments for test suites.  The formalization enables reasoning about test dependence
  as well as a proof that finding \emph{manifest} dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).

  \item \textbf{Algorithms.} We present two algorithms
  to detect dependent tests (Section~\ref{sec:detecting}).
  The first exhaustive $k$-bounded algorithm
  executes all possible $k$-tests for
  a bounding parameter $k$ to identify tests exhibiting different results.
  The second dependence-aware $k$-bounded
  algorithm uses dynamic program analyses
  to reduce the search space. It records fields read and
  written by each test, and then safely ignores test execution orders
  that are guaranteed to produce the same results as in the default
  order. Both algorithms are \emph{sound} but \emph{incomplete}:
  every dependent test they identify is real, but the algorithms
  do not guarantee to find all dependent tests. 
  %\edit{check above when the algorithm section is written}

  \item \textbf{Evaluation.} We implemented our algorithms in a prototype
  tool, called \ourtool (Section~\ref{sec:impl}), and applied it to \subjnum real-world subject programs.
  \ourtool detected previously-unknown dependent tests in \textit{every}
  subject program we studied, from both human-written unit tests and automatically-generated
  unit tests (Section~\ref{sec:evaluation}).
  %been discovered before, showing that on average \todo{xx}\% of the human-written
  %unit tests are dependent and \todo{xx}\% of the automatically-generated
  %unit tests are dependent
  Finally, we discuss a set of open questions and other possible impacts of dependent
  tests in Section~\ref{sec:discussion}.
\end{itemize}


%  LocalWords:  Kapfhammer Soffa subsequence SWT CLI NUM

