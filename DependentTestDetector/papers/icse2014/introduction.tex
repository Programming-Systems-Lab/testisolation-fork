\section{Introduction}

Informally, \emph{dependent tests} produce different test results when
executed in different contexts. %, while \emph{independent tests} produce
%the same test results regardless of execution order.  
It is easy to
construct an example of dependence between two tests \code{A}
and \code{B}, where running \code{A} and then \code{B} leads
to both tests passing, while running \code{B} and then
\code{A} leads to either or both tests failing---the order
of applying the tests, in this case, changes the execution context.

%~\cite{KapfhammerS03}
%Chays:2000:FTD:347324.348954,
%Gray:1994:QGB:191843.191886}, 

Definitions in the testing literature are generally clear that the
conditions under which a test is executed may affect its result.  The
importance of context in testing has been explored in some depth in
some domains including databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,
kapfhammeretal:FSE:2003}, with results about test
generation, test adequacy criteria, etc., and mobile
applications~\cite{Wang:2007:AGC}.
For the database domain, Kapfhammer and Soffa formally
define and distinguish independent test suites from those that are
\emph{non-restricted\/} and thus ``can capture more of an application's
interaction with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations of test
adequacy''~\cite[p.~101]{kapfhammeretal:FSE:2003}.

At the same time, there is little focus on the core
issues of
test dependence itself.
Is this because test dependence does
not arise in practice (beyond domains such as databases)?  Is it because, even if-and-when it does arise, there
are few if any repercussions?  Is it because it is difficult to
notice if-and-when it arises?

\subsection{Manifest Test Dependence}

To explore these questions, we consider a narrow characterization
of test dependence that:
\begin{itemize}
\item Adopts the results of the default, usually implicit,
  order of execution of a test suite as the \emph{expected results}. 
\item Asserts \emph{test dependence\/} when there is a possibly
  reordered subsequence of the original test suite that, when
  executed, has at least one test result that differs from the
  expected result for that test.  
\end{itemize}
That is, we focus on a \emph{manifest\/} perspective of test dependence,
requiring a \emph{concrete\/} variant of the test suite that
\emph{dynamically\/} produces different results from the expected.  Our
definition differs from that of Kapfhammer and Soffa by considering
test results rather than program and database states.
As we discuss later, considering only manifest test dependences allows
us to more easily situate this research in the empirical domain.


\subsection{Examples and Repercussions}

We have identified a number of substantive examples of test suites
from fielded programs that manifest dependences.
%in fielded programs. 
%with test suites that manifest test dependence.  
We examined
six projects and found in their human-written test suites a total
of 75 dependent tests ($1.4 \%$). For the same set of
programs, we also generated test suites automatically using
Randoop~\cite{PachecoLET2007} and found that on average $14 \%$ of
the generated tests are dependent.


By analyzing these examples of test dependence, we identified three
categories of problems that can arise due to the presence of dependent tests.
First, test suites that unexpectedly contain dependent tests can
\emph{mask faults in a program}.  We present examples where
executing a test suite in the default order does not expose the fault, whereas
executing the same test suite in a different order does.
Second, test suites that unexpectedly contain dependent tests can \emph{conceal
weaknesses in the test suite} itself.  We present examples where exposing
dependent tests can identify situations where some tests do not perform
proper initialization.
Third, a test suite containing undocumented test dependences can lead
to \emph{spurious bug reports}.  We present an example where it took the developers
more than a month to realize that the test dependences were intentional,
allowing them to close the bug report without a change to the system.


\subsection{Test Execution Environment}

Our examples highlight varying execution environments as the
unsurprising central cause of test dependence. Specifically, when a
test is executed in different environments---global variables
with different values, differences in the file system, differences in
data obtained from web services, etc.---it has the potential to return
a different result.  
Most of the dependences we see in our
examples ultimately stem from incorrect or incomplete initialization
of the program environment.

Why does this happen? Especially given frameworks such as
JUnit that facilitate the process of clean setup by providing means to
automatically execute methods (\code{setUp()} and \code{tearDown()} in JUnit
3.x, and methods annotated with \code{@Before} and \code{@After} in
JUnit 4.x) that should handle all common setup and clean-up between
test cases. 

It appears that the answer is that developers are as likely
to make mistakes when writing tests as when they are writing other code.
And while frameworks make it easier to get environment setup right, 
they cannot ensure that it is done properly. 
Like with other code, this means that tests in some cases will have
unintended and unexpected behaviors.  And as programs increase in complexity,
so may tests, which may increase the frequency of such problems in tests,
which may in turn increase the frequency of test dependence.

Another situation in which the underlying test execution context can
unexpectedly change is when a tool or technique that takes a test
suite as input is used.  Examples of such techniques include
test selection techniques (that identify a subset of
the input test suite that will guarantee some properties during
regression testing)~\cite{harroldetal:OOPSLA:2001}, test prioritization techniques (that reorder the
input to increase the rate of fault detection)~\cite{Elbaum:2000:PTC:347324.348910}, test parallelization
techniques (that schedule the input for execution across multiple
CPUs), test factoring~\cite{Saff:2005} and test carving~\cite{Elbaum:2006} (which take system tests as
input and create unit tests and differential unit tests,
respectively), etc. 

Of these techniques, we are most concerned about
those that modify the organization of test suites, rather than the tests
they contain.
Many such downstream testing techniques implicitly assume that
there are no test dependences in the input test suite.  Our concern is
that this assumption can cause \emph{delayed problems} in the face
of latent test dependence in the input.  As an
example, test selection may report a subsequence of tests that do not
return the same results as they do when executed originally, as part of the full suite.

\subsection{Contributions}

At its heart, this paper addresses and questions
conventional wisdom about the test independence assumption. 
This paper makes the following contributions:

\begin{itemize}
\item \textbf{Formalization.} We provide a formalization of test dependence
  in terms of test suites as ordered sequences of tests and explicit execution
  environments for test suites, which enables reasoning about test dependence
  as well as a proof that finding \emph{manifest} dependences is an NP-complete
  problem (Section~\ref{sec:formalism}).
  \item \textbf{Study.} We describe a
  \item \textbf{Algorithms.} We present \todo{a family} of algorithms
  to detect dependent tests (Section~\ref{}). \todo{one more sentence}
  \item \textbf{Evaluation.} We implemented our algorithms, and
  evaluated them on \todo{NUM} real-world subject programs.
  \todo{results here} (Section~\ref{}). We finally discuss
  a set of open questions and other possible impacts of dependent
  tests in Section~\ref{}.
\end{itemize}


