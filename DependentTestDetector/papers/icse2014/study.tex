%\section{Dependent Tests in Practice}
\section{Real-World Dependent Tests}
\label{sec:study}

\newcommand{\unum}{{{14}}\xspace}
\newcommand{\svratio}{{{61}}}
\newcommand{\svnum}{{{59}}\xspace}
\newcommand{\unfixed}{{{58}}\xspace}


Dependent tests are known to occur in practice, but little is known about
their characteristics.  This section qualitatively studies
concrete examples of test dependence found in
well-known open source software. 


\subsection{Sources and Study Methodology}

We examined five
% well-known, publicly-accessible 
software issue
tracking systems: Apache~\cite{apachebug},
Eclipse~\cite{eclipsebug}, JBoss~\cite{jbossbug},
Hibernate~\cite{hibernatebug}, and Codehaus~\cite{codehausbug}.
Each issue tracking system serves tens of projects.
% , and
% holds thousands of bug reports, feature requests, improvement
% suggestions, etc.

For each issue tracking system, we searched for four phrases
(``dependent test'', ``test dependence'', ``test execution order'',
``different test outcome'') and manually examined the matched results. For each match, we read the
description of the issue report, the discussions between reporters
and developers, and the fixing patches (if available). This information
helped us understand whether the report is about test dependence.
%--- a test manifesting different results under different
%test execution orders. 
Each dependent test candidate was examined by
at least two people and the whole process consisted of several
rounds of (re-)study and cross checking. We ignored reports
that are described vaguely, and we excluded tests whose results are
affected by non-determinism (e.g., multi-threading).
All collected dependent tests are publicly available 
at: \url{http://homes.cs.washington.edu/~szhang/dependent\_tests.html}


\subsection{Findings}
\label{sec:studyfindings}

\input{studyresult}

Table~\ref{tab:studyresults} summarizes the dependent tests.


\subsubsection{Characteristics}


We summarize three characteristics of dependent tests:
manifestation, root cause, and developers' reaction.

\vspace{1mm}
\noindent \textbf{{Manifestation: at least \pertange of the dependent
tests in the study can be manifested by no more than 2 tests.}}
A dependent test is manifested if we produce a possibly reordered
subsequence of the original test suite, such that the test
produces a different
result than when run in the original suite.
We measure the size of the subsuite.
If the test produces a different result when run
in isolation, the number of tests to manifest
the dependent test is 1.
If the test produces a different result
when run after one other test (often, the subsuite is
running these two tests in the opposite order as the full original test
suite), then the number of tests to manifest the dependent test is 2.
Among the \dtnum studied dependent tests, we found only 2 of them
require 3 tests to manifest the dependence.
One test depends on itself:
running the test twice produces different results than running it once,
because this test side-effects a database it reads.
We count this special case separately in the ``Self'' column
of Table~\ref{tab:studyresults}.

For the remaining \unum dependent tests, the number of involved tests
is unknown, since the relevant information is missing
or vaguely described in the issue tracking systems. For example,
some reports simply stated that ``running \textit{all} tests in one class before
test \emph{t} makes \emph{t} fail'' or ``randomizing the test execution order
makes test \emph{t} fail''.

%Due to the extremely intricated
%environment needed for each open-source project, we were unable
%to reproduce all such dependent tests and minimize the involved tests.


% In theory,
% given a $n$-sized test suite, dependent test can occur in any
% length of permutations. However, among \dtnum collected tests,
% 86 (82\%) of them can be manifested by running no more than
% 2 tests. 

%\todo{Discuss the ``unknown'' column.  What happened?  Could we not
%  reproduce it at all?  Why not?}

\vspace{1mm}
\noindent \textbf{{Root cause: at least \svratio\% of the dependent tests
in the study arise because of improper access to shared static
variables.}} Among \dtnum dependent tests, \svnum (\svratio\%) of them
arise due to inappropriate access to
shared static variables; 4 (4\%) of them arise
due to inappropriate access to the file system, and 10 (10\%) of them arise
due to inappropriate access to a database.
The root cause for the remaining 23 (25\%) tests is not apparent
in the issue tracking system.

\vspace{1mm}
\noindent \textbf{{Developer actions: dependent tests
often indicate flaws in the test code, and developers usually
modify the test code to remove them.}}
% In some cases, dependent tests are intentional and developers
% document them, but in other cases they are
% inadvertent.
Among \dtnum dependent tests,
94 (98\%) of them were treated as major or minor problems in the report,
but only 38 (40\%) of all the \dtnum dependent tests
got fixed by developers. 
%There is no statistical
%significant correlation between severity and fixing.
A dependent tests usually
reveals a flaw in the test code rather than the program code:
16\% of fixes (6 out of 38) are
on the program code, while the other fixes are on the
test code.
Based on developers' discussions, we found that although
developers admitted that such test dependence should be removed,
they often leave the dependence unresolved.
The primary reason is that
% developers often \textit{intentionally} introduce test dependence because 
it is
easier and more convenient to write the test code. When such a dependent
test is reported, developers may just add comments to the test code
to document the existing dependency. This applies to 49 (51\%) out
of all the \dtnum dependent tests. For the remaining 9 (9\%) unfixed tests,
developers thought they were not important enough given the limited
development time, so they simply closed the issue report without taking
any action.

%Second, many popular testing
%frameworks such as JUnit does not support to explicitly specify
%test dependence in the test code\footnote{In fact, the execution order of
%JUnit tests depends on the underlying JVM implementation~\cite{junitordering}}.
%It is non-trivial 

When fixing a dependent test, developers used
four strategies. For all the 6 dependent tests
fixed on the program code, developers changed
code that performs static variable initialization, which ensures that
each dependent test will not read an undesired value.
Section~\ref{sec:repercussion} gives an example.
Among the other 32 dependent tests fixed on the test code,
28 (87\%) of them were fixed by manually specifying
the test execution order in a test script or a configuration file,
3 (10\%) of them were simply deleted by developers
from the test suite, and the remaining 1 (3\%) test was merged with its
initializing test.



%Test dependence can cause problems, not only
%when test suites are reordered, but even when they are
%executed in the intended order.



\subsubsection{Manifestation of Dependent Tests}
\label{sec:repercussion}

\input{categorytable}

A dependent test may manifest as a false alarm or a missed alarm
(Table~\ref{tab:reper}).

\vspace{1mm}

\noindent \textbf{False alarm.} Most of
the dependent tests (94 out of \dtnum) indicate a weakness in the test suite rather than the
tested code:  
the test should pass but fails after reordering due to the dependence.
The test dependence arise due to incorrect initialization
of program state by one
or more tests. Typically, one test initializes
a global variable or the execution environment, and another
test does not perform any initialization, but
relies on the program state after the first test's execution.
Such dependence in the test code is often masked because
the initializing test always executes before other tests in the
default execution order. The dependent tests are not revealed
until the initializing test is reordered to execute
after other tests. 
%In this category, the test dependency
%is introduced \textit{unintentionally} by developers. 
%the default test execution order includes tests that initialize the library.  The defect is
%inconsequential until and unless the flawed test is reordered, either manually or by
%a downstream tool, to execute before any other initializing test.

%\vspace{1mm}

Sometimes developers introduce dependent tests intentionally because it is
more efficient or convenient~\cite{kapfhammeretal:FSE:2003, whittakeretal:2012}.
%DB-testing}.
Even though the developers are aware of these dependences
when they create tests, this knowledge can get lost.
Other people who are not aware of these dependences can get confused 
when they run a subset of the test suite that manifests the
dependences, and might report bugs about the failing tests,
even though this is exactly the expected behavior. 
If the dependence is not documented clearly and
correctly, it can take a considerable amount of time to work out that
these reported failures are spurious. We found
49 such dependent tests in our study from the Eclipse issue tracking system.
%Or worse, the developers may try
%to fix a bug that is not there.
In September 2003, a user filed a
bug report in SWT~\cite{swt}~\cite{eclipsebug},
stating that 49 tests were failing unexpectedly
if she runs any other test before \code{TestDisplay} --- 
a test suite creates a new \code{Display} object and tests it.
However, this bug report was spurious and was
caused by undocumented test dependence.
All 49 failing tests are dependent tests with the same
root cause: in SWT, only one global \code{Display}
object is allowed; the tests that the user tried to run,
create, but do not dispose of a \code{Display} object, while
the tests in \code{TestDisplay} attempt to create
a new \code{Display} object, which fails, as one
is already created. This is the desired behavior,
and points to a potential weakness in the test suite.
% rather
%than the code.

\vspace{1mm}

\todo{Masking Faults in a Program}

\noindent \textbf{Missed alarm}. In rare cases,
dependent tests can hide a fault in the
program, \emph{exactly} when the test suite is executed in its default
order. Masking occurs when a test case $t$ \emph{should}
reveal a fault, but tests executed before $t$ in a test suite always
generate environments in which $t$ passes accidently and
does not reveal the fault. 
Tests in this category result in \textit{missed alarms} ---
a test should fail but passes due to the dependence.

\input{clicode}

We found two dependent tests in
the Apache CLI library~\cite{cli} for this category.
Figure~\ref{fig:option_builder} shows the simplified fault-related
code. The fault is due to the improper initialization of the static variable
\CodeIn{argName}. The static variable \CodeIn{argName} should be set
to its default value \CodeIn{"arg"} by CLI's clients via calling
method \CodeIn{reset()}. Otherwise, \CodeIn{argName}'s
default value remains \CodeIn{null} and should \emph{not} be
used in creating an \CodeIn{OptionBuilder} object.
In CLI, two test cases 
\code{Bugs\-Test.test13666} and \code{Bugs\-Test.test27635}
can reveal this potential fault by directly initializing
a \CodeIn{OptionBuilder} object without calling \CodeIn{reset()}.
These two tests fail when run in isolation,
but both pass when run in the default order. This is because
in the default order, tests running \emph{before} these
two tests call \CodeIn{reset()} at least once, which sets
initialize the value of \CodeIn{argName} and masks the fault.

%Both dependent tests can reveal this fault,  but
%the default order of test execution makes both tests pass
%accidentally. 

Such dependent tests have a non-trivial impact in practice.
This fault was reported in the bug database several times~\cite{clibug},
starting on March 13, 2004 (CLI-26). The report was marked as resolved
\emph{three years} later on March 15, 2007 when developers
realized the test dependence. Developers fixed this
fault by adding a static initialization block which
calls \CodeIn{reset()} in class \CodeIn{OptionBuilder}.

%\edit{where should we emphasize that masking faults is an
%orthognonal issue of fixing the dependence on code or test?}


\subsubsection{Implications for Dependent Test Detection}

We summarize the main implications of our findings.

\noindent \textbf{{Dependent tests exist in practice, but
they are not obvious to identify.}}
None of the dependent tests we studied can be identified by
running the existing test suite in the default order. 
Every dependent test was reported when the
test suite is reordered, either accidentally by a user or
by a testing tool. This indicates that
a tool for detecting dependent tests is needed.
%dependent test detection techniques should
%explicitly search for such dependent tests.

\vspace{1mm}
\noindent \textbf{Dependent test detection techniques
can bound the search space to a small number of tests.}
In theory, a technique needs to exhaustively execute
all $n!$ permutations of a $n$-sized
test suite to detect all dependent tests. This is
not feasible for realistic $n$.  Our study shows that
most dependent tests can be manifested by executing
no more than 2 tests. Thus, a practical technique
can focuses on running only short subsequences (whose
length is bounded by a parameter $k$)
of a test suite. This will reduce the number of permutations
to $O(n^k)$, which for small $k$ is tractable.

\vspace{1mm}
\noindent \textbf{Dependent test detection techniques
should focus on analyzing accesses to global variables.}
Dependent tests can result from many
interactions with the execution environment, including
global variables, file systems, databases, network, etc.
However, as reflected by our study, more than half of the
real-world dependent tests are caused
by improper static variable accesses. This implies that a technique
may achieve a high return by focusing on global variables.


%\vspace{1mm}
%\noindent \textbf{Dependent test fixing tool
%Test dependence reveals flaws in the test code.}
%This indicates that a potential dependent test fixing tool should target
%the test code


\subsection{Threats to validity}

Our findings apply in the context of our study and methodology and may not
apply to arbitrary programs.
The applications we studied are all written in 
Java and have JUnit test suites.  

We accepted the developers' judgment regarding which tests are dependent
and the severity of each test.  We did not intentionally ignore
any test dependence in the issue tracking system.
However, a limitation is that the developers might have made a mistake,
might not have marked a test dependence in a way we found it, and are
unlikely to have found all the dependent tests in those projects. 


%  LocalWords:  JBoss Codehaus reproducibility multi dependences SWT CLI
%  LocalWords:  TestDisplay test13666 subsequences subsuite
