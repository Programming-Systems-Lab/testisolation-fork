%\section{Dependent Tests in Practice}
\section{Real-World Dependent Tests}
\label{sec:study}

\newcommand{\unum}{{{14}}\xspace}
\newcommand{\svratio}{{{61}}}
\newcommand{\svnum}{{{59}}\xspace}
\newcommand{\unfixed}{{{58}}\xspace}


% are known to occur in practice, but
Little is known about the characteristics of dependent tests.
This section qualitatively studies
concrete examples of test dependence found in
well-known open source software. 


\subsection{Sources and Study Methodology}

We examined five
% well-known, publicly-accessible 
software issue
tracking systems: Apache \cite{apachebug},
Eclipse~\cite{eclipsebug}, JBoss~\cite{jbossbug},
Hibernate~\cite{hibernatebug}, and Codehaus~\cite{codehausbug}.
Each issue tracking system serves tens of projects.
% , and
% holds thousands of bug reports, feature requests, improvement
% suggestions, etc.

\input{studyresult}

For each issue tracking system, we searched for four phrases
(``dependent test'', ``test dependence'', ``test execution order'',
``different test outcome'') and manually examined the matched results. For each match, we read the
description of the issue report, the discussions between reporters
and developers, and the fixing patches (if available). This information
helped us understand whether the report is about test dependence.
%--- a test manifesting different results under different
%test execution orders. 
Each dependent test candidate was examined by
at least two people and the whole process consisted of several
rounds of (re-)study and cross checking. We ignored reports
that are described vaguely, and we excluded tests whose results are
affected by non-determinism (e.g., multi-threading).
In total, we examined the first 450 matched reports, of which 53
reports are about test dependence (some reports contain multiple
dependent tests).
All collected dependent tests are publicly available 
at: \url{http://homes.cs.washington.edu/~szhang/dependent\_tests.html}


\subsection{Findings}
\label{sec:studyfindings}

Table~\ref{tab:studyresults} summarizes the dependent tests.


\subsubsection{Characteristics}
\label{sec:characteristics}

We summarize three characteristics of dependent tests:
manifestation, root cause, and developer actions.

%\todo{It's confusing that there is a paragraph about ``manifestation'' here,
%  but also Section~\ref{sec:repercussion} is titled ``manifestation''; the
%  reader will not be able to easily understand the distinction between
%  them.  This suggests that you need to think about what information is
%  being conveyed, what the conceptual organization should be, whether that
%  is the current one, and what words should be used to name the concepts.}

\tinyrelax
\noindent \textbf{{Manifestation: at least \dtrate of the dependent
tests in the study can be manifested by 2 or fewer tests.}}
A dependent test is manifested if there exists a possibly reordered
subsequence of the original test suite, such that the test
produces a different
result than when run in the original suite.
We measure the size of the reported subsequence 
in the issue report.
%\todo{Why is the size of the reported sequence
%  important?  The person making the report might not have minimized the
%  sequence, so this might not be an accurate measure of how many tests are
%  required for minimization.}
If the test produces a different result when run
in isolation, the number of tests to manifest
the dependent test is 1.
If the test produces a different result
when run after one other test (often, the subsequence is
running these two tests in the opposite order as the full original test
suite), then the number of tests to manifest the dependent test is 2.
Among the \dtnum studied dependent tests, we found only 2 of them
require 3 tests to manifest the dependence.
One other test depends on itself:
running the test twice produces different results than running it once,
because this test side-effects a database it reads.
We count this special case separately in the ``Self'' column
of Table~\ref{tab:studyresults}.

For the remaining \unum dependent tests, the number of involved tests
is unknown, since the relevant information is missing
or vaguely described in the issue tracking systems. For example,
some reports simply stated that ``running \textit{all} tests in one class before
test \emph{t} makes \emph{t} fail'' or ``randomizing the test execution order
makes test \emph{t} fail''.

%Due to the extremely intricated
%environment needed for each open-source project, we were unable
%to reproduce all such dependent tests and minimize the involved tests.


% In theory,
% given a $n$-sized test suite, dependent test can occur in any
% length of permutations. However, among \dtnum collected tests,
% 86 (82\%) of them can be manifested by running no more than
% 2 tests. 

%\todo{Discuss the ``unknown'' column.  What happened?  Could we not
%  reproduce it at all?  Why not?}

%\todo{In the following text, replace ``improper'' and ``inappropriate'' by
%  more specific and concrete terms that do not make value judgments.  One
%  suggestion is to replace ``improper'' by ``side-effecting'' and remove
%  all occurrences of ``inappropriate''.}

\tinyrelax
\noindent \textbf{{Root cause: at least \svratio\% of the dependent tests
in the study arise because of side-effecting access to shared static
variables.}} Among \dtnum dependent tests:
\svnum (\svratio\%) of them
arise due to access to
shared static variables;
10 (10\%) of them arise
due to access to a database; and
4 (4\%) of them arise
due to access to the file system.
The root cause for the remaining 23 (25\%) tests is not apparent
in the issue tracking system.

\tinyrelax
\noindent \textbf{{Developer actions: dependent tests
often indicate flaws in the test code, and developers usually
modify the test code to remove them.}}
% In some cases, dependent tests are intentional and developers
% document them, but in other cases they are
% inadvertent.
Among \dtnum dependent tests, developers considered 
94 (98\%) to be major or minor problems, and the 
developers' discussions showed that the developers thought that the test
dependence should be removed.
Nonetheless, developers fixed only 38 (40\%) of the \dtnum dependent tests.
Another 49 (51\%) were ``fixed'' 
by adding comments to the test code to document the existing dependence.
For the remaining 9 (9\%) unfixed tests,
developers thought they were not important enough given the limited
development time, so they simply closed the issue report without taking
any action.

%% Do we have evidence of this?  I'm inclined to omit the discussion.  -MDE
% The primary reason is that
% % developers often \textit{intentionally} introduce test dependence because 
% it is
% easier and more convenient to write the test code. 

%There is no statistical
%significant correlation between severity and fixing.
A dependent test usually
reveals a flaw in the test code rather than the program code:
only 16\% of the code fixes (6 out of 38) are
on the program code.
In all 6 cases, the developers changed
code that performs static variable initialization, which ensures that
the tests will not read an uninitialized value.
Section~\ref{sec:repercussion} gives an example.
The other 32 code fixes were in the test code:
28 (87\%) of the dependent tests were fixed by manually specifying
the test execution order in a test script or a configuration file,
3 (10\%) of them were simply deleted by developers
from the test suite, and the remaining 1 (3\%) test was merged with its
initializing test.


%Second, many popular testing
%frameworks such as JUnit does not support to explicitly specify
%test dependence in the test code\footnote{In fact, the execution order of
%JUnit tests depends on the underlying JVM implementation~\cite{junitordering}}.
%It is non-trivial 



%Test dependence can cause problems, not only
%when test suites are reordered, but even when they are
%executed in the intended order.



\subsubsection{Repercussions of Dependent Tests}
\label{sec:repercussion}

\input{categorytable}

A dependent test may manifest as a false alarm or a missed alarm
(Table~\ref{tab:reper}).

\tinyrelax
\noindent \textbf{False alarm.} Most of
the dependent tests (94 out of \dtnum) 
result in false alarms:
%indicate a weakness in the test suite rather than the
%tested code:  
the test should pass but fails after reordering due to the dependence.
The test dependence arises due to incorrect initialization
of program state by one
or more tests. Typically, one test initializes
a global variable or the execution environment, and another
test does not perform any initialization, but
relies on the program state after the first test's execution.
Such dependence in the test code is often masked because
the initializing test always executes before other tests in the
default execution order. The dependent tests are not revealed
until the initializing test is reordered to execute
after other tests. 
%In this category, the test dependency
%is introduced \textit{unintentionally} by developers. 
%the default test execution order includes tests that initialize the library.  The defect is
%inconsequential until and unless the flawed test is reordered, either manually or by
%a downstream tool, to execute before any other initializing test.


Sometimes developers introduce dependent tests intentionally because it is
more efficient or convenient~\cite{kapfhammeretal:FSE:2003, whittakeretal:2012}.
%DB-testing}.
Even though the developers are aware of these dependences
when they create tests, this knowledge can get lost.
Other people who are not aware of these dependences can get confused 
when they run a subset of the test suite that manifests the
dependent tests, and might report bugs about the failing tests,
even though this is exactly the intended behavior. 
If the dependence is not documented clearly and
correctly, it can take a considerable amount of time to work out that
these reported failures are spurious. 
The Eclipse project contains at least
49 such dependent tests.
%\todo{An issue tracking system does not contain
%  tests.  It may contain reports of test dependence, or reports that are
%  manifestations of or caused by test dependence.}
%Or worse, the developers may try
%to fix a bug that is not there.
In September 2003, a user filed a
bug report in SWT~\cite{swt}~\cite{eclipsebug},
stating that 49 tests were failing unexpectedly
if she ran any other test before \code{TestDisplay} --- 
a test suite that creates a new \code{Display} object and tests it.
However, this bug report was spurious and was
caused by undocumented test dependence.
All 49 failing tests are dependent tests with the same
root cause: in SWT, only one global \code{Display}
object is allowed; the user ran tests that
create but do not dispose of a \code{Display} object, while
the tests in \code{TestDisplay} attempt to create
a new \code{Display} object, which fails, as one
is already created. This is the desired behavior of SWT,
and points to a weakness in the test suite.
% rather
%than the code.

\tinyrelax
\noindent \textbf{Missed alarm}. In rare cases,
dependent tests can hide a fault in the
program, \emph{exactly} when the test suite is executed in its default
order. Masking occurs when a test case $t$ \emph{should}
reveal a fault, but tests executed before $t$ in a test suite always
generate environments in which $t$ passes accidentally and
does not reveal the fault. 
Tests in this category result in \textit{missed alarms} ---
a test should fail but passes due to the dependence.

\input{clicode}

We found two such dependent tests in
the Apache CLI library~\cite{cli, DBLP:conf/sigsoft/MusluSW11}.
Figure~\ref{fig:option_builder} shows the simplified fault-related
code. The fault is due to side-effecting initialization of the static variable
\CodeIn{argName}. The static variable \CodeIn{argName} should be set
to value \CodeIn{"arg"}
%\todo{I'm confused by this terminology.
%  According to Java language semantics, the default value of the static
%  variable is null.  Should the field initializer
% have been changed or removed?}
by CLI's clients via calling method \CodeIn{reset()}. Otherwise, \CodeIn{argName}'s
value remains \CodeIn{null} and should \emph{not} be
used in creating an \CodeIn{OptionBuilder} object.
In CLI, two test cases 
\code{Bugs\-Test.test13666} and \code{Bugs\-Test.test27635}
can reveal this fault by directly instantiating
%\todo{I don't understand
%  ``directly initializing''.  Should that be replaced by ``instantiating''?}
a \CodeIn{OptionBuilder} object without calling \CodeIn{reset()}.
These two tests fail when run in isolation,
but both pass when run in the default order. This is because
in the default order, tests running \emph{before} these
two tests call \CodeIn{reset()} at least once, which sets
the value of \CodeIn{argName} and masks the fault.

%Both dependent tests can reveal this fault,  but
%the default order of test execution makes both tests pass
%accidentally. 

%Such dependent tests have a non-trivial impact in practice.
This fault was reported in the bug database several times \cite{clibug},
starting on March 13, 2004 (CLI-26). The report was marked as resolved
\emph{three years} later on March 15, 2007 when developers
realized the test dependence. The developers fixed this
fault by adding a static initialization block which
calls \CodeIn{reset()} in class \CodeIn{OptionBuilder}.

%\edit{where should we emphasize that masking faults is an
%orthognonal issue of fixing the dependence on code or test?}


\subsubsection{Implications for Dependent Test Detection}

We summarize the main implications of our findings.

\noindent \textbf{{Dependent tests exist in practice, but
they are not easy to identify.}}
None of the dependent tests we studied can be identified by
running the existing test suite in the default order. 
Every dependent test was reported when the
test suite was reordered, either accidentally by a user or
by a testing tool. This indicates the need
for a tool to detect dependent tests.
%dependent test detection techniques should
%explicitly search for such dependent tests.

\tinyrelax
\noindent \textbf{Dependent test detection techniques
can bound the search space to a small number of tests.}
In theory, a technique needs to exhaustively execute
all $n!$ permutations of a $n$-sized
test suite to detect all dependent tests. This is
not feasible for realistic $n$.  Our study shows that
most dependent tests can be manifested by executing
no more than 2 tests together (Section~\ref{sec:characteristics}).
Thus, a practical technique
can focus on running only short subsequences (whose
length is bounded by a parameter $k$)
of a test suite. This reduces the permutation number 
to $O(n^k)$, which is tractable for small $k$ and $n$.

\tinyrelax
\noindent \textbf{Dependent test detection techniques
should focus on analyzing accesses to global variables.}
Dependent tests can result from many
interactions with the execution environment, including
global variables, databases, the file system, network, etc.
However, as reflected by our study, more than half of the
real-world dependent tests are caused
by side-effecting static variable accesses. This implies that a dependent
test detection technique may find more dependent tests
by focusing on global variables.
%achieve a high return\todo{I'm not sure what ``achieve a high return'' means} by focusing on global variables.


%\noindent \textbf{Dependent test fixing tool
%Test dependence reveals flaws in the test code.}
%This indicates that a potential dependent test fixing tool should target
%the test code


\subsection{Threats to validity}

Our findings apply in the context of our study and methodology and may not
apply to arbitrary programs.
The applications we studied are all written in 
Java and have JUnit test suites.  

We accepted the developers' judgment regarding which tests are dependent,
the severity of each dependent test, and how many tests are needed
to manifest the dependence.  We did not intentionally ignore
any test dependence in the issue tracking system.
However, a limitation is that the developers might have made a mistake or
might not have marked a test dependence in a way we found it
(different search terms might discover additional dependent tests).  We are
unlikely to have found all the dependent tests in those projects. 


%  LocalWords:  JBoss Codehaus reproducibility multi dependences SWT CLI
%  LocalWords:  TestDisplay test13666 subsequences subsuite whittakeretal
%%  LocalWords:  argName arg CLI's OptionBuilder test27635
