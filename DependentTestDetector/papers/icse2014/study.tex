%\section{Dependent Tests in Practice}
\section{Real-World Dependent Tests}
\label{sec:study}

To investigate whether dependent tests can reach beyond theory
and appear in real-world programs, this section presents an empirical
study of concrete examples of test dependence found in
well-known open source programs. 


\subsection{Sources and Study Methodology}

We chose five well-known, public-accessible software issue
tracking systems to examine: Apache~\cite{apachebug},
Eclipse~\cite{eclipsebug}, JBoss~\cite{jbossbug},
Hibernate~\cite{hibernatebug}, and Codehaus~\cite{codehausbug}.
Each issue tracking system serves tens of projects, and
holds thousands of bug reports, feature requests, improvement
suggestions, etc.

For each issue tracking system, we searched for a set of keywords
(``dependent test'', ``test dependence'', ``test execution order'',
etc.\todo{for concreteness and reproducibility, list every search term}), and manually examined the matched results. For each match, we read the
description of the issue report, the discussions between reporters
and developers, and the fixing patches (if available). This information
helped us understand whether the report is about test dependence
--- a test manifesting different behaviors under different
test execution orders. Each dependent test candidate was examined by
at least two people and the whole process consisted of several
rounds of (re-)study, cross checking, and efforts to reproduce. We ignored reports
that are described vaguely, and excluded tests whose results are
affected by non-determinism (e.g., multi-threading).
In total, we have spent more than 6 person-months to collect and analyze
the dependent tests. 


\subsection{Findings}

\input{studyresult}

Table~\ref{tab:studyresults} summarizes the dependent tests.


\subsubsection{Characteristics}

\todo{We may be able to reduce the length of this section.}

We summarize three characteristics of dependent tests:
manifestation, root cause, and developers' reaction.

\vspace{1mm}
\noindent \textbf{{Manifestation: at least 82\% of the dependent
tests in the study can be manifested by no more than 2 tests.}}
A dependent test is manifested if we produce a test suite that is a 
subset of the original suite, such that the test fails.
We measure the size of the subsuite.
If the test fails when run in isolation, the number of tests to manifest
the dependent test is 1.
If the test fails when run after one other test (often, the subsuite is
running these two tests in the opposite order as the full original test
suite), then the number of tests to manifest the dependent test is 2.

% In theory,
% given a $n$-sized test suite, dependent test can occur in any
% length of permutations. However, among \dtnum collected tests,
% 86 (82\%) of them can be manifested by running no more than
% 2 tests. 

\todo{Discuss the ``unknown'' column.  What happened?  Could we not
  reproduce it at all?  Why not?}

\vspace{1mm}
\noindent \textbf{{Root cause: at least 52\% of the dependent tests
in the study arise because of the improper access to shared static
variables.}} Among \dtnum dependent tests, 58 (52\%) of them
arise due to inappropriate access to
shared static variables; and 13 (12\%) of them arise
due to inappropriate access to file systems or other
execution environment. 

\vspace{1mm}
\noindent \textbf{{Developers' reactions: dependent tests
often indicate flaws in the test code, and developers usually
ignore dependent tests due to the lack of tool support.}}
In some cases, they are intentional, developers are aware
of them and document them, but in other cases they are
inadvertent. Among \dtnum collected dependent tests,
97 (93\%) of them were treated as major or minor problems,
but only 34 (32\%) of them got fixed by developers. Among
the 34 fixed dependent tests, only 7 fixes are
on the program code, while the other fixes are on the
tested code. This indicates that dependent tests usually
reveal potential flaws in the test code rather than the test code.
Based on developers' discussion, we found that although
developers admitted that such test dependence should be removed,
they often leave the dependence unresolved, sometimes by merging
two tests or adding explanatory documentation.
\todo{It seems to me that merging two tests does resolve the test
  dependence, in the sense that there is no longer any order in which the
  new tests can be run such that they fail.  Why do you call this leaving
  the dependence unresolved?  Also, there are so many items in the
  ``unfixed'' column that I think you should subdivide it further, for
  example into ``merged tests'', ``documentation'', and ``unfixed'' (for
  those the developers really did nothing).  Are there other possibilities?}
The primary
reason is that the current JUnit testing framework does not
support to explicitly specify test dependence in the test code.
\todo{Does it support specifing an order in which to run all tests?}


%Test dependence can cause problems, not only
%when test suites are reordered, but even when they are
%executed in the intended order.



\subsubsection{Repercussions of Dependent Tests}
\label{sec:repercussion}

\input{categorytable}

We found three major consequences of dependent tests (Table~\ref{tab:reper}).
%  We describe each category below and give concrete examples. 

\vspace{1mm}

\noindent \textbf{Poor Test Construction.} Most identified
dependent tests fall into this category. The test dependences
arise due to incorrect initialization of program state by one
or more tests, and reveal flaws in the test suite itself
rather than the tested code. Typically, one test initializes
a global variable or the execution environment; and another
test does not perform any initialization, but
relies on the program state after the first test's execution.
%global variable that is a part of the environment, but the test does
%not properly initialize it.  In the second case, a test should but
%does not call
%an initialization function before later invocations to a complex library.
Such dependence in the test code is often masked because
the initializing test always executes before other tests in the
default execution order. The dependent tests are revealed
until the initializing test is reordered to execute
\textit{after} other tests. 

%the default test execution order includes tests that initialize the library.  The defect is
%inconsequential until and unless the flawed test is reordered, either manually or by
%a downstream tool, to execute before any other initializing test.

\vspace{1mm}

\noindent \textbf{Spurious Bug Reports}
Sometimes developers introduce dependent tests intentionally because it is
easier, more efficient, or more convenient~\cite{kapfhammeretal:FSE:2003, whittakeretal:2012}.
%DB-testing}.
Even though the developers are aware of these dependences
when they create tests, this knowledge can get lost, 
and other people who are not aware of these dependences can get confused 
when they run a subset of the test suite that manifests the
dependences.

As a result, software users or maintainers
might report bugs about the failing tests, even though this
is exactly the expected behavior. 
If the dependence is not documented clearly and
correctly, it can take a considerable amount of time to work out that
these reported failures are spurious.
%Or worse, the developers may try
%to fix a bug that is not there.
For example,
in September 2003, a user filed a
bug report in SWT~\cite{swt}\footnote{\url{https://bugs.eclipse.org/bugs/show_bug.cgi?id=43500}},
stating that tests were failing unexpectedly
if she runs any other test before \texttt{TestDisplay} --- 
a test suite creates a new \code{Display} object and tests it.
However, this bug report was spurious and was
caused by undocumented test dependence.
Its root cause is quite simple: in SWT, only one global \texttt{Display}
object is allowed; the tests that reporters try to run
create, but do not dispose of a \code{Display} object, while
the tests in \code{TestDisplay} attempt to create
a new \code{Display} object, which fails, as one
is already created. This is the desired behavior,
and points to a potential problem in the test suite rather
than the code.

\todo{What is the relationship between the categories?  It seems that the
  above example belongs in ``poor test construction'' as well as ``spurious
  bug reports''.  In fact, doesn't \emph{every} dependent test belong in
  ``poor test construction''?  Why or why not?}


\vspace{1mm}

\noindent \textbf{Masking Faults}. In rare cases,
dependent tests can hide a fault in the
program, \emph{exactly} when the test suite is executed in its default
order. Masking occurs when a test case $t$ \emph{should}
reveal a fault, but tests executed before $t$ in a test suite always
generate environments in which $t$ does not reveal the fault.

\input{clicode}

We only found two dependent tests in
the Apache CLI library~\cite{cli} for this category.
In CLI, two test cases 
\code{Bugs\-Test.test13666} and \code{Help\-For\-mat\-ter\-Test.test\-Op\-tion\-With\-out\-Short\-For\-mat2}
fail when run in isolation,
but both pass when run in the default order.

Figure~\ref{fig:option_builder} shows the simplified code and
tests. Both dependent tests can reveal this fault,  but
the default order of test execution makes both tests pass
accidentally. Such dependent tests
have a non-trivial impact in practice.
This fault is reported in the bug
database several times,\footnote{\url{https://issues.apache.org/jira/browse/CLI-26} \url{https://
issues.apache.org/jira/browse/CLI-186} \url{https://issues.apache.org/jira/browse/
CLI-187}} starting on March 13, 2004 (CLI-26). The report was marked as resolved
\emph{three years} later on March 15, 2007 when developers
realized the test dependence.

%, but is then reopened as CLI-186 on
%July 31, 2009. About one month later, the bug is duplicated as
%CLI-187, and the actual fix happens one 
%year later on June 19, 2010, about six years after the bug was first reported (and four years
%total on the open-issue list).


%On this report, one of the developers commented:
%\begin{quote}
%I reproduced the issue, it requires a dedicated test case since it is tied to the initialization 
%of a static field in OptionBuilder.
%\end{quote}

%Despite the realization that a dedicated test is required, no such
%test was ever created.

%\paragraph{Eclipse SWT: Causing Spurious Bug Reports}


\subsubsection{Implications}

We summarize the main implications of our findings.

\noindent \textbf{{Dependent tests exist in practice, but
they can be hard to identify unless explicitly searched for.}}
None of the dependent tests we studied is identified by
running the existing test suite in the default order. Often,
the test dependence is not reported until and unless the
test suite is reordered, either manually by a user or
a maintainer or by a downstream tool. This indicates that
specialized tools to detect such dependence are needed.

\vspace{1mm}
\noindent \textbf{Dependent test detection techniques
can bound the search space to a small number of tests.}
In theory, it is necessary
to exhaustively execute all $n!$ permutations of a $n$-sized
test suite to detect all dependent tests. This is
not feasible for realistic $n$.  Our study demonstrates that
most dependent tests can be manifested by executing
no more than 2 tests.  Exhaustively executing all 
short subsequences of a test suite is tractable.

\vspace{1mm}
\noindent \textbf{Dependent test detection techniques
should focus on analyzing static variable accesses.}
More than half of the studied dependent tests are caused
by static variable access. Thus, focusing on analyzing
static variables can be a cost-effective way for a
tool design. \todo{need to re-write the above}

%dependent tests.
%can bound Tools


%\vspace{1mm}
%\noindent \textbf{Dependent test fixing tool
%Test dependence reveals flaws in the test code.}
%This indicates that a potential dependent test fixing tool should target
%the test code


\subsection{Threats to validity}

Our findings apply in the context of our study and methodology and may not
apply to arbitrary programs.
The applications we studied are widely used and have comprehensive test suites.
However, a limitation is that they are all written in 
Java and have JUnit test suites.  

We accepted the developers' judgment regarding which tests are dependent
and the severity of each test.  We did not intentionally ignore
any test dependence in the issue tracking system.
However, a limitation is that the developers might have made a mistake,
might not have marked a test dependence in a way we found it, and may not
have found all the dependent tests in those projects. 

%We believe the dependent tests in our study
%provide a representative sample in these software applications.

% In our study, we do not emphasize any quantitative characteristic
% results, and most of our findings are consistent across
% the examined dependent tests.

%  LocalWords:  JBoss Codehaus reproducibility multi dependences SWT CLI
%  LocalWords:  TestDisplay test13666 subsequences
