%\section{Dependent Tests in Practice}
\section{Real-World Dependent Tests}
\label{sec:study}

To investigate whether dependent tests can reach beyond theory
and appear in real-world programs, this section presents an empirical
study of concrete examples of test dependence found in
well-known open source programs. 


\subsection{Sources and Study Methodology}

We chose five well-known, public-accessible software issue
tracking systems to examine: Apache~\cite{apachebug},
Eclipse~\cite{eclipsebug}, JBoss~\cite{jbossbug},
Hibernate~\cite{hibernatebug}, and Codehaus~\cite{codehausbug}.
Each issue tracking system serves tens of projects, and
holds thousands of bug reports, feature requests, improvement
suggestions, etc.

For each issue tracking system, we searched for a set of keywords
(``dependent test'', ``test dependence'', ``test execution order'',
etc.), and manually examined the matched results. For each match, we read the
description of the issue report, the discussions between reporters
and developers, and the fixing patches (if available). This information
helped us understand whether the report is about test dependence
--- a test manifesting different results under different
test execution orders. Each dependent test candidate was examined by
at least two people and the whole process consisted of several
rounds of (re-)study, cross checking, and efforts to reproduce. We ignored reports
that are described vaguely, and excluded tests whose results are
affected by non-determinism (e.g., multi-threading).
%In total, we have spent more than 6 person-months to collect and analyze
%the dependent tests. 
All collected dependent tests are publicly-accessible
at: \url{http://homes.cs.washington.edu/~szhang/dependent\_tests.html}


\subsection{Findings}

\input{studyresult}

Table~\ref{tab:studyresults} summarizes the dependent tests.


\subsubsection{Characteristics}

We summarize three characteristics of dependent tests:
manifestation, root cause, and developers' reaction.

\vspace{1mm}
\noindent \textbf{{Manifestation: at least 82\% of the dependent
tests in the study can be manifested by no more than 2 tests.}}
A dependent test is manifested if we produce a test suite that is a 
subset of the original suite, such that the test fails.
We measure the size of the subsuite.
If the test fails when run in isolation, the number of tests to manifest
the dependent test is 1.
If the test fails when run after one other test (often, the subsuite is
running these two tests in the opposite order as the full original test
suite), then the number of tests to manifest the dependent test is 2.

% In theory,
% given a $n$-sized test suite, dependent test can occur in any
% length of permutations. However, among \dtnum collected tests,
% 86 (82\%) of them can be manifested by running no more than
% 2 tests. 

\todo{Discuss the ``unknown'' column.  What happened?  Could we not
  reproduce it at all?  Why not?}

\vspace{1mm}
\noindent \textbf{{Root cause: at least 52\% of the dependent tests
in the study arise because of the improper access to shared static
variables.}} Among \dtnum dependent tests, 58 (52\%) of them
arise due to inappropriate access to
shared static variables; and 13 (12\%) of them arise
due to inappropriate access to file systems or other
execution environment. 

\vspace{1mm}
\noindent \textbf{{Developers' reactions: dependent tests
often indicate flaws in the test code, and developers usually
ignore dependent tests due to the lack of tool support.}}
In some cases, they are intentional, developers are aware
of them and document them, but in other cases they are
inadvertent. Among \dtnum collected dependent tests,
97 (93\%) of them were treated as major or minor problems,
but only 34 (32\%) of them got fixed by developers. Among
the 34 fixed dependent tests, only 7 fixes are
on the program code, while the other fixes are on the
tested code. This indicates that dependent tests usually
reveal potential flaws in the test code rather than the test code.
Based on developers' discussion, we found that although
developers admitted that such test dependence should be removed,
they often leave the dependence unresolved. The primary
The primary reason is that the current widely-used testing
framework such as JUnit does not support to explicitly specify
test dependence in the test code; and even the default test
execution order in JUnit depends on the
underlying JVM implementation~\cite{junitordering}.
\todo{re-write below}
Developers fixed dependent by merging two tests,
or manually adding execution orders in test
scripts (or configuration files).
For \todo{xx} tests that developers did not fix,
developers added additional documentation to
explain the test dependence.
For the remaining \todo{xx} unfixed tests,
developers simply ignored them without taking any
actions.



%Test dependence can cause problems, not only
%when test suites are reordered, but even when they are
%executed in the intended order.



\subsubsection{Repercussions of Dependent Tests}
\label{sec:repercussion}

\input{categorytable}

We classify the repercussions of dependent tests
into two major categories (Table~\ref{tab:reper}).

\vspace{1mm}

\noindent \textbf{Weakness in a Test Suite.} Most (\todo{xx}\%) of
the identified dependent tests fall into this category.
The test dependences arise due to incorrect initialization
of program state by one
or more tests, and reveal flaws in the test suite itself
rather than the tested code. Typically, one test initializes
a global variable or the execution environment; and another
test does not perform any initialization, but
relies on the program state after the first test's execution.
%global variable that is a part of the environment, but the test does
%not properly initialize it.  In the second case, a test should but
%does not call
%an initialization function before later invocations to a complex library.
Such dependence in the test code is often masked because
the initializing test always executes before other tests in the
default execution order. The dependent tests are revealed
until the initializing test is reordered to execute
\textit{after} other tests. 
%In this category, the test dependency
%is introduced \textit{unintentionally} by developers. 
%the default test execution order includes tests that initialize the library.  The defect is
%inconsequential until and unless the flawed test is reordered, either manually or by
%a downstream tool, to execute before any other initializing test.

%\vspace{1mm}

%\noindent \textbf{Spurious Bug Reports}
Sometimes developers introduce dependent tests intentionally because it is
easier, more efficient, or more convenient~\cite{kapfhammeretal:FSE:2003, whittakeretal:2012}.
%DB-testing}.
Even though the developers are aware of these dependences
when they create tests, this knowledge can get lost.
Other people who are not aware of these dependences can get confused 
when they run a subset of the test suite that manifests the
dependences, and might report bugs about the failing tests,
even though this is exactly the expected behavior. 
If the dependence is not documented clearly and
correctly, it can take a considerable amount of time to work out that
these reported failures are spurious.
%Or worse, the developers may try
%to fix a bug that is not there.
For example,
in September 2003, a user filed a
bug report in SWT~\cite{swt}~\cite{eclipsebug},
stating that \todo{xx} tests were failing unexpectedly
if she runs any other test before \texttt{TestDisplay} --- 
a test suite creates a new \code{Display} object and tests it.
However, this bug report was spurious and was
caused by undocumented test dependence.
Its root cause is quite simple: in SWT, only one global \texttt{Display}
object is allowed; the tests that reporters try to run
create, but do not dispose of a \code{Display} object, while
the tests in \code{TestDisplay} attempt to create
a new \code{Display} object, which fails, as one
is already created. This is the desired behavior,
and points to a potential weakness in the test suite.
% rather
%than the code.

\vspace{1mm}

\noindent \textbf{Masking Faults in a Program}. In rare cases,
dependent tests can hide a fault in the
program, \emph{exactly} when the test suite is executed in its default
order. Masking occurs when a test case $t$ \emph{should}
reveal a fault, but tests executed before $t$ in a test suite always
generate environments in which $t$ passes accidently and
does not reveal the fault.

\input{clicode}

We only found two dependent tests in
the Apache CLI library~\cite{cli} for this category.
Figure~\ref{fig:option_builder} shows the simplified fault-related
code. The fault is due to the improper initialization of the static variable
\CodeIn{argName}. The static variable \CodeIn{argName} should be set
to its default value \CodeIn{``arg''} by calling
method \CodeIn{reset()}. Otherwise, \CodeIn{argName}'s
default value remains \CodeIn{null} and should \emph{not} be
used in creating an \CodeIn{OptionBuilder} object.
In CLI, two test cases 
\code{Bugs\-Test.test13666} and \code{Bugs\-Test.test27635}
can reveal this potential fault by directly initializing
a \CodeIn{OptionBuilder} object without calling method \CodeIn{reset()}.
These two tests fail when run in isolation,
but both pass when run in the default order. This is because
in the default order, tests running \emph{before} these
two tests call \textit{reset()} at least once, which sets
initialize the value of \CodeIn{argName} and masks the fault.

%Both dependent tests can reveal this fault,  but
%the default order of test execution makes both tests pass
%accidentally. 

Such dependent tests have a non-trivial impact in practice.
This fault is reported in the bug database several times~\cite{clibug},
starting on March 13, 2004 (CLI-26). The report was marked as resolved
\emph{three years} later on March 15, 2007 when developers
realized the test dependence. Developers simply fixed this
fault by adding a static initialization block which
calls \CodeIn{reset()} in class \CodeIn{OptionBuilder}.


\subsubsection{Implications for Dependent Test Detection}

We summarize the main implications of our findings.

\noindent \textbf{{Dependent tests exist in practice, but
they can be hard to identify.}}
None of the dependent tests we studied is identified by
running the existing test suite in the default order. Often,
a dependent test is reported when the
test suite is reordered, either accidentally by a user or
a maintainer or by a testing tool. This indicates that
dependent test detection techniques should
explicitly search for such dependent tests.

\vspace{1mm}
\noindent \textbf{Dependent test detection techniques
can bound the search space to a small number of tests.}
In theory, a technique needs to exhaustively execute
all $n!$ permutations of a $n$-sized
test suite to detect all dependent tests. This is
not feasible for realistic $n$.  Our study shows that
most dependent tests can be manifested by executing
no more than 2 tests. Thus, a practical technique
can focuses on running only short subsequences (whoses
length is bounded by a parameter $k$)
of a test suite. This will reduce the number of permutations
to $O(n^k)$, which for small $k$ is tractable.

\vspace{1mm}
\noindent \textbf{Dependent test detection techniques
should focus on analyzing accesses to global variables.}
Dependent tests can result from many
interactions with the execution envrionment, including
global variables, file systems, databases, network, etc.
However, as reflected by our study, more than half of the
real-world dependent tests are simply caused
by static variable accesses. This implies a technique
may achieve a high return by focusing on global variables.


%\vspace{1mm}
%\noindent \textbf{Dependent test fixing tool
%Test dependence reveals flaws in the test code.}
%This indicates that a potential dependent test fixing tool should target
%the test code


\subsection{Threats to validity}

Our findings apply in the context of our study and methodology and may not
apply to arbitrary programs.
The applications we studied are widely used and have comprehensive test suites.
However, a limitation is that they are all written in 
Java and have JUnit test suites.  

We accepted the developers' judgment regarding which tests are dependent
and the severity of each test.  We did not intentionally ignore
any test dependence in the issue tracking system.
However, a limitation is that the developers might have made a mistake,
might not have marked a test dependence in a way we found it, and are
unlikely to have found all the dependent tests in those projects. 


%  LocalWords:  JBoss Codehaus reproducibility multi dependences SWT CLI
%  LocalWords:  TestDisplay test13666 subsequences
