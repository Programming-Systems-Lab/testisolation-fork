\begin{abstract}
Test dependence arises when executing a test in different environments
causes it to return different results. It has been broadly assumed
that two tests in a suite are inherently independent. To investigate
the validity of this assumption, this paper presents four results. 

First, we formally define test dependence in terms of
test suites as ordered sequences of tests along with explicit
environments in which these tests are executed. We use this
formalization to formulate the problem
of detecting dependent tests, and prove that a useful special
case is NP-complete. 

Second, we describe a study of \todo{NUM}
real-world dependent tests from \todo{NUM} bug repositories
to show that test dependence arises in practice.
We also show that test dependence can have potentially costly
repercussions such as masking program faults and leading
to spurious bug reports, and can be hard to identify
unless explicitly searched for.

Third, guided by the study, we propose two algorithms to detect
dependent tests in a test suite. 
%Our algorithms use both static and
%dynamic program analyses to quickly \todo{the goal
%of algorithms here.}

Fourth, we implement our dependent test detection algorithms
in a prototype tool, and apply it to \todo{NUM} real-world programs.
Our tool revealed a large number of unknown dependent tests, suggesting
that \todo{the implication.}

\end{abstract}
