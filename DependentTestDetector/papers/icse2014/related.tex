\section{Related Work}
\label{sec:related}

%\enlargethispage{5pt}

%Denoting a group of test cases as a ``suite of test programs'' began around the
%mid-1970's~\cite[p.~217]{brown:CSUR:1974}; similar terms include
%``testcase dataset''~\cite{milleretal:ICRS:1975} and ``scenario,''
%which an IEEE Standard defines as ``groups of test cases;
%synonyms are script, set, or suite''~\cite[p.~10]{IEEE:829-1998}.

Treating test suites explicitly as \emph{mathematical sets} of tests dates at least
to Howden~\cite[p.~554]{howden:ToC:1975} and remains common in the literature.
The execution order of tests in a suite is usually not considered:
%or informally, suggesting that the potential of executing a given test
%in different contexts is immaterial to those results: 
that is, test independence is assumed. Nonetheless,
some research has considered it. We next discuss some
existing definitions of test dependence, techniques that
assume test dependence, and tools that support specifying
test dependence.

\subsection{Test Dependence}

Definitions in the testing literature are generally clear that the
conditions under which a test is executed may affect its result. 
The
importance of context in testing has been explored 
in databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,
kapfhammeretal:FSE:2003}, with results about test
generation, test adequacy criteria, etc., and mobile
applications~\cite{Wang:2007:AGC}.
For the database domain, Kapfhammer and Soffa formally
define independent test suites and distinguish them from
other suites that ``can capture more of an application's
interaction with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations of test
adequacy''~\cite[p.~101]{kapfhammeretal:FSE:2003}.
By contrast, our definition differs from that of Kapfhammer
and Soffa by considering
test results rather than program and database states
(which may not affect the test results).
%Considering only manifest test dependences allows
%us to more easily situate this research in the empirical domain (Section~\ref{sec:formaldiscussion}).

The IEEE Standard for Software and System Test
Documentation (829-1998) \S 11.2.7, ``Intercase
Dependencies,'' says in its entirety: ``List the identifiers of
test cases that must be executed prior to this test
case. Summarize
the nature of the dependences''~\cite{IEEE:829-1998}.  The succeeding version of this
standard (829-2008) adds a single sentence: ``If
test cases are documented (in a tool or otherwise) in the order in
which they need to be executed, the Intercase Dependencies for most or
all of the cases may not be needed''~\cite{IEEE:829-2008}.


%In addition to the work by Kapfhammer and
%Soffa~\cite{kapfhammeretal:FSE:2003},
%there are a handful of categorical references that
%acknowledge that tests can
%be dependent based on context, suggesting 
%ways to document or find situations where the independence
%assumption fails to hold.  


%McGregor and Korson discuss interaction tests that
%are intended to identify ``two methods that may directly or indirectly
%cause each other to produce incorrect results'' and suggest constructing such
%interaction tests by identifying the values shared via parameter passing
%between methods
% that two or more test cases share~\cite[p~.69]{mcgregoretal:CACM:1994}.

Bergelson and Exman characterize a form of test dependence informally:
given two tests that each pass, the composite
execution of these tests may still
fail~\cite[p.~38]{bergelsonetal:EEE:2006}.
%% Cut for space
% That is, if 
% $t_1$ executed by itself passes and $t_2$ executed by itself passes,
% executing the sequence \suite{t_1, t_2} in the same context may fail.
However, they do not provide any empirical evidence of
test dependence nor any detection algorithms.

The C2 wiki acknowledges test dependence as undesirable~\cite{unit-test-def}:
%a possible, albeit low probability, event:
\tinysqueeze
\begin{quote}
Unit testing \dots  
requires that we test the unit in isolation. That is, we
want to be able to say, \emph{to a very high degree of confidence} [emphasis added], that
any actual results obtained from the execution of test cases are
purely the result of the unit under test. The introduction of
other units may color our results.
\end{quote}
\tinysqueeze
They further note that other tests, as well as stubs and drivers,
may ``interfere with the straightforward
execution of one or more test cases.''


Compared with these informal definitions,
we formalize test dependence and characterize 
test dependence in practice.
%, and could
%have costly repercussions.
%They give an informal definition of what it means for the execution of a
%test to influence the outcome of another test.  We define
%this precisely, and we also define manifest test dependence in terms
%of execution environments
%and test execution order rather than in terms of code use.

%Other definitions of test dependence are primarily considered
%to be \textit{syntactic} dependences between program units, for example
%methods calling other methods, and classes using other classes~\cite{bergelsonetal:EEE:2006,briandetal:SEKE:2002}. 
%\emph{Syntactic} dependence here means that a unit \code{A} cannot be
%compiled and executed without unit \code{B} being present. If we test
%such a unit \code{A} without convincing ourselves first that \code{B}
%is correct, a test failure for \code{A} is harder to interpret,
%because it could just as well indicate a fault in \code{B}.
%Zhang and Ryder extend this notion to \emph{semantic} dependences,
%which is closer to our approach~\cite{zhangetal:TR:2006}. 
%They use a notion of
%``test outcome'' to determine whether or not syntactically dependent
%classes or methods can influence each others results, and consider
%only those that can to be semantically dependent.
%They give an informal definition of what it means for the execution of a
%test to influence the outcome of another test.  We define
%this precisely, and we also define manifest test dependence in terms
%of execution environments
%and test execution order rather than in terms of code use.


%\tinysqueeze
\subsection{Techniques Assuming Test Independence}

The assumption of test independence lies at the heart of most
techniques for automated regression test selection~\cite{harroldetal:OOPSLA:2001, Orso:2004:SRT,
Briand:2009:ART, Zhang:2012:RMT, Nanda:2011:RTP},
test case prioritization~\cite{Elbaum:2000:PTC:347324.348910, Kim:2002:HTP:581339.581357, Rummel:2005:TPR:1066677.1067016, Srivastava:2002:EPT:566172.566187, Jiang:2009:ART}, 
coverage-based fault localization~\cite{Steimann:2013, Zhang:2013:IMF, Jones:2002:VTI}, etc. 


Test prioritization seeks to reorder a test suite to detect
software defects more quickly. 
Early work in test
prioritization~\cite{Wong:1997:SER:851010.856115,Rothermel:1999:TCP:519621.853398}
laid the foundation for the most commonly used problem definition:
consider the set of all permutations of a test suite and find the best
award value for an objective function over that
set~\cite{Elbaum:2000:PTC:347324.348910}.  The most common objective
functions favor permutations where higher code coverage
is achieved and more faults in the underlying
program  are found with running fewer tests.
Test independence is
%often explicitly asserted as
a requirement for most test selection and prioritization work (e.g.,~\cite[p.~1500]{Rummel:2005:TPR:1066677.1067016}).
%For some test selection and prioritization work,
%test independence is even explicitly asserted as a requirement.
%For example, Rummel et al.\ states in
%A number of studies carefully evaluation various prioritization techniques
%empirically~\cite[\emph{et
%alia}]{Rothermel:1999:TCP:519621.853398,Do:2010:ETC:1907658.1908088}. 
Evaluations of selection and prioritization techniques~\cite[\emph{et alia}]{Rothermel:1999:TCP:519621.853398,Do:2010:ETC:1907658.1908088}
are based in part on the test independence
assumption as well as the assumption that the set of faults in the underlying
program is known beforehand; the possibility that test dependence may
interfere with these techniques is not studied.
%unmask additional faults in the program is not studied.

%\begin{quote}
%A test suite contains a tuple of tests \suite{T_1 $\ldots$ T_R} that execute in a specified order.  We require that each test is
%independent so that there are no test execution ordering dependencies.  This requirement enables our prioritization algorithm to
%re-order the tests in any sequence that maximizes the suite's
%ability to isolate defects.  The assumption of test dependence
%is acceptable because the JUnit test execution framework
%provides \code{setUp} and \code{tearDown} methods that execute before
%and after a test case and can be used to clear application
%state.
%\end{quote}


Coverage-based fault localization techniques~\cite{Jones:2002:VTI}
often treat a test suite as a collection of test cases
whose result is \textit{independent} of the order of their
execution. They can also be impacted by test dependence.
In a recent evaluation of several coverage-based fault locators,
 Steimann et al.\ found that fault locators' accuracy is
 affected by tests that fail due to violation of the test
 independence assumption~\cite{Steimann:2013}. 
 %For example, if a test depends on a static field whose value is set by
 %previous test cases. 
 Compared to our work, Steimann et al.'s
 work focuses on identifying possible threats to validity
 in evaluating coverage-based fault locators, and does
 not present any formalism, study, or detection algorithms
 for dependent tests.

Test independence is different than determinism.
%
Non-determinism does not imply dependence:  a program may execute
non-deterministically, but its tests may deterministically succeed.
Further, a test may non-deterministically pass/fail without being
affected by any other test, including its own previous executions.
%
Determinism does not imply independence:  a program may have no sources of
nondeterminism, but two of its tests can be dependent.
%
The testing community sometimes mentions determinism (such as
multithreading) and execution environment (such as library
versions), without considering test dependence~\cite{Orso:2004:SRT}.
%
A stronger assumption than determinism is the Controlled Regression
Testing Assumption (CRTA)~\cite{Rothermel:1996:ART}.  It forbids porting to another system,
nondeterminism, time-dependencies, and interactions with the external
environment.  It also forbids test dependence, though the authors did not
mention test dependence explicitly.  The authors state that CRTA is ``not
necessarily impossible'' to employ.  We have a practical focus on the
often-overlooked issue of test dependence.

As shown in Sections~\ref{sec:study} and~\ref{sec:evaluation},
the test independence assumption often does not hold for either
human-written or automatically-generated tests; and the dependent
tests identified in our subject programs interfere with
existing test prioritization techniques. Thus, techniques
that rely on this assumption may need to be reformulated.

\begin{comment}
Most automated test generation
techniques~\cite{PachecoLET2007, Wang:2007:AGC,
ZhangSBE2011} do not take test dependence
into consideration. As shown in our experiments
(Section~\ref{sec:evaluation}) and previous work~\cite{RobinsonEPAL2011},
a large number of tests generated by Randoop are dependent.
We speculate that these dependences arise because automated
test generators generally create new tests
based on the program state after executing the previous test,
for the sake of test diversity and efficiency. 
When Randoop generates a nondeterministic test, it can disable the test but
leave it in the suite where it is executed in order to prevent other tests
that are dependent on it from beginning to fail~\cite{RobinsonEPAL2011}.
%I feel test generation should not belong here
%introduce 130X overhead
%may make generated
%tests less behaviorally-diverse --- as they cannot be constructed
%on top of previous tests.
Exploring how to incorporate test dependence into the design of an automated
test generator is future work.
\end{comment}

%define a test suite as a
%collection of test cases whose result is \textit{independent}
%of the order of their execution~\cite{Steimann:2013}.


\subsection{Tools Supporting Test Dependence}
\label{sec:supporting}

%\todo{This section can probably be shortened.}

Testing frameworks provide mechanisms
for developers to define the context for tests.
%JUnit, for example, provides means to
%automatically execute setup and clean-up tasks
%(\code{setUp()} and \code{tearDown()} in JUnit
%3.x, and annotations \code{@Before} and \code{@After} in
%JUnit 4.x). 
JUnit 4.11 supports
executing tests in lexicographic order by test method name~\cite{junitordering}.
%However, ensuring that these mechanisms are used properly is
%beyond the scope and capability of any framework. 
%Further, our empirical study and
%experimental results indicate that programmers often do not
%use them properly and introduce dependent tests. 
%
%Only a few tools explicitly 
%allow developers to annotate dependent tests and
%provide supporting mechanisms to ensure that the test execution framework
%respects those annotations. 
DepUnit~\cite{depunit} allows developers to define
dependences between two unit tests.
%Soft dependences control test ordering, while hard dependences in addition control whether specific tests are run at all.  
TestNG~\cite{testng} 
allows dependence annotations and supports a variety of execution policies
that respect these dependences.
% such as sequential execution
%in a single thread, execution of a single test class per thread, etc.\
What distinguishes our work from these testing frameworks is that, while they allow dependences
to be made explicit and respected during execution, they do not help developers
\emph{identify} dependences.  
%A tool that finds dependences
%(Section~\ref{sec:impl}) could co-exist
%with such frameworks by generating annotations for them.

Haidry and Miller~\cite{10.1109/TSE.2012.26} proposed a set of
test prioritization techniques that consider
test dependence.  
Their work assumes that dependencies between tests are
known, and improves existing test prioritization techniques
to make them produce a test ordering that preserves the test dependencies.
%Their work
%assumes that dependencies between tests are known (and are represented as
%partial orderings, such as that one test should be executed before another)
%without providing any empirical evidence of whether dependent tests
%exist in practice.
%\todo{Can/should we say that they did not motivate that their techniques
%  are needed in practice, but we have provided evidence of their value?}
By contrast, our work formally defines test dependence,
studies the characteristics of real-world test dependence,
shows how to detect dependent tests,
and empirically evaluates whether dependent tests exist in real-world
programs and
their impact on test prioritization techniques.


Our previous work~\cite{DBLP:conf/sigsoft/MusluSW11} proposed
an algorithm to find bugs by executing each unit
test in isolation. With a different focus,
this work investigates the validity of the test independence assumption
rather than finding new bugs,
and it presents five new results.
Further, as indicated by our study and experiments, most dependent
tests reveal weakness in the test code rather than bugs in the program. Thus,
using test dependence may not achieve a high return in bug finding.

A simple way to eliminate test dependence is
starting a new process or otherwise completely re-initializing the environment (variables,
heap, files, etc.)\ before executing each test;
JCrasher~\cite{Csallner:2004} does this, as do
some SIR applications~\cite{sir} and
some database or GUI testing tools~\cite{kapfhammeretal:FSE:2003,
Chays:2000:FTD:347324.348954, Gray:1994:QGB:191843.191886
}.
%\todo{Check the math.}
However, such an approach is computationally expensive:
Table~\ref{tab:results} shows that executing each test in a
separate JVM introduces 10--138$\times$ slowdown (compare the 
``Exhaustive $k=1$'' column to the ``Rev'' column).

% (let ((ratios (list (/ 1265 57 .1) (/ 106 65 .1) (/ 106 14 .1) (/ 166 14 .1) (/ 25 7 .1) (/ 133 50 .1) (/ 2477 103 .1) (/ 454 81 .1)))) (list (apply #'min ratios) (apply #'max ratios)))


%Such an overhead 
%may not be acceptable in practice.

%JCrasher~\cite{Csallner:2004}
%provides a mode to clear the environment changes caused
%by a previous test before generating a new one. Such functionality may happen to eliminate
%potential test dependence, but requires a tester to
%run each generated test in a separate JVM\@.
%Executing each test in a separate JVM is prohibitively expensive
%and might be useful when executing
%a test is much more expensive than re-initializing the environment.
%However, 

%  LocalWords:  Howden Kapfhammer Soffa dependences Intercase Bergelson C2
%  LocalWords:  Exman JCrasher Steimann setUp tearDown DepUnit TestNG
%%  LocalWords:  locators nondeterminism multithreading nondeterministic
%  LocalWords:  Haidry
