\section{Related Work}
\label{sec:related}

%Denoting a group of test cases as a ``suite of test programs'' began around the
%mid-1970's~\cite[p.~217]{brown:CSUR:1974}; similar terms include
%``testcase dataset''~\cite{milleretal:ICRS:1975} and ``scenario,''
%which an IEEE Standard defines as ``groups of test cases;
%synonyms are script, set, or suite''~\cite[p.~10]{IEEE:829-1998}.

Treating test suites explicitly as \emph{mathematical sets} of tests dates at least
to Howden~\cite[p.~554]{howden:ToC:1975} and remains common in the literature.
The execution order of tests in a suite is usually not considered:
%or informally, suggesting that the potential of executing a given test
%in different contexts is immaterial to those results: 
that is, test independence is assumed. We next discuss some
existing definitions of test dependence, techniques that
assume test dependence, and tools that support specifying
test dependence.


\subsection{Test Dependence}

Definitions in the testing literature are generally clear that the
conditions under which a test is executed may affect its result. 
The
importance of context in testing has been explored in some depth in
some domains including databases~\cite{Gray:1994:QGB:191843.191886,Chays:2000:FTD:347324.348954,
kapfhammeretal:FSE:2003}, with results about test
generation, test adequacy criteria, etc., and mobile
applications~\cite{Wang:2007:AGC}.
For the database domain, Kapfhammer and Soffa formally
define independent test suites and distinguish them from
other suites that ``can capture more of an application's
interaction with a database while requiring the constant monitoring of
database state and the potentially frequent re-computations of test
adequacy''~\cite[p.~101]{kapfhammeretal:FSE:2003}.
By contrast, our definition differs from that of Kapfhammer
and Soffa by considering
test results rather than program and database states.
Considering only manifest test dependences allows
us to more easily situate this research in the empirical domain (Section~\ref{sec:formaldiscussion}).

The IEEE Standard for Software and System Test
Documentation (829-1998) \S 11.2.7, ``Intercase
Dependencies,'' says in its entirety: ``List the identifiers of
test cases that must be executed prior to this test
case. Summarize
the nature of the dependences''~\cite{IEEE:829-1998}.  The succeeding version of this
standard (829-2008) adds a single sentence: ``If
test cases are documented (in a tool or otherwise) in the order in
which they need to be executed, the Intercase Dependencies for most or
all of the cases may not be needed''~\cite{IEEE:829-2008}.


%In addition to the work by Kapfhammer and
%Soffa~\cite{kapfhammeretal:FSE:2003},
%there are a handful of categorical references that
%acknowledge that tests can
%be dependent based on context, suggesting 
%ways to document or find situations where the independence
%assumption fails to hold.  


%McGregor and Korson discuss interaction tests that
%are intended to identify ``two methods that may directly or indirectly
%cause each other to produce incorrect results'' and suggest constructing such
%interaction tests by identifying the values shared via parameter passing
%between methods
% that two or more test cases share~\cite[p~.69]{mcgregoretal:CACM:1994}.

Bergelson and Exman characterize a form of test dependence informally:
given two tests that each pass, the composite
execution of these tests may still
fail~\cite[p.~38]{bergelsonetal:EEE:2006}.  That is, if 
\suite{t_1} executed by itself passes and \suite{t_2} executed by itself passes,
executing the sequence \suite{t_1, t_2} in the same context may fail.
However, they do not provide any empirical evidence of
test dependence nor any detection algorithms.

Some practitioners acknowledge test dependence as a possible, albeit low probability, event:
\begin{quote}
Unit testing \dots  
requires that we test the unit in isolation. That is, we
want to be able to say, \emph{to a very high degree of confidence} [emphasis added], that
any actual results obtained from the execution of test cases are
purely the result of the unit under test. The introduction of
other units may color our results~\cite{unit-test-def}.
\end{quote}
They further note that other tests, as well as stubs and drivers,
may ``interfere with the straightforward
execution of one or more test cases.''


Compared with these informal definitions,
we define test dependence precisely, and we also provide empirical evidence
to show that test dependence does arise in practice, and could
have costly repercussions.
%They give an informal definition of what it means for the execution of a
%test to influence the outcome of another test.  We define
%this precisely, and we also define manifest test dependence in terms
%of execution environments
%and test execution order rather than in terms of code use.

%Other definitions of test dependence are primarily considered
%to be \textit{syntactic} dependences between program units, for example
%methods calling other methods, and classes using other classes~\cite{bergelsonetal:EEE:2006,briandetal:SEKE:2002}. 
%\emph{Syntactic} dependence here means that a unit \code{A} cannot be
%compiled and executed without unit \code{B} being present. If we test
%such a unit \code{A} without convincing ourselves first that \code{B}
%is correct, a test failure for \code{A} is harder to interpret,
%because it could just as well indicate a fault in \code{B}.
%Zhang and Ryder extend this notion to \emph{semantic} dependences,
%which is closer to our approach~\cite{zhangetal:TR:2006}. 
%They use a notion of
%``test outcome'' to determine whether or not syntactically dependent
%classes or methods can influence each others results, and consider
%only those that can to be semantically dependent.
%They give an informal definition of what it means for the execution of a
%test to influence the outcome of another test.  We define
%this precisely, and we also define manifest test dependence in terms
%of execution environments
%and test execution order rather than in terms of code use.


\subsection{Techniques Assuming Test Independence}

The assumption of test independence lies at the heart of most,
if not all, techniques for automated regression test selection,
test case prioritization, test generation, coverage-based
fault localization, etc. 


Test prioritization seeks to reorder a test suite to detect
software defects more quickly. 
Early work in test
prioritization~\cite{Wong:1997:SER:851010.856115,Rothermel:1999:TCP:519621.853398}
laid the foundation for the most commonly used problem definition:
consider the set of all permutations of a test suite and find the best
award value for an objective function over that
set~\cite{Elbaum:2000:PTC:347324.348910}.  The most common objective
functions favor permutations where more faults in the underlying
program  are found with running fewer tests.
Test independence is often explicitly asserted as a
requirement for most test selection and prioritization work (e.g.,~\cite[p.~1500]{Rummel:2005:TPR:1066677.1067016}).
%For some test selection and prioritization work,
%test independence is even explicitly asserted as a requirement.
%For example, Rummel et al.\ states in
%A number of studies carefully evaluation various prioritization techniques
%empirically~\cite[\emph{et
%alia}]{Rothermel:1999:TCP:519621.853398,Do:2010:ETC:1907658.1908088}. 
Evaluations of selection and prioritization techniques
~\cite[\emph{et alia}]{Rothermel:1999:TCP:519621.853398,Do:2010:ETC:1907658.1908088}
are based in part on the test independence
assumption as well as the assumption that the set of faults in the underlying
program is known beforehand; the possibility that test dependence may unmask additional faults in the program is not studied.

%\begin{quote}
%A test suite contains a tuple of tests \suite{T_1 $\ldots$ T_R} that execute in a specified order.  We require that each test is
%independent so that there are no test execution ordering dependencies.  This requirement enables our prioritization algorithm to
%re-order the tests in any sequence that maximizes the suite's
%ability to isolate defects.  The assumption of test dependence
%is acceptable because the JUnit test execution framework
%provides \code{setUp} and \code{tearDown} methods that execute before
%and after a test case and can be used to clear application
%state.
%\end{quote}

Most automated test generation
techniques~\cite{PachecoLET2007, Wang:2007:AGC,
ZhangSBE2011} do not take test dependence
into consideration. As shown in our experiments (Section~\ref{sec:evaluation}),
a large number of tests generated by Randoop are dependent.
We speculate that these dependences arise because automated
test generation techniques generally create new tests
based on the program state after executing the previous test,
for the sake of test diversity and efficiency. 
To the best of our knowledge, only JCrasher~\cite{Csallner:2004}
provides a mode to clear the environment changes caused
by a previous test. Such functionality helps eliminate
potential test dependence, but may make generated
tests less behaviorally-diverse --- as they cannot be constructed
on top of previous tests. Exploring how to
incorporate test dependence into the design of automated
test generator is our future work.

Coverage-based fault localization techniques~\cite{Jones:2002:VTI}
often treat a test suite as a collection of test cases
whose result is \textit{independent} of the order of their
execution. They can also be impacted by test dependence.
In a recent evaluation of several coverage-based fault locators,
 Steimann et al.\ found fault locators' accuracy has been significantly
 affected by tests failed due to the violation of the test
 independence assumption~\cite{Steimann:2013}. 
 %For example, if a test depends on a static field whose value is set by
 %previous test cases. 
 Compared to our work, Steimann et al.'s
 work focuses on identifying possible threats to validity
 in evaluating coverage-based fault localization, and does
 not present any formalism, study, or detection algorithms
 for dependent tests.


%define a test suite as a
%collection of test cases whose result is \textit{independent}
%of the order of their execution~\cite{Steimann:2013}.

As shown in Sections~\ref{sec:study} and~\ref{sec:evaluation},
the test independence assumption often does not hold for either
human-written or automatically-generated tests. Thus, techniques
that rely on this assumption may need to be reformulated.

\subsection{Tools Supporting Test Dependence}
\label{sec:supporting}

Testing frameworks provide mechanisms
for developers to define the context for tests.
JUnit, for example, provides means to
automatically execute setup and clean-up tasks
(\code{setUp()} and \code{tearDown()} in JUnit
3.x, and methods annotated with \code{@Before} and \code{@After} in
JUnit 4.x). Ensuring that these mechanisms are used properly, however, is
beyond the scope of any framework, although the latest release of JUnit
(version 4.11)
supports executing tests in lexicographic order by test method name~\cite{junitordering}.


Only a few tools explicitly consider test dependence, by
allowing developers to annotate dependent tests and
provide supporting mechanisms to ensure that the test execution framework
respects those annotations.  DepUnit~\cite{depunit}
allows developers to define soft and hard dependences. Soft dependences control
test ordering, while hard dependences in addition control whether specific tests are
run at all.  TestNG~\cite{testng} 
allows dependence annotations and supports a variety of execution policies
that respect these dependences
such as sequential execution
in a single thread, execution of a single test class per thread, etc.\
What distinguishes our work from these approaches is that, while they allow dependences
to be made explicit and respected during execution, they do not help developers
\emph{identify} dependences.  A tool that finds dependences
(Section~\ref{sec:impl}) could co-exist
with such frameworks by generating annotations for them.

In our previous work~\cite{DBLP:conf/sigsoft/MusluSW11}, we proposed
an algorithm to find bugs by executing each unit
test in isolation. With a significantly different focus,
in this work, we investigate the validity of the test independence assumption
in the testing literature rather than finding new bugs,
and present four new results (summarized in Section~\ref{sec:contributions}).
Further, as indicated by our study and experiments, most dependent
tests reveal weakness in the test code rather than bugs in the program. Thus,
using test dependence may not achieve a high return in finding bugs.

%  LocalWords:  Howden Kapfhammer Soffa dependences Intercase Bergelson
%  LocalWords:  Exman JCrasher Steimann setUp tearDown DepUnit TestNG
